---
title: "MSc. 1046495 Youssuf Abdelatif"
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
date: "2023-09-28"
---

```{r Setup, include=FALSE}

#In this code section, all required standard packages are loaded and the working directory is set up
#Special packages that are used for only once for a specific part of the empirical analysis are 
#loaded separately, directly preceding their usage. 

setComputerWD <- function() {
  cat("Enter the computer type (HP or Surface): ")
  computerType <- readline()
  
  if (tolower(computerType) == "hp") {
    setwd("C:/Users/Yussuf/OneDrive/Desktop/UniVWL/Projekt")
  } else if (tolower(computerType) == "surface") {
    setwd("C:/Users/Yussuf Schwarz/OneDrive/Desktop/UniVWL/Projekt")
  } else {
    cat("Invalid input. Please enter HP or Surface.")
  }
}
setComputerWD()


packages <- c("tibble", "quantmod", "fBasics", "ggplot2", "zoo", "readxl",
              "kableExtra", "stargazer", "moments", "lubridate", "lmtest",
              "forecast", "gridExtra", "tseries", "vars", "bruceR", "fGarch",
              "dplyr", "glmnet", "plotmo", "tsDyn", "writexl", "knitr","caret", "qpcR",
              "factoextra", "FactoMineR", "stm", "tm", "rgl", "car", "plotly",
              "scatterplot3d", "stylo", "cluster", "fpc", "sparsevar","doParallel")

for (pkg in packages) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg)
    library(pkg, character.only = TRUE)
  } else {
    library(pkg, character.only = TRUE)
  }
}

```

```{r Get approval data, include=FALSE}

############################Approval Data################################################

#In this code section, the approval data is importet into R and formatted.

approval=as.data.frame(read_excel("Approval_RoosevelttoBiden.xlsx"))
approval=approval[1:1837,] #take data starting 1950...only for dates starting 1950 observations of all quarters 

for (i in 1:ncol(approval)) {
  approval[,i]=rev(approval[,i])
}
#are available




#Change the format of the date columns to a date format used by R:
unique(nchar(approval$`Start Date`))
unique(nchar(approval$`End Date`))

Start_Date=c()
for (i in 1:length(approval$`Start Date`)) {
  if (nchar(approval$`Start Date`[i]) == 5) {
    Start_Date[i] <- format(as.Date(as.numeric(approval$`Start Date`[i]), origin = "1899-12-30"),"%Y-%d-%m")
  } else if (nchar(approval$`Start Date`[i]) == 7) {
    Start_Date[i] <- format(as.Date(approval$`Start Date`[i], format = "%m/%d/%y"))
  } else if (nchar(approval$`Start Date`[i]) == 9) {
    Start_Date[i] <- format(as.Date(approval$`Start Date`[i], format = "%m/%d/%Y"))
  }else if (nchar(approval$`Start Date`[i]) == 10) {
    Start_Date[i] <- format(as.Date(approval$`Start Date`[i], format = "%m/%d/%Y"))
  }
}

End_Date=c()
for (i in 1:length(approval$`End Date`)) {
  if (nchar(approval$`End Date`[i]) == 5) {
    End_Date[i] <- format(as.Date(as.numeric(approval$`End Date`[i]), origin = "1899-12-30"),"%Y-%d-%m")
  } else if (nchar(approval$`End Date`[i]) == 9) {
    End_Date[i] <- format(as.Date(approval$`End Date`[i], format = "%m/%d/%Y"))
  }else if (nchar(approval$`End Date`[i]) == 10) {
    End_Date[i] <- format(as.Date(approval$`End Date`[i], format = "%m/%d/%Y"))
  }
}

approval$`Start Date` = as.Date(Start_Date,"%Y-%m-%d")
approval$`End Date` = as.Date(End_Date,"%Y-%m-%d" )
##

#Create a matrix of dummies that show 1 for each president
DPresident=matrix(nrow=length(approval$President),ncol=length(unique(approval$President)))
for (i in 1:length(unique(approval$President))){
  DPresident[,i]=ifelse(approval$President==unique(approval$President)[i],1,0)
}
colnames(DPresident)=unique(approval$President)
approval=as.data.frame(cbind(approval,DPresident))
head(approval)
##




knitr::opts_chunk$set(echo = TRUE)


```

```{r Create approval plot with original data, include=FALSE}

#In this code section, the approval data in its original fequency is plotted for exploratory reasons,
#and to cross-check if the retrieved data is correct.
#The plots are not used in the study. 

###Extract first date and last date of each term for plot:

Begin=c()
End=c()

for (i in 1:length(unique(approval$President))) {
  print(unique(approval$President)[i])
  End[i]=as.character(head(approval$`End Date` [approval[,unique(approval$President)[i]]==1],1))
  Begin[i]=as.character(tail(approval$`End Date` [approval[,unique(approval$President)[i]]==1],1))
}

Begin = as.Date(Begin,format = "%Y-%m-%d")
End = as.Date(End,format = "%Y-%m-%d")

###Plot monthly approval data
plot1=ggplot() +
  geom_line(aes(x = approval$`Start Date`, y =approval$Approving))+
    theme_classic() +
  labs(title="Gallup Presidential Approval Rating",subtitle="1950-2023")+xlab("Time")+ylab("Percent")+
  geom_rect(aes(xmin=Begin[1],xmax=End[1],fill="aquamarine"),ymin=0,ymax=100, size=0.5, alpha=0.1)+
  geom_rect(aes(xmin=Begin[2],xmax=End[2]
            ,fill="bisque"),ymin=0,ymax=100, size=0.5, alpha=0.1) +
  geom_rect(aes(xmin=Begin[3],xmax=End[3]
            ,fill="black"),ymin=0,ymax=100, size=0.5, alpha=0.1) +
  geom_rect(aes(xmin=Begin[4],xmax=End[4]
            ,fill="blue"),ymin=0,ymax=100, size=0.5, alpha=0.1) +
  geom_rect(aes(xmin=Begin[5],xmax=End[5]
            ,fill="brown"),ymin=0,ymax=100, size=0.5, alpha=0.1) +
    geom_rect(aes(xmin=Begin[6],xmax=End[6]
            ,fill="cyan"),ymin=0,ymax=100, size=0.5, alpha=0.1) +
    geom_rect(aes(xmin=Begin[7],xmax=End[7]
            ,fill="green"),ymin=0,ymax=100, size=0.5, alpha=0.1) +
    geom_rect(aes(xmin=Begin[8],xmax=End[8]
            ,fill="orange"),ymin=0,ymax=100, size=0.5, alpha=0.1) +
    geom_rect(aes(xmin=Begin[9],xmax=End[9]
            ,fill="pink"),ymin=0,ymax=100, size=0.5, alpha=0.1) +
    geom_rect(aes(xmin=Begin[10],xmax=End[10]
            ,fill="purple"),ymin=0,ymax=100, size=0.5, alpha=0.1) +
    geom_rect(aes(xmin=Begin[11],xmax=End[11]
            ,fill="red"),ymin=0,ymax=100, size=0.5, alpha=0.1) +
    geom_rect(aes(xmin=Begin[12],xmax=End[12]
            ,fill="yellow"),ymin=0,ymax=100, size=0.5, alpha=0.1) +
  scale_fill_identity(labels = c(
      aquamarine = unique(approval$President)[1], bisque = unique(approval$President)[2],
      black = unique(approval$President)[3], blue = unique(approval$President)[4],
      brown = unique(approval$President)[5], cyan = unique(approval$President)[6],
      green = unique(approval$President)[7], orange = unique(approval$President)[8],
      pink = unique(approval$President)[9], purple = unique(approval$President)[10],
      red = unique(approval$President)[11], yellow = unique(approval$President)[12]
    ),guide="legend",name=NULL,aesthetics = "fill")
print(plot1)


knitr::opts_chunk$set(echo = TRUE)

```

```{r Process approval data to monthly and quarterly data, include=FALSE}

#In this code section, the frequency of the approval data is standardized to quarterly and monthly.
#In the analysis, only the quarterly data is used.

############################################APPROVAL QUARTERLY##########################################

#Approval surveys are taken over a period of days each month. Turning monthly data into quarterly data by calculating the mean
approval$quarter <-quarter(approval$`End Date`)
approval$year <-year(approval$`End Date`)
approval_quarterly=data.frame(cbind(aggregate(approval$Approving,by=list(approval$quarter,approval$year),FUN=mean)))
disapproval_quarterly=data.frame(cbind(aggregate(approval$Disapproving,by=list(approval$quarter,approval$year),FUN=mean)))

DPresident_quarterly=matrix(nrow=length(approval_quarterly$x),ncol=length(unique(approval$President)))
for (i in 1:length(unique(approval$President))){
  DPresident_quarterly[,i]=aggregate(approval[,unique(approval$President)[i]],by = list(approval$quarter,approval$year),FUN=max)[,3]
}

approval_q = as.data.frame(cbind(approval_quarterly$Group.1,approval_quarterly$Group.2,approval_quarterly$x,disapproval_quarterly$x,DPresident_quarterly))
colnames(approval_q)=c("Quarter","Year","Approval","Disapproval",unique(approval$President))

write_xlsx(approval_q,"Approval_quarterly.xlsx") #creating a table of the retrieved data 


############################################APPROVAL MONTHLY##########################################

approval$month = month(approval$`End Date`)
approval_monthly=data.frame(cbind(aggregate(approval$Approving,by=list(approval$month,approval$year),FUN=mean)))
disapproval_monthly=data.frame(cbind(aggregate(approval$Disapproving,by=list(approval$month,approval$year),FUN=mean)))
DPresident_monthly=matrix(nrow=length(approval_monthly$x),ncol=length(unique(approval$President)))
for (i in 1:length(unique(approval$President))){
  DPresident_monthly[,i]=aggregate(approval[,unique(approval$President)[i]],by = list(approval$month,approval$year),FUN=max)[,3]
}
approval_m = as.data.frame(cbind(approval_monthly$Group.1,approval_monthly$Group.2,approval_monthly$x,disapproval_monthly$x,DPresident_monthly))

colnames(approval_m)=c("Month","Year","Approval","Disapproval",unique(approval$President))
approval_m$`End Date`= ym((paste0(approval_m$Year, "-", approval_m$Month)))
colnames(approval_m)=c("Month","Year","Approval","Disapproval",unique(approval$President))
approval_q$`End Date`= yq((paste0(approval_q$Year, "-", approval_q$Quarter)))
colnames(approval_m)=c("Month","Year","Approval","Disapproval",unique(approval$President),"Date")

write_xlsx(approval_m,"Approval_monthly.xlsx") #creating a table of the retrieved data 



knitr::opts_chunk$set(echo = TRUE)

```

```{r Create approval plots, include=FALSE}

#In this code section, both monthly and quarterly approval rates are plotted for exploratory reasons.
#However, only the plots of the quarterly data are used in the analysis.
#Furthermore, the descriptive statistics of the approval rates are created.

###Extract first date and last date of presidents for quarterly plot:

approval_q$`End Date`= yq((paste0(approval_q$Year, "-", approval_q$Quarter)))

for (i in 2:((ncol(approval_q))-1)) {
  approval_q[,i]=as.numeric(approval_q[,i])
}

Begin_q=c()
End_q=c()

for (i in 1:length(unique(approval$President))) {
  print(unique(approval$President)[i])
  End_q[i]=as.character(head(approval_q$`End Date` [approval_q[,unique(approval$President)[i]]==1],1))
  Begin_q[i]=as.character(tail(approval_q$`End Date` [approval_q[,unique(approval$President)[i]]==1],1))
}

Begin_q = as.Date(Begin_q,format = "%Y-%m-%d")
End_q = as.Date(End_q,format = "%Y-%m-%d")

###Plot monthly approval data
plot2=ggplot() +
  geom_line(aes(x = approval_q$`End Date`, y =approval_q$Approval))+
    theme_classic() +
  labs(title="Gallup Presidential approval_q Rating",subtitle="1950-2023")+xlab("Time")+ylab("Percent")+
  geom_rect(aes(xmin=Begin_q[1],xmax=End_q[1],fill="aquamarine"),ymin=0,ymax=100, size=0.5, alpha=0.1)+
  geom_rect(aes(xmin=Begin_q[2],xmax=End_q[2]
            ,fill="bisque"),ymin=0,ymax=100, size=0.5, alpha=0.1) +
  geom_rect(aes(xmin=Begin_q[3],xmax=End_q[3]
            ,fill="black"),ymin=0,ymax=100, size=0.5, alpha=0.1) +
  geom_rect(aes(xmin=Begin_q[4],xmax=End_q[4]
            ,fill="blue"),ymin=0,ymax=100, size=0.5, alpha=0.1) +
  geom_rect(aes(xmin=Begin_q[5],xmax=End_q[5]
            ,fill="brown"),ymin=0,ymax=100, size=0.5, alpha=0.1) +
    geom_rect(aes(xmin=Begin_q[6],xmax=End_q[6]
            ,fill="cyan"),ymin=0,ymax=100, size=0.5, alpha=0.1) +
    geom_rect(aes(xmin=Begin_q[7],xmax=End_q[7]
            ,fill="green"),ymin=0,ymax=100, size=0.5, alpha=0.1) +
    geom_rect(aes(xmin=Begin_q[8],xmax=End_q[8]
            ,fill="orange"),ymin=0,ymax=100, size=0.5, alpha=0.1) +
    geom_rect(aes(xmin=Begin_q[9],xmax=End_q[9]
            ,fill="pink"),ymin=0,ymax=100, size=0.5, alpha=0.1) +
    geom_rect(aes(xmin=Begin_q[10],xmax=End_q[10]
            ,fill="purple"),ymin=0,ymax=100, size=0.5, alpha=0.1) +
    geom_rect(aes(xmin=Begin_q[11],xmax=End_q[11]
            ,fill="red"),ymin=0,ymax=100, size=0.5, alpha=0.1) +
    geom_rect(aes(xmin=Begin_q[12],xmax=End_q[12]
            ,fill="yellow"),ymin=0,ymax=100, size=0.5, alpha=0.1) +
  scale_fill_identity(labels = c(
      aquamarine = unique(approval$President)[1], bisque = unique(approval$President)[2],
      black = unique(approval$President)[3], blue = unique(approval$President)[4],
      brown = unique(approval$President)[5], cyan = unique(approval$President)[6],
      green = unique(approval$President)[7], orange = unique(approval$President)[8],
      pink = unique(approval$President)[9], purple = unique(approval$President)[10],
      red = unique(approval$President)[11], yellow = unique(approval$President)[12]
    ),guide="legend",name=NULL,aesthetics = "fill")
print(plot2)




###Plot monthly approval data
approval_q_graph = approval_q[(13:(nrow(approval_q))),]
boxes <- data.frame(xmin = as.Date(c("1953-01-01", "1961-01-01", "1963-10-01",
                             "1969-01-01", "1974-10-01", "1977-01-01",
                             "1981-01-01", "1989-01-01", "1993-01-01",
                             "2001-01-01", "2009-01-01", "2017-01-01",
                             "2021-01-01")),
                    xmax = as.Date(c("1961-01-01", "1963-10-01",
                             "1969-01-01", "1974-10-01", "1977-01-01",
                             "1981-01-01", "1989-01-01", "1993-01-01",
                             "2001-01-01", "2009-01-01", "2017-01-01",
                             "2021-01-01", "2023-10-01")),
                    President = ordered(1:13, labels = c("Eisenhower",
                                  "Kennedy", "Johnson",
                                  "Nixon","Ford",
                                  "Carter","Reagan",
                                  "Bush Senior", "Clinton",
                                  "Bush Junior", "Obama", 
                                  "Trump","Biden"))
)

plot3 = ggplot(approval_q_graph) +
  geom_rect(data = boxes, 
            aes(xmin = xmin, xmax = xmax, ymin = 0, ymax = 100, fill = President), 
                alpha = 0.1) + #plot the rectangles before the line is plotted
  geom_line(aes(x = `End Date`, y = Approval))+
  theme_classic() +
  labs(title="Presidential Approval Rating Quarterly",subtitle="Gallup; 1953-2023")+
  xlab("Time")+ylab("Percent") +
  scale_fill_manual(values = setNames(c("bisque", "black", "blue", "brown", "cyan", "green",
                               "orange", "pink", "purple", "red", "yellow", "sienna", "royalblue1"),
                               levels(boxes$President)))
print(plot3)



par(mfrow=c(1,2))
acf(approval_q_graph$Approval,main="Autocorrelation Function (ACF) of Presidential Approval")
pacf(approval_q_graph$Approval,main="Partial Autocorrelation Function (PACF) of Presidential Approval")

arimmodelexp = arima(approval_q_graph$Approval,order = c(1,0,0))
stargazer(arimmodelexp)


summary_statistics = data.frame(matrix(NA,nrow =length(unique(boxes$President)) ,ncol = 7))
boxes <- data.frame(xmin = as.Date(c("1953-01-01", "1961-01-01", "1963-10-01",
                             "1969-01-01", "1974-10-01", "1977-01-01",
                             "1981-01-01", "1989-01-01", "1993-01-01",
                             "2001-01-01", "2009-01-01", "2017-01-01",
                             "2021-01-01")),
                    xmax = as.Date(c("1961-01-01", "1963-10-01",
                             "1969-01-01", "1974-10-01", "1977-01-01",
                             "1981-01-01", "1989-01-01", "1993-01-01",
                             "2001-01-01", "2009-01-01", "2017-01-01",
                             "2021-01-01", "2023-07-01")),
                    President = ordered(1:13, labels = c("Eisenhower",
                                  "Kennedy", "Johnson",
                                  "Nixon","Ford",
                                  "Carter","Reagan",
                                  "Bush Senior", "Clinton",
                                  "Bush Junior", "Obama", 
                                  "Trump","Biden")))
party = c("Republican","Democratic","Democratic","Republican","Republican","Democratic","Republican","Republican","Democratic","Republican","Democratic","Republican","Democratic")
for (i in 1:nrow(boxes)) {
  begin = boxes$xmin[i]
  end = boxes$xmax[i]
  approval_summary = approval_q_graph$Approval[ which(approval_q_graph$`End Date` == begin) : which(approval_q_graph$`End Date` == end)]
  summary_statistics[i,1] = as.character(unique(boxes$President)[i])
  summary_statistics[i,2] = as.character(party[i])
  summary_statistics[i,3] = round(mean(approval_summary),2)
  summary_statistics[i,4] = round(sd(approval_summary)^2,2)
  summary_statistics[i,5] = round(min(approval_summary),2)
  summary_statistics[i,6] = round(max(approval_summary),2)
  summary_statistics[i,7] = round(max(approval_summary),2)-round(min(approval_summary),2)
} 

colnames(summary_statistics) = c("President","Affiliation","Mean","Variance","Min.","Max.","Range")

stargazer(summary_statistics,title = "main",header = T,summary = F,rownames = F,digits = 2)



###Plot monthly approval data with recessions
approval_q_graph = approval_q[(13:(nrow(approval_q))),]
boxes <- data.frame(xmin = as.Date(c("1953-01-01", "1961-01-01", "1963-10-01",
                             "1969-01-01", "1974-10-01", "1977-01-01",
                             "1981-01-01", "1989-01-01", "1993-01-01",
                             "2001-01-01", "2009-01-01", "2017-01-01",
                             "2021-01-01")),
                    xmax = as.Date(c("1961-01-01", "1963-10-01",
                             "1969-01-01", "1974-10-01", "1977-01-01",
                             "1981-01-01", "1989-01-01", "1993-01-01",
                             "2001-01-01", "2009-01-01", "2017-01-01",
                             "2021-01-01", "2023-10-01")),
                    President = ordered(1:13, labels = c("Eisenhower",
                                  "Kennedy", "Johnson",
                                  "Nixon","Ford",
                                  "Carter","Reagan",
                                  "Bush Senior", "Clinton",
                                  "Bush Junior", "Obama", 
                                  "Trump","Biden"))
)
recessions = data.frame( xmin = as.Date(c("1953-01-01","1957-01-01","1960-01-01","1973-01-01","1980-01-01","1990-01-01","2000-01-01","2007-01-01","2020-01-01")),
                          
                          xmax = as.Date(c("1954-10-01","1958-10-01","1960-10-01","1975-10-01","1982-10-01","1991-10-01","2002-10-01","2011-10-01","2021-10-01")),
                           
                           
                           Recession = ordered(1:9,labels = c("Recession of 1953","Recession of 1957","Recession of 1960","Oil crisis","Early 1980s Recession","Early 1990s Recession","Dotcom Bubble and 9/11","Great Recession","COVID19 crisis" )))

plot4 = ggplot(approval_q_graph) +
  geom_rect(data = recessions, 
            aes(xmin = xmin, xmax = xmax, ymin = 0, ymax = 100, fill = Recession), 
                alpha = 0.3) + #plot the rectangles before the line is plotted
  geom_line(aes(x = `End Date`, y = Approval))+
  theme_classic() +
  labs(title="Presidential Approval Rating Quarterly",subtitle="Gallup; 1953-2023")+
  xlab("Time")+ylab("Percent") +
  scale_fill_manual(values = setNames(c("grey","red","green","yellow","purple","blue","brown","bisque","cyan"),
                               levels(recessions$Recession)))


plot4 = plot4 + 
  geom_vline(aes(xintercept=as.Date("1953-01-01"),color="black"), linetype="longdash")+
  geom_vline(xintercept=boxes$xmax[1], linetype="longdash")+
  geom_vline(xintercept=boxes$xmax[2], linetype="longdash")+
  geom_vline(xintercept=boxes$xmax[3], linetype="longdash")+
  geom_vline(xintercept=boxes$xmax[4], linetype="longdash")+
  geom_vline(xintercept=boxes$xmax[5], linetype="longdash")+
  geom_vline(xintercept=boxes$xmax[6], linetype="longdash")+
  geom_vline(xintercept=boxes$xmax[7], linetype="longdash")+
  geom_vline(xintercept=boxes$xmax[8], linetype="longdash")+
  geom_vline(xintercept=boxes$xmax[9], linetype="longdash")+
  geom_vline(xintercept=boxes$xmax[10], linetype="longdash")+
  geom_vline(xintercept=boxes$xmax[11], linetype="longdash")+
  geom_vline(xintercept=boxes$xmax[12], linetype="longdash")+
  scale_color_manual(name = "", values = c("black"),labels="Change in office")

print(plot4)

ggsave("plot4.png", plot = plot4,width = 12,height =8,device = "png")


```

```{r Show plots, include=T}

write_xlsx(approval_q, "approval_q.xlsx")

plot1
plot2

ggsave("plot1.png", plot = plot1,width = 12,height =8,device = "png")
ggsave("plot2.png", plot = plot2,width = 12,height =8,device = "png")
ggsave("plot3.png", plot = plot3,width = 12,height =8,device = "png")
ggsave("plot4.png", plot = plot4,width = 12,height =8,device = "png")


```


```{r Stationarity test of quarterly approval data, include=FALSE}

#In this code section, the approval rates are tested for nonstationarity. 
#The tests are conducted for the overall sample and for each presidency. 
#For all, both the ADF and KPSS test are used. 
#The data is differenced until until the whole sample which is used in the analysis is found to be
#stationary according to the KPSS test.

approval_q = approval_q[,c(1:4,17)]

quarters_between <- function(start_date, end_date) {
  start_quarter <- as.yearqtr(start_date)
  end_quarter <- as.yearqtr(end_date)-0.25
  
  num_quarters <- length(seq(start_quarter, end_quarter, by = 1/4))
  return(num_quarters)
}

quarters_each_president=c()
for (i in 1:nrow(boxes)) {
  quarters_each_president[i]=quarters_between(boxes$xmin[i],boxes$xmax[i])
}


for (i in 1:length(as.character(boxes$President))) {
  name = paste0(as.character(boxes$President)[i],"_length")
  length_president=c()
 length_president = rep(1,quarters_each_president[i])
  assign(name,length_president, env = .GlobalEnv)
} 

start_quarter <- as.yearqtr("1953 Q1")
end_quarter <- as.yearqtr("2023 Q2")
all_quarters <- as.yearqtr(seq(as.Date(start_quarter), as.Date(end_quarter), by = "3 months"))
quarters_to_find <- c("1964 Q3", "1972 Q3")
positions <- which(all_quarters %in% as.yearqtr(quarters_to_find))
cat("Position of Q3 1964:", positions[1], "\n")
cat("Position of Q3 1972:", positions[2], "\n")
#47 and 79 is missing


Eisenhower_length
Kennedy_length 
Johnson_length
Nixon_length
Ford_length
Carter_length
Reagan_length
`Bush Senior_length`
Clinton_length
`Bush Junior_length`
Obama_length
Trump_length
Biden_length

quarters_passed=0
for (i in 1:length(as.character(boxes$President))) {
  final=c()
  name = paste0(as.character(boxes$President)[i],"_length")
  name1 = paste0(as.character(boxes$President)[i],"_length_final")
  vector = get(name,envir=.GlobalEnv)
  final = c(rep(0,quarters_passed),vector,rep(0,(length(all_quarters)-length(vector)-quarters_passed)))
  assign(name1,final, env = .GlobalEnv)
  quarters_passed=quarters_passed+length(vector)
  print(name)
  print(quarters_passed)
  #print(vector)
} 


for (i in 1:length(as.character(boxes$President))) {
  name1 = paste0(as.character(boxes$President)[i],"_length_final")
  vector = get(name1,envir=.GlobalEnv)
  print(length(vector))
  approval_q = as.data.frame(cbind(approval_q,vector[-c(47,79)]))
}

colnames(approval_q)= c(colnames(approval_q)[1:5],as.character(boxes$President))

#Testing approval_q#Testing quarterly approval for stationarity, using KPSS:

#the whole period:
kpss.test(approval_q$Approval,null = "L")$p.value
kpss.test(approval_q$Approval,null = "T")$p.value

President_names = as.character(boxes$President)
#Each presidency

kpsspvaluelevel=c()
for (i in 1:length(unique(President_names))) {
  kpsspvaluelevel[i]=kpss.test(approval_q$Approval[approval_q[,unique(President_names)[i]] == 1],null = "L")$p.value
  print(unique(President_names)[i])
}

kpsspvaluetrend=c()
for (i in 1:length(unique(President_names))) {
  kpsspvaluetrend[i]=kpss.test(approval_q$Approval[approval_q[,unique(President_names)[i]] == 1],null = "T")$p.value
}

resultapprovalkpss=as.data.frame(cbind(c("1950-2023",unique(President_names)),round(c(kpss.test(approval_q$Approval,null = "L")$p.value,kpsspvaluelevel),2),c(kpss.test(approval_q$Approval,null = "T")$p.value,kpsspvaluetrend
)))
colnames(resultapprovalkpss)=c("Period","KPSS P-Value Level","KPSS P-Value Trend")

####################################################################

testdecisionlevel=c()
for (i in 1:length(resultapprovalkpss$`KPSS P-Value Level`)) {
  if (resultapprovalkpss$`KPSS P-Value Level`[i]<0.05) {
  testdecisionlevel[i]="Reject"
} else{
  testdecisionlevel[i]="Accept"
}
}

testdecisiontrend=c()
for (i in 1:length(resultapprovalkpss$`KPSS P-Value Trend`)) {
  if (resultapprovalkpss$`KPSS P-Value Trend`[i]<0.05) {
  testdecisiontrend[i]="Reject"
} else{
  testdecisiontrend[i]="Accept"
}
}




adf=c()
for (i in 1:length(unique(President_names))) {
if (abs(attr(ur.df(approval_q$Approval[approval_q[,unique(President_names)[i]] == 1],type = "drift"),"teststat")[1])>abs(attr(ur.df(approval_q$Approval[approval_q[,unique(President_names)[i]] == 1],type = "drift"),"cval")[1,2])) {adf[i]="Reject"
}else{adf[i]="Accept"}
}
summary(ur.df(approval_q$Approval,type="drift"))
adf=c("Reject",adf)

#resultapprovalkpss=as.data.frame(cbind(c("1950-2023",unique(President_names)) #,round(c(kpss.test(approval_q$Approval,null = #"L")$p.value,kpsspvaluelevel),2),testdecisionlevel,round(c(kpss.test(approval_q$Approval,null = #"T")$p.value,kpsspvaluetrend),2),testdecisiontrend,adf))

resultapprovalkpss=as.data.frame(cbind(c("1950-2023",unique(President_names)),round(c(kpss.test(approval_q$Approval,null = "L")$p.value,kpsspvaluelevel),2) ,testdecisionlevel,adf))


colnames(resultapprovalkpss)=c("Period","KPSS P-Value Level","KPSS Test Decision","ADF Test Decision Level")

####################1st Difference#######################################################################


adf_diff=c()
for (i in 1:length(unique(President_names))) {
if (abs(attr(ur.df(na.omit(diff(approval_q$Approval)[approval_q[,unique(President_names)[i]] == 1]),type = "drift"),"teststat")[1])>abs(attr(ur.df(na.omit(diff(approval_q$Approval)[approval_q[,unique(President_names)[i]] == 1]),type = "drift"),"cval")[1,2])) {adf_diff[i]="Reject"
}else{adf_diff[i]="Accept"}
  print(abs(attr(ur.df(na.omit(diff(approval_q$Approval)[approval_q[,unique(President_names)[i]] == 1]),type = "drift"),"teststat")[1]))
  print(abs(attr(ur.df(na.omit(diff(approval_q$Approval)[approval_q[,unique(President_names)[i]] == 1]),type = "drift"),"cval")[1,2]))
}
summary(ur.df(diff(approval_q$Approval),type="drift"))
adf_diff=c("Reject",adf_diff)

kpsspvaluelevel_diff=c()
for (i in 1:length(unique(President_names))) {
  kpsspvaluelevel_diff[i]=kpss.test(na.omit(diff(approval_q$Approval)[approval_q[,unique(President_names)[i]] == 1]),null = "L")$p.value
  print(unique(President_names)[i])
}

resultapprovalkpss_diff=as.data.frame(cbind(c("1950-2023",unique(President_names)),round(c(kpss.test(diff(approval_q$Approval),null = "L")$p.value,kpsspvaluelevel_diff),2) ,adf_diff))


colnames(resultapprovalkpss_diff)=c("Period","KPSS P-Value Level","ADF Test Decision Level")

testdecisionlevel_diff=c()
for (i in 1:length(resultapprovalkpss_diff$`KPSS P-Value Level`)) {
  if (resultapprovalkpss_diff$`KPSS P-Value Level`[i]<0.05) {
  testdecisionlevel_diff[i]="Reject"
} else{
  testdecisionlevel_diff[i]="Accept"
}
}



resultapprovalkpss_diff=as.data.frame(cbind(c("1950-2023",unique(President_names)),round(c(kpss.test(diff(approval_q$Approval),null = "L")$p.value,kpsspvaluelevel),2) ,testdecisionlevel_diff,adf_diff))


colnames(resultapprovalkpss_diff)=c("Period","KPSS P-Value Level","KPSS Test Decision","ADF Test Decision Level")

```


```{r Show stationarity test results, results='asis',include = T}

kbl(resultapprovalkpss,align = "c")%>%
  kable_classic(full_width = F, html_font = "Cambria")

stargazer(resultapprovalkpss,summary = F)
stargazer(resultapprovalkpss_diff,summary = F)

```

```{r Download of economic variables, include=FALSE}

#In this section, the economic variables are downloaded using an API.
#Subsequently, the downloaded data is processed.
#Variables found to be duplicates, discontinued, seasonally adjusted,
#have a lower frequency than quartlery, and are on county level are eliminated.

################################################DOWNLOAD################################################

#############################DOWNLOAD Economic Variables################################################
library(eFRED)
api_key <- "115e23ba735d76c74afa31892b835b12"
set_fred_key(api_key)


queries = c("USA CPI national", "USA Unemployment national", "USA Debt national", "USA Uncertainty", 
  "USA Stock market", "USA Trade Balance", "USA Government Spending national", 
  "USA Interest Rates", "USA Consumer Confidence Index", "USA Labor Force Participation Rate", 
  "USA GDP national", "USA Housing Market", "USA Savings", "USA Tax national", 
  "USA Dollar Exchange rate", "USA Dollar", "USA money", "USA national employment", 
  "USA wage", "USA Inequality", "USA Productivity", "USA Education", 
  "USA Business Cycle", "USA Health", "USA Health Insurance", "USA Expenditures", 
  "USA Services", "USA Patents", "USA Manufacturing", "USA Technology", 
  "USA Transportation", "USA Housing", "USA Business Surveys", "USA Retail trade", 
  "USA Commodities", "USA Institutions", "USA Bonds", "USA Recession", 
  "USA Income distribution", "USA Population", "USA Emissions", "USA Crime", 
  "USA military", "USA Police", "USA Transfers", "USA Social Security", 
  "USA Benefits", "USA Food", "USA Nutrition", "USA Pension", "USA Assets", 
  "USA Age", "USA Liabilities", "USA Life", "USA Births", "USA Child", 
  "USA Parents", "USA Marriages", "USA Corporate", "USA Education", 
  "USA Schooling", "USA Research", "USA Immigration", "USA Migration", 
  "USA Race", "USA Urban", "USA Urbanization", "USA Hospital", 
  "USA Welfare", "USA Poverty", "USA Homelessness", "USA Disease", 
  "USA Cultural", "USA Elections", "USA Party", "USA Rental", 
  "USA Mortgage", "USA foreign aid", "USA Social media", "USA Job market", 
  "USA ethnicity", "USA media", "USA Internet", "USA Civil rights", 
  "USA Social Justice", "USA Access", "USA Infrastructure", "USA nature", 
  "USA Disasters", "USA Import and Export", "USA Environment", 
  "USA Temperature", "USA Energy", "USA Oil", "USA Renewable", 
  "USA Political", "USA Culture", "USA Emigration", "USA Trade relations", 
  "USA Gender Inequality", "USA Community Development", "USA Agriculture", 
  "USA Innovation Ecosystem", "USA Non-Profit Organizations", "USA Philanthropy", 
  "USA Consumer Debt", "USA Sustainable Development", "USA Transportation Infrastructure", 
  "USA Globalization", "USA Financial Markets", "USA Monetary Policy", "USA Taxation Policy", 
  "USA International Aid", "USA Social Movements", "USA Happiness Index", "USA Family Structure", 
  "USA Consumer Trends", "USA Digital Divide", "USA Sustainability"
)

stargazer(queries,summary = F)


search=data.frame(id = character(),realtime_start = character(),realtime_end = character(),title = character(),observation_start = character(),observation_end = character(),frequency = character(),frequency_short = character(),units = character(),units_short = character(),seasonal_adjustment = character(),seasonal_adjustment_short = character(),last_updated = character(),popularity = numeric(),group_popularity = numeric(),notes = character(),stringsAsFactors = FALSE)
for (i in queries) {
  print(i)
  search_result= fred_search(i, args = list(limit =   1000,sort_order="desc",order_by="popularity"), key=api_key)  
    search=rbind(search,search_result)
}

#Remove duplicated lines:
search = search[!duplicated(search), ]

#Remove all variables that are on a county level or have been discontinued
search=search[!grepl("County", search$title),]
search=search[!grepl("Income inequality in", search$title),]
search=search[!grepl("DISCONTINUED", search$title),]

#Reset the rownames of the dataframe
rownames(search) = NULL

#Exclude seasonally adjusted data
search = search[search$seasonal_adjustment_short=="NSA",]

#Exclude all time series which have an end date before 1950
search=search[!c(year(as.Date(search$observation_end)))<1950,]


#Separate search results according to frequency: Monthly, Quarterly, Daily, Weekly, Biweekly. Other frequencies are disregarded as it would shrink number of observations significantly
separatefreq = function(Freq){
  namefreq=paste0("search_",Freq)
  subset_data = search[search$frequency_short == Freq, ]
  assign(namefreq,subset_data, env = .GlobalEnv)
}
for (i in c("M","Q","D","W","BW")) {
  separatefreq(i)
}

###################################NOT CONDUCTED: ACCURACY OF DETECTING VARIABLES ON CITY LEVEL TURNS OUT TO BE TOO BAD#######################################################################################
#Remove all variables on city level
#name <- c("Business Applications for New York", "Proprietors' Farm Income in New York", "Farm Business")
#library(maps)
#us.cities$name
#city=c()
#for (j in 1:length(name)) {
#  testresult=c()
#  for (i in 1:length(us.cities$name)) {
#    testresult[i] = agrepl(us.cities$name[i], name[j], max.distance=3, ignore.case=TRUE,fixed = T)
#  }
#  if (sum(testresult>0)) {
#    city[j]=1
#  } else{
#    city[j]=0 }
#}
########################################################################################################

#Download monthly Fred Data:
data_monthly=c()
colnames_data_monthly=c()
counter=0
for (i in search_M$id) {
time_series=fred(y = i, all=T,key=api_key)
time_series=ts(time_series$y,start = c(year(as.Date(time_series$date[1])),month(as.Date(time_series$date[1]))),end =c(year(tail(time_series$date,1)),month(tail(time_series$date,1))),frequency = 12)
time_series=window(time_series, start = c(1950,1), end = c(2023,8))
data_monthly=cbind(data_monthly,time_series)
counter=counter+1
print(counter)
colnames_data_monthly[counter]=search_M$title[counter]
  if (counter %% 60 == 0) {
    cat("Pausing for 2 minutes...\n")
    Sys.sleep(120) }
}
colnames(data_monthly)=colnames_data_monthly

library("writexl")
write_xlsx(data.frame(data_monthly),"data_monthly.xlsx") #creating a table of the retrieved data 

#Download quarterly Fred Data:
library(qpcR)
data_quarterly=c()
colnames_data_quarterly=c()
counter=0
for (i in search_Q$id) {
time_series=fred(y = i, all=T,key=api_key)
time_series=ts(time_series$y,start = c(year(as.Date(time_series$date[1])),quarter(as.Date(time_series$date[1]))),end =c(year(tail(time_series$date,1)),quarter(tail(time_series$date,1))),frequency = 4)
time_series=window(time_series, start = c(1950,1), end = c(2023,3))
data_quarterly=cbind(data_quarterly,time_series)
counter=counter+1
print(counter)
colnames_data_quarterly[counter]=search_Q$title[counter]
  if (counter %% 60 == 0) {
    cat("Pausing for 2 minutes...\n")
    Sys.sleep(120) }
  if (counter %% 25 == 0) {
    cat("Pausing for 1 minutes...\n")
    Sys.sleep(60) }
}
colnames(data_quarterly)=colnames_data_quarterly

#Download daily Fred Data:
library(qpcR)
data_daily=c()
colnames_data_daily=c()
counter=0
for (i in search_D$id) {
time_series=fred(y = i, all=T,key=api_key)
time_series=xts(time_series$y,order.by = time_series$date,frequency = 365)
time_series=window(time_series, start = as.Date("1950-01-01"), end = as.Date("2023-08-31"))
data_daily=cbind(data_daily,time_series)
counter=counter+1
print(counter)
colnames_data_daily[counter]=search_D$title[counter]
  if (counter %% 60 == 0) {
    cat("Pausing for 2 minutes...\n")
    Sys.sleep(120) }
  if (counter %% 25 == 0) {
    cat("Pausing for 1 minutes...\n")
    Sys.sleep(60) }
}
colnames(data_daily)=colnames_data_daily


#Download weekly Fred Data:
library(qpcR)
data_weekly=c()
colnames_data_weekly=c()
counter=0
for (i in search_W$id) {
time_series=fred(y = i, all=T,key=api_key)
time_series=xts(time_series$y,order.by = time_series$date,frequency = "weekly")
time_series=window(time_series, start = as.Date("1950-01-01"), end = as.Date("2023-08-31"))
data_weekly=cbind(data_weekly,time_series)
counter=counter+1
print(counter)
colnames_data_weekly[counter]=search_W$title[counter]
  if (counter %% 60 == 0) {
    cat("Pausing for 2 minutes...\n")
    Sys.sleep(120) }
  if (counter %% 25 == 0) {
    cat("Pausing for 1 minutes...\n")
    Sys.sleep(60) }
}
colnames(data_weekly)=colnames_data_weekly



#Download bi-weekly Fred Data:

data_biweekly=c()
colnames_data_biweekly=c()
time_series_1=fred(y = search_BW$id[1], all=T,key=api_key)
time_series_1=xts(time_series_1$y,order.by = time_series_1$date,frequency = "weekly")
time_series_1=window(time_series_1, start = as.Date("1950-01-01"), end = as.Date("2023-08-31"))

time_series_2=fred(y = search_BW$id[2], all=T,key=api_key)
time_series_2=xts(time_series_2$y,order.by = time_series_2$date,frequency = "weekly")
time_series_2=window(time_series_2, start = as.Date("1950-01-01"), end = as.Date("2023-08-31"))

data_biweekly=cbind(time_series_1,time_series_2)
colnames_data_biweekly[1]=search_BW$title[1]
colnames_data_biweekly[2]=search_BW$title[2]




```

```{r Quarterly Data Processing, include=FALSE}

#In this code section, the frequency of the data is standardized to quarterly. 
#Q3 1964 and Q3 1972 are removed as they are missing in the approval data.
#As the starting point, Q1 1953 is chosen as this maximizes the number of variables - 
#number of observations trade-off.
#All steps are also conducted for subsamples of each presidency. However, these were not used
#in the empirical analysis.

#################################################QUARTERLY##############################################

#######################PROCESS Economic Variables TO QUARTERLY DATA#####################################

data_monthly=as.data.frame(data_monthly)
data_monthly$Date=as.Date(seq(as.Date("1950-01-01"),as.Date("2023-08-01"),by="month"))
data_monthly$Quarter=quarter(data_monthly$Date)
data_monthly$Year=year(data_monthly$Date)
data_monthly_to_quarterly = aggregate(data_monthly,by = list(data_monthly$Quarter,data_monthly$Year),FUN=mean,na.rm=TRUE)
write_xlsx(data.frame(data_monthly_to_quarterly ),"data_monthly_to_quarterly.xlsx") #creating a table of the retrieved data 


#Aggregate the daily data to quarterly data
data_daily=as.data.frame(data_daily)
data_daily$Quarter=quarter(rownames(data_daily))
data_daily$Year=year(rownames(data_daily))
data_daily_to_quarterly = aggregate(data_daily,by = list(data_daily$Quarter,data_daily$Year),FUN=mean,na.rm=TRUE)
write_xlsx(data.frame(data_daily_to_quarterly ),"data_daily_to_quarterly.xlsx") #creating a table of the retrieved data 

#Aggregate the weekly data to quarterly data
data_weekly=as.data.frame(data_weekly)
data_weekly$Quarter=quarter(rownames(data_weekly))
data_weekly$Year=year(rownames(data_weekly))
data_weekly_to_quarterly = aggregate(data_weekly,by = list(data_weekly$Quarter,data_weekly$Year),FUN=mean,na.rm=TRUE)
write_xlsx(data.frame(data_weekly_to_quarterly ),"data_weekly_to_quarterly.xlsx") #creating a table of the retrieved data 

#Aggregate the biweekly data to quarterly data
data_biweekly=as.data.frame(data_biweekly)
data_biweekly$Quarter=quarter(rownames(data_biweekly))
data_biweekly$Year=year(rownames(data_biweekly))
data_biweekly_to_quarterly = aggregate(data_biweekly,by = list(data_biweekly$Quarter,data_biweekly$Year),FUN=mean,na.rm=TRUE)
write_xlsx(data.frame(data_biweekly_to_quarterly ),"data_biweekly_to_quarterly.xlsx") #creating a table of the retrieved data 

#Create security copies and export them
data_monthly_to_quarterly_securitycopy = data_monthly_to_quarterly
data_quarterly_securitycopy = data_quarterly
data_daily_securitycopy = data_daily
data_monthly_securitycopy = data_monthly
data_daily_to_quarterly_securitycopy=data_daily_to_quarterly
data_weekly_to_quarterly_securitycopy=data_weekly_to_quarterly
data_biweekly_to_quarterly_securitycopy=data_biweekly_to_quarterly
write_xlsx(data.frame(data_monthly_to_quarterly_securitycopy),"data_monthly_to_quarterly_securitycopy.xlsx") #creating a table of the retrieved data 
write_xlsx(data.frame(data_quarterly_securitycopy),"data_quarterly_securitycopy.xlsx") #creating a table of the retrieved data 
write_xlsx(data.frame(data_daily_securitycopy ),"data_daily_securitycopy .xlsx") #creating a table of the retrieved data 
write_xlsx(data.frame(data_monthly_securitycopy),"data_monthly_securitycopy.xlsx") #creating a table of the retrieved data 
write_xlsx(data.frame(data_monthly_securitycopy),"data_monthly_securitycopy.xlsx") #creating a table of the retrieved data
write_xlsx(data.frame(data_daily_to_quarterly_securitycopy),"data_daily_to_quarterly_securitycopy.xlsx") #creating a table of the retrieved data 
write_xlsx(data.frame(data_weekly_to_quarterly_securitycopy),"data_weekly_to_quarterly_securitycopy.xlsx") #creating a table of the retrieved data 
write_xlsx(data.frame(data_biweekly_to_quarterly_securitycopy),"data_biweekly_to_quarterly_securitycopy.xlsx") #creating a table of the retrieved data 


#Store all of the retrieved data as quarterly xts() objects
data_daily_to_quarterly_xts = xts(data_daily_to_quarterly,order.by = as.Date(as.yearqtr(paste(data_daily_to_quarterly$Group.1,data_daily_to_quarterly$Group.2), format =  "%q %Y")),frequency = 4)
data_weekly_to_quarterly_xts = xts(data_weekly_to_quarterly,order.by = as.Date(as.yearqtr(paste(data_weekly_to_quarterly$Group.1,data_weekly_to_quarterly$Group.2), format =  "%q %Y")),frequency = 4)
data_biweekly_to_quarterly_xts = xts(data_biweekly_to_quarterly,order.by = as.Date(as.yearqtr(paste(data_biweekly_to_quarterly$Group.1,data_biweekly_to_quarterly$Group.2), format =  "%q %Y")),frequency = 4)
data_quarterly_to_quarterly_xts = xts(as.data.frame(data_quarterly),order.by = as.Date(seq(as.Date("1950-01-01"),as.Date("2023-08-01"),by="quarter"))
,frequency = 4)
data_monthly_to_quarterly_xts =  xts(as.data.frame(data_monthly_to_quarterly),order.by = as.Date(seq(as.Date("1950-01-01"),as.Date("2023-08-01"),by="quarter"))
,frequency = 4)

#Deleting the excess columns that were created to aggregate the data
data_daily_to_quarterly_xts$Group.1=NULL
data_daily_to_quarterly_xts$Group.2=NULL
data_weekly_to_quarterly_xts$Group.1=NULL
data_weekly_to_quarterly_xts$Group.2=NULL
data_biweekly_to_quarterly_xts$Group.1=NULL
data_biweekly_to_quarterly_xts$Group.2=NULL
data_monthly_to_quarterly_xts$Group.1=NULL
data_monthly_to_quarterly_xts$Group.2=NULL

#Creating one dataframe that contains all editted variables --> starting points: Quarterly data of all possibly relevant variables between 1950 Q1 until 2023 Q3.
data=cbind(data_quarterly_to_quarterly_xts,data_daily_to_quarterly_xts,data_weekly_to_quarterly_xts,data_biweekly_to_quarterly_xts,data_monthly_to_quarterly_xts)
data_securitycopy=data
write_xlsx(data.frame(data),"data.xlsx") #creating a table of the retrieved data 


#data=read_excel("data.xlsx")
data_f = data[1:294,] #Exclude quarter 2023 Q3 as the data is retrieved in October 2023, most quarterl data for Q3 2023 has yet to be realeased.

#58 to #59 --> in 1964 Q3 is missing ... there are no observations of approval available
#89 to #90 --> in 1972 Q3 is missing ... there are no observations of approval available

cbind(data_monthly_to_quarterly$Group.1,data_monthly_to_quarterly$Group.2)[59,]
cbind(data_monthly_to_quarterly$Group.1,data_monthly_to_quarterly$Group.2)[91,]

#As approval data for Q3 1964 an Q3 1972 is missing, the observations of these dates have to be also deleted from the data frame of the economic variables.

data_f = data_f[-c(59,91), ]  
data_final = as.data.frame(data_f)
data_final[,1]
Dates_final=as.Date(seq(as.Date("1950-01-01"),as.Date("2023-08-01"),by="quarter"))[-c(59,91)]

#Find the starting date of each economic variable:
firstnonna=data.frame()
for (i in 1:ncol(data_final)) {
  print(i)
  firstnonna[i,1]=as.Date(Dates_final[which.min(is.na(data_final[,i]))]) #saves last NA value of each variable
}
firstnonna$V1=firstnonna$V1+quarters(1) #Last NA + 1 quarter is the quarter with the first observation of each variable
unique(firstnonna$V1)
firstnonna$year=year(firstnonna$V1)
unique(firstnonna$year)
data_final[nrow(data_final)+1,] =firstnonna$year


#########################SUBSETTING COMPLETE QUARTERLY DATA SETS IN SAMPLES############################

##########################################Overall STARTING 1950######################################### 
data_final$Dates=as.Date(seq(as.Date("1950-01-01"),as.Date("2023-08-01"),by="quarter"))[-c(59,91)]
data_overall = data_final[,data_final[293,]==1950] 
data_overall = data_overall[,colSums(is.na(data_overall))<12] #Delete variables that were discontinued
data_overall = data_overall[1:291,] #Exclude Q2 2023 as a lot of variables do not have observations for this quarter
sum(is.na(data_overall)) #11 NAs in 465 variables are left --> substitute with 0:
data_overall[is.na(data_overall)]=0
write_xlsx(data_overall,"data_overall.xlsx") #creating a table of the retrieved data 

##########################################Overall STARTING 1953######################################### 
#data=read_excel("data.xlsx")
data_f = data[1:294,] #Exclude quarter 2023 Q3 as the data is retrieved in October 2023, most quarterl data for Q3 2023 has yet to be realeased.

#58 to #59 --> in 1964 Q3 is missing ... there are no observations of approval available
#89 to #90 --> in 1972 Q3 is missing ... there are no observations of approval available

cbind(data_monthly_to_quarterly$Group.1,data_monthly_to_quarterly$Group.2)[59,]
cbind(data_monthly_to_quarterly$Group.1,data_monthly_to_quarterly$Group.2)[91,]

#As approval data for Q3 1964 an Q3 1972 is missing, the observations of these dates have to be also deleted from the data frame of the economic variables.

data_f = data_f[-c(59,91), ]  
data_final = as.data.frame(data_f)
data_final[,1]
Dates_final=as.Date(seq(as.Date("1950-01-01"),as.Date("2023-08-01"),by="quarter"))[-c(59,91)]

#Find the starting date of each economic variable:
firstnonna=data.frame()
for (i in 1:ncol(data_final)) {
  print(i)
  firstnonna[i,1]=as.Date(Dates_final[which.min(is.na(data_final[,i]))]) #saves last NA value of each variable
}
firstnonna$V1=firstnonna$V1+quarters(1) #Last NA + 1 quarter is the quarter with the first observation of each variable
unique(firstnonna$V1)
firstnonna$year=year(firstnonna$V1)
unique(firstnonna$year)
data_final[nrow(data_final)+1,] =firstnonna$year

data_overall_start1953 = data_final[,data_final[293,]<1953] 
data_overall_start1953 = data_overall_start1953[13:293,]
data_overall_start1953$Dates=as.Date(seq(as.Date("1953-01-01"),as.Date("2023-08-01"),by="quarter"))[-c(48,79)]
data_overall_start1953 = data_overall_start1953[,colSums(is.na(data_overall_start1953))<10] #Delete variables that were discontinued
data_overall_start1953  = data_overall_start1953 [1:280,] #Exclude Q2 2023 as a lot of variables do not have observations for this quarter
sum(is.na(data_overall_start1953 )) #11 NAs in 465 variables are left --> substitute with 0:
data_overall_start1953[is.na(data_overall_start1953)]=0
data_overall_start1953$Dates=NULL
write_xlsx(data_overall_start1953,"data_overall_start1953.xlsx") #creating a table of the retrieved data 

####################################QUARTERLY DATA FOR EACH PRESIDENT###################################
#
#for (i in 1:length(unique(approval$President))) {
#  print(Begin_q[i])
#  print(End_q[i])
#  print(unique(approval$President)[i])
#  print(which(data_final$Dates==Begin_q[i]))
#  print(which(data_final$Dates==End_q[i]))
#  namefreq=paste0("data_overall_",unique(approval$President)[i])
#  data_subset = data_final[(which(data_final$Dates==Begin_q[i]):which(data_final$D#ates==End_q[i])),]
#  assign(namefreq,data_subset, env = .GlobalEnv)
#}#
#
#data_frame_names=c()
#for (i in 1:length(unique(approval$President))) {
#  data_frame_names[i]=paste0("data_overall_",unique(approval$President)[i])
#}
#
#
#Delete_NA_Cols = function(president,data_frame_name,NumberofNAs){
#subset_name=paste0("data_overall_",president)
#data_frame_name=get(data_frame_name)
#data_frame_name = data_frame_name [,colSums(is.na(data_frame_name ))<NumberofNAs] #Delete variables that were discontinue for each president
#assign(subset_name,data_frame_name, env = .GlobalEnv)
#}
#
#for (i in 1:length(unique(approval$President))) {
#  Delete_NA_Cols(unique(approval$President)[i],(data_frame_names[i]),3)
#}
#
#for (i in 1:length(unique(approval$President))){
#  subset_name_NAs=paste0("data_overall_",unique(approval$President)[i])
#  data_Na_president=get(subset_name_NAs)
#  print(c(subset_name_NAs,length(data_Na_president[is.na(data_Na_president)])))
#  data_Na_president[is.na(data_Na_president)]=0
#  assign(subset_name_NAs,data_Na_president, env = .GlobalEnv)
#}
#
#for (i in 1:length(unique(approval$President)) ) {
#  save_name=paste0("data_overall_",unique(approval$President)[i])
#  print(save_name)
#  xlsx_save_name=paste0("data_overall_",unique(approval$President)[i],".xlsx")
#  print(xlsx_save_name)
#  data_to_save=get(save_name)
#  write_xlsx(data_to_save,xlsx_save_name) #creating a table of the retrieved data 
#}
#
####################################QUARTERLY DATA IN EQUAL PARTS#######################################

Equal_split=function(data_frame_name,split){
  split=split
  data_frame_name=get(data_frame_name)
  for (i in c(1:split)) {
  split_name=paste0("data_overall_split_",i)
  Begin=((i-1)*(nrow(data_frame_name)/split)+1)
  End=((i)*(nrow(data_frame_name)/split))
  data_frame_subset=data_frame_name[Begin:End,]
  assign(split_name,data_frame_subset, env = .GlobalEnv)
  print(c(i,split_name,Begin,End))}
}
data_final=data_final[1:292,]
Equal_split(data_frame_name ="data_final",split = 4)


Delete_NA_Cols = function(split,data_frame_name,NumberofNAs,i){
subset_name=paste0("data_overall_split_",i)
data_frame_name_subset=get(data_frame_name)
data_frame_name_subset = data_frame_name_subset[,colSums(is.na(data_frame_name_subset))<NumberofNAs] #Delete variables that were discontinue for each split
assign(subset_name,data_frame_name_subset, env = .GlobalEnv)
}

split=4
data_frame_names_split=c()
for (i in 1:split) {
  data_frame_names_split[i]=paste0("data_overall_split_",i)
}
for (i in 1:split) {
  Delete_NA_Cols(split,(data_frame_names_split[i]),3,i)
}

data_overall_split_1[is.na(data_overall_split_1)]=0
data_overall_split_2[is.na(data_overall_split_2)]=0
data_overall_split_3[is.na(data_overall_split_3)]=0
data_overall_split_4[is.na(data_overall_split_4)]=0

for (i in 1:split) {
  save_name=paste0("data_overall_split_",i)
  print(save_name)
  xlsx_save_name=paste0("data_overall_split",i,".xlsx")
  print(xlsx_save_name)
  data_to_save=get(save_name)
  write_xlsx(data_to_save,xlsx_save_name) #creating a table of the retrieved data 
}

```

```{r Monthly Data Processing, include=FALSE}

#In this code section, the frequency of the data is standardized to monthly. 
#This data set was not used in the empirical analysis.
#Months in Q3 1964 and Q3 1972 are removed as they are missing in the approval data.
#As the starting point, Jan. 1953 is chosen as this maximizes the number of variables - 
#number of observations trade-off.
#All steps are also conducted for subsamples of each presidency. However, these were not used
#in the empirical analysis.

#################################################MONTHLY##############################################

############################PROCESS Economic Variables TO MONTHLY#####################################

#Aggregate the daily data to monthly data
data_daily=as.data.frame(data_daily)
data_daily$Month=month(rownames(data_daily))
data_daily$Year=year(rownames(data_daily))
data_daily_to_monthly = aggregate(data_daily,by = list(data_daily$Month,data_daily$Year),FUN=mean,na.rm=TRUE)
write_xlsx(data.frame(data_daily_to_monthly ),"data_daily_to_monthly.xlsx") #creating a table of the retrieved data 

#Aggregate the weekly data to monthly data
data_weekly=as.data.frame(data_weekly)
data_weekly$Month=month(rownames(data_weekly))
data_weekly$Year=year(rownames(data_weekly))
data_weekly_to_monthly = aggregate(data_weekly,by = list(data_weekly$Month,data_weekly$Year),FUN=mean,na.rm=TRUE)
write_xlsx(data.frame(data_weekly_to_monthly ),"data_weekly_to_monthly.xlsx") #creating a table of the retrieved data 

#Aggregate the biweekly data to monthly data
data_biweekly=as.data.frame(data_biweekly)
data_biweekly$Month=month(rownames(data_biweekly))
data_biweekly$Year=year(rownames(data_biweekly))
data_biweekly_to_monthly = aggregate(data_biweekly,by = list(data_biweekly$Month,data_biweekly$Year),FUN=mean,na.rm=TRUE)
write_xlsx(data.frame(data_biweekly_to_monthly ),"data_biweekly_to_monthly.xlsx") #creating a table of the retrieved data 

data_daily_to_monthly_xts = xts(data_daily_to_monthly,order.by = as.Date(as.yearmon(paste(data_daily_to_monthly$Group.1,data_daily_to_monthly$Group.2), format =  "%m %Y")),frequency = 12)
data_weekly_to_monthly_xts = xts(data_weekly_to_monthly,order.by = as.Date(as.yearmon(paste(data_weekly_to_monthly$Group.1,data_weekly_to_monthly$Group.2), format =  "%m %Y")),frequency = 12)
data_biweekly_to_monthly_xts = xts(data_biweekly_to_monthly,order.by = as.Date(as.yearmon(paste(data_biweekly_to_monthly$Group.1,data_biweekly_to_monthly$Group.2), format =  "%m %Y")),frequency = 12)
data_monthly_to_monthly_xts = xts(as.data.frame(data_monthly),order.by = as.Date(seq(as.Date("1950-01-01"),as.Date("2023-08-01"),by="month"))
,frequency = 12)

data_daily_to_monthly_xts$Group.1=NULL
data_daily_to_monthly_xts$Group.2=NULL
data_weekly_to_monthly_xts$Group.1=NULL
data_weekly_to_monthly_xts$Group.2=NULL
data_biweekly_to_monthly_xts$Group.1=NULL
data_biweekly_to_monthly_xts$Group.2=NULL



data_monthly_complete=cbind(data_monthly_to_monthly_xts,data_daily_to_monthly_xts,data_weekly_to_monthly_xts,data_biweekly_to_monthly_xts)
data_monthly_complete_securitycopy=data_monthly_complete
write_xlsx(data.frame(data_monthly_complete),"data_monthly_complete.xlsx") #creating a table of the retrieved data 


#Checking which months are not covered by the approval data:
d <-as.Date(as.yearmon(paste(approval_m$Month,approval_m$Year), format =  "%m %Y"),frequency = 12)
date_range <- as.Date(seq(as.Date("1950-01-01"),as.Date("2023-08-01"),by="month"))
missing_dates=date_range[!date_range %in% d] 

missing=c()
for (i in 1:length(missing_dates)) {
  print(c(missing_dates[i],which(date_range==missing_dates[i])))
  missing[i]=which(date_range==missing_dates[i])
}



#the observations in missing are missing ... there are no observations of approval available
#As approval data with positions saved in the vector "missing" is missing, the observations of these dates have to be also deleted from the data frame of the economic variables.


data_monthly_complete = data_monthly_complete[-missing, ]  
data_monthly_complete = as.data.frame(data_monthly_complete)
Dates_monthly_complete=as.Date(seq(as.Date("1950-01-01"),as.Date("2023-08-01"),by="month"))[-missing]

#Find the starting date of each economic variable:
firstnonnamonthly=data.frame()
for (i in 1:ncol(data_monthly_complete)) {
  print(i)
  firstnonnamonthly[i,1]=as.Date(Dates_monthly_complete[which.min(is.na(data_monthly_complete[,i]))]) #saves last NA value of each variable
}
firstnonnamonthly$V1=firstnonnamonthly$V1 #Last NA + 1 quarter is the quarter with the first observation of each variable
unique(firstnonnamonthly$V1)
firstnonnamonthly$year=year(firstnonnamonthly$V1)
unique(firstnonnamonthly$year)
data_monthly_complete[(nrow(data_monthly_complete)+1),] =firstnonnamonthly$year

data_monthly_complete$Date =NULL

##########################################Overall STARTING 1950#########################################

#Overall: 
data_monthly_complete$Dates=as.Date(seq(as.Date("1950-01-01"),as.Date("2023-09-01"),by="month"))[-missing]
data_monthly_complete_overall = data_monthly_complete[,data_monthly_complete[838,]==1950] 
data_monthly_complete_overall = data_monthly_complete_overall[,colSums(is.na(data_monthly_complete_overall))<12] #Delete variables that were discontinued
data_monthly_complete_overall = data_monthly_complete_overall[1:828,] 
sum(is.na(data_monthly_complete_overall)) # NAs left --> substitute with 0:
data_monthly_complete_overall[is.na(data_monthly_complete_overall)]=0

president_monthly_data=data.frame(cbind(data_monthly_complete[1:837,],approval_m[,5:16]))

##########################################Overall STARTING 1953######################################### 
data_monthly_complete=cbind(data_monthly_to_monthly_xts,data_daily_to_monthly_xts,data_weekly_to_monthly_xts,data_biweekly_to_monthly_xts)
data_monthly_complete_starting1953=data_monthly_complete[37:884,]
rownames(data_monthly_complete_starting1953) <- NULL

#Checking which months are not covered by the approval data:
d <-as.Date(as.yearmon(paste(approval_m$Month[29:837],approval_m$Year[29:837]), format =  "%m %Y"),frequency = 12)
date_range <- as.Date(seq(as.Date("1953-01-01"),as.Date("2023-08-01"),by="month"))
missing_dates=date_range[!date_range %in% d] 

missing=c()
for (i in 1:length(missing_dates)) {
  print(c(missing_dates[i],which(date_range==missing_dates[i])))
  missing[i]=which(date_range==missing_dates[i])
}

Dates_monthly_complete_starting1953=as.Date(seq(as.Date("1953-01-01"),as.Date("2023-08-01"),by="month"))[-missing]
data_monthly_complete_starting1953 = data_monthly_complete_starting1953[-missing, ]  

#Find the starting date of each economic variable:
firstnonnamonthly=data.frame()
for (i in 1:ncol(data_monthly_complete_starting1953)) {
  print(i)
  firstnonnamonthly[i,1]=as.Date(Dates_monthly_complete_starting1953[which.min(is.na(data_monthly_complete_starting1953[,i]))]) #saves last NA value of each variable
}
firstnonnamonthly$V1=firstnonnamonthly$V1 #Last NA + 1 quarter is the quarter with the first observation of each variable
unique(firstnonnamonthly$V1)
firstnonnamonthly$year=year(firstnonnamonthly$V1)
unique(firstnonnamonthly$year)
data_monthly_complete_starting1953=as.data.frame(data_monthly_complete_starting1953)
data_monthly_complete_starting1953[(nrow(data_monthly_complete_starting1953)+1),] =firstnonnamonthly$year

data_monthly_complete_starting1953$Date =NULL
result_cluster = data.frame(cbind(loadings_1,kc$cluster))
data_monthly_complete_starting1953$Dates=as.Date(seq(as.Date("1953-01-01"),as.Date("2023-09-01"),by="month"))[-missing]
data_monthly_complete_starting1953 = data_monthly_complete_starting1953[,data_monthly_complete_starting1953[810,]==1953] 
data_monthly_complete_starting1953 = data_monthly_complete_starting1953[,colSums(is.na(data_monthly_complete_starting1953))<12] #Delete variables that were discontinued
data_monthly_complete_starting1953 = data_monthly_complete_starting1953[1:809,] 
sum(is.na(data_monthly_complete_starting1953)) # NAs left --> substitute with 0:
data_monthly_complete_starting1953[is.na(data_monthly_complete_starting1953)]=0


########################################EACH PRESIDENT MONTHLY DATA##################################### 
president_monthly_data=data.frame(cbind(data_monthly_complete[1:837,],approval_m[,5:16]))
for (i in 1:length(unique(approval$President))) {
 subset_name=paste0("data_overall_monthly_",unique(approval$President)[i])
 pres=subset = unique(approval$President)[i]
 subset = president_monthly_data[president_monthly_data[pres]==1,]
 assign(subset_name,subset, env = .GlobalEnv)}

data_frame_month_names=c()
for (i in 1:length(unique(approval$President))) {
  data_frame_month_names[i]=paste0("data_overall_monthly_",unique(approval$President)[i])
}


Delete_NA_Cols = function(president,data_frame_name,NumberofNAs){
subset_name=paste0("data_overall_monthly_",president)
data_frame_name=get(data_frame_name)
data_frame_name = data_frame_name [,colSums(is.na(data_frame_name ))<NumberofNAs] #Delete variables that were discontinue for each president
assign(subset_name,data_frame_name, env = .GlobalEnv)
}

for (i in 1:length(unique(approval$President))) {
  Delete_NA_Cols(unique(approval$President)[i],(data_frame_month_names[i]),8)
}

for (i in 1:length(unique(approval$President))){
  subset_name_NAs=paste0("data_overall_monthly_",unique(approval$President)[i])
  data_Na_president=get(subset_name_NAs)
  print(c(subset_name_NAs,length(data_Na_president[is.na(data_Na_president)])))
  data_Na_president[is.na(data_Na_president)]=0
  assign(subset_name_NAs,data_Na_president, env = .GlobalEnv)
}

for (i in 1:length(unique(approval$President)) ) {
  save_name=paste0("data_overall_monthly_",unique(approval$President)[i])
  print(save_name)
  xlsx_save_name=paste0("data_overall_monthly_",unique(approval$President)[i],".xlsx")
  print(xlsx_save_name)
  data_to_save=get(save_name)
  write_xlsx(data_to_save,xlsx_save_name) #creating a table of the retrieved data 
}
write_xlsx(data_overall_start1953, "data_overall_start1953.xlsx")

```

```{r Reload data for analysis, include=FALSE}


data_overall_start1953 <- as.data.frame( read_excel("data_overall_start1953.xlsx"))
approval_q <- as.data.frame(read_excel("approval_q.xlsx"))

write_xlsx(data_overall_start1953, "data_overall_start1953.xlsx")
approval_q <- as.data.frame(read_excel("approval_q.xlsx"))


```


```{r Principle component analysis for graphical depiction, include=FALSE}

#In this section, the Principle Component Analysis for exploring the dataset of economic variables is estimated.
#The first six components that describe close to 100% of the variation present in the data set are then used
#for further analysis. To label them according to their loadings, the 1000 variable names with the highest absolute 
#loading are used in a KNN to label the components. 

#Estimation of Principal component anylsis with the goal to reduce the dimensionality of the data-->describe the data of economic variables used for the empirical analysis.

logLik.kmeans <- function(object) structure(
     -object$tot.withinss/2,
     df = nrow(object$centers)*ncol(object$centers),
     nobs = length(object$cluster))

delete=c()
for (i in 1:ncol(data_overall_start1953)){
  variance=sd(data_overall_start1953[,i])
  print(c(i,variance))
  if(variance==0){delete[i]=i}
  else{next}
}
delete=na.omit(delete)
data_principal_components = data_overall_start1953[,-delete]
data_principal_components$Dates=NULL
data_principal_components=scale(data_principal_components)
sum(is.na(data_principal_components))
corr_matrix <- cor(data_principal_components)
data.pca <- princomp(corr_matrix)
sumpca=summary(data.pca)
importance=cumsum(sumpca$sdev^2 / sum(sumpca$sdev^2))

loadings_1 = as.data.frame(sort(data.pca$loadings[,1],decreasing = T)[1:1000])
loadings_2= as.data.frame(sort(data.pca$loadings[,2],decreasing = T)[1:1000])
loadings_3= as.data.frame(sort(data.pca$loadings[,3],decreasing = T)[1:1000])
loadings_4= as.data.frame(sort(data.pca$loadings[,4],decreasing = T)[1:1000])
loadings_5= as.data.frame(sort(data.pca$loadings[,5],decreasing = T)[1:1000])
loadings_6= as.data.frame(sort(data.pca$loadings[,6],decreasing = T)[1:1000])



#Which topics dominate the first six compnents: Can they be characterized by a specific economic domain?


#Cluster the description of the components and try to find a label for each group using clustering methods. Optimal k is found by cross-validation. The largest clust of each component gives the label for each component.
library(quanteda)
findcomptopics=function(component,graphname){
sentences=c(rownames(component))
sentences=tolower(sentences)
sentences=gsub(x=sentences,pattern = "[[:punct:]]",replacement = " ")
stop=c(tm::stopwords(kind="en"),"from","level","index","funds","value","asset","levels","total","united","states","market","fund","paper","amlf","mutual","urban","city","average","net","real","bonds","backed","excluding","rest","world","revaluation","private","foreign","life","held","one","four","family","casualty","car","term","sectors","issued","fwtw","care","items","cbsa","issued","gse","pools","financial","long","NA","imf","gross","direct","time","defined","percent","mae","fannie","na","abroad","rate","end","basis","strctures","include","agreement","agreements","operations","ima","reverse","closed","residual","current","structures","local","yrs","discrepancies","instrument","men","women","mortgages","producer","products","farm","corporate","depository","chartered","companies","domestic","securities","enterprises","sponsored","federal","transactions","liability","commodity","nonfinancial","residential","nonprofit","nonfinancial","mortgage","consumers","based","peak","trough","period","weeks","indicators","nber","luber","apparel","wood","including","multifamily","business")
sentences = removeWords(sentences, words = stop)
corpus=Corpus(VectorSource(sentences))
tdm = TermDocumentMatrix(corpus,control = list(minWordLength=c(1,Inf)))
t=removeSparseTerms(tdm,sparse=0.98)
m=as.matrix(t)
freq = rowSums(m)
distance=dist(scale(m))
sumofsquares=c()
count=0
m1=t(m)
for (k in seq(2,10,1)) {
kc=kmeans(m1,k)
kc
count=count+1
sumofsquares[count]=kc$tot.withinss
print(c(count,sumofsquares[count]))
}
print(sumofsquares)
print(c(which.min(sumofsquares),sumofsquares[which.min(sumofsquares)]))
kc=kmeans(m1,(which.min(sumofsquares)+1))
print((which.min(sumofsquares)+1))
result_cluster = data.frame(cbind(sentences,as.numeric(kc$cluster)))
result_cluster$V2=as.numeric(result_cluster$V2)
group1=result_cluster$sentences[result_cluster$V2==which.max(kc$size)]
corpus_group1=Corpus(VectorSource(group1))
tdm = TermDocumentMatrix(corpus_group1,control = list(minWordLength=c(1,Inf)))
t=removeSparseTerms(tdm,sparse=0.98)
m=as.matrix(t)
freq = rowSums(m)
freq=sort(freq,decreasing = T)
data_df <- data.frame(Category = names(freq), Frequency = freq)
data_df <- data_df[order(-data_df$Frequency), ]
plot1 = ggplot(data = data_df[1:10,], aes(x = factor(Category, levels = data_df$Category), y = Frequency)) +
  geom_bar(stat = "identity", fill = "steelblue") +
    labs(x = "Category", y = "Frequency") +
    scale_x_discrete(labels = data_df$Category) +  # Set labels for x-axis
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))+  theme_classic() +  # Apply classic theme
  theme(panel.background = element_rect(fill = "white")) +  # Set white background
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggsave(graphname, plot = plot1)
print(k)
print(plot1)
}
set.seed(42)
findcomptopics(loadings_1,"plot_loadings_1.png")
findcomptopics(loadings_2,"plot_loadings_2.png")
findcomptopics(loadings_3,"plot_loadings_3.png")
findcomptopics(loadings_4,"plot_loadings_4.png")
findcomptopics(loadings_5,"plot_loadings_5.png")
findcomptopics(loadings_6,"plot_loadings_6.png")


complete_components <- data_principal_components %*% data.pca$loadings

x1 <- complete_components[,1]
x2 <- complete_components[,2]
x3 <- complete_components[,3]
x4 <- complete_components[,4]
x5 <- complete_components[,5]
x6 <- complete_components[,6]
z <- approval_q$Approval[13:292]
PC1_names=c("PC1: Price, Security, Unemployment, Money")
PC2_names=c("PC2: Organizations, Households, Pensions, Credit")
PC3_names=c("PC3: Investment, Pensions, Estate, Equities")
PC4_names=c("PC4: Price, Households, Organizations, Deposits")
PC5_names=c("PC5: Deposits, Institutions, Unemployment, Investment")
PC6_names=c("PC6: Price, Government, Deposits, Equities")
centers=rbind(PC1_names,PC2_names,PC3_names,PC4_names,PC5_names,PC6_names)

l1=as.data.frame(data.pca$loadings[,1],decreasing = T)
l2=as.data.frame(data.pca$loadings[,2],decreasing = T)
l3=as.data.frame(data.pca$loadings[,3],decreasing = T)
l4=as.data.frame(data.pca$loadings[,4],decreasing = T)
l5=as.data.frame(data.pca$loadings[,5],decreasing = T)
l6=as.data.frame(data.pca$loadings[,6],decreasing = T)


sumofsquares=c()
sentences=c(rownames(data.pca$loadings))
sentences=tolower(sentences)
sentences=gsub(x=sentences,pattern = "[[:punct:]]",replacement = " ")
stop=c(tm::stopwords(kind="en"),"from","level","index","funds","value","asset","levels","total","united","states","market","fund","paper","amlf","mutual","urban","city","average","net","real","bonds","backed","excluding","rest","world","revaluation","private","foreign","life","held","one","four","family","casualty","car","term","sectors","issued","fwtw","care","items","cbsa","issued","gse","pools","financial","long","NA","imf","gross","direct","time","defined","percent","mae","fannie","na","abroad","rate","end","basis","strctures","include","agreement","agreements","operations","ima","reverse","closed","residual","current","structures","local","yrs","discrepancies","instrument","men","women","mortgages","producer","products","farm","corporate","depository","chartered","companies","domestic","securities","enterprises","sponsored","federal","transactions","liability","commodity","nonfinancial","residential","nonprofit","nonfinancial","mortgage","consumers","based","peak","trough","period","weeks","indicators","nber","luber","apparel","wood","including","multifamily","business")
sentences = removeWords(sentences, words = stop)
corpus=Corpus(VectorSource(sentences))
tdm = TermDocumentMatrix(corpus,control = list(minWordLength=c(1,Inf)))
t=removeSparseTerms(tdm,sparse=0.98)
m=as.matrix(t)
freq = rowSums(m)
barplot(freq,las=2)
distance=dist(scale(m))
m1=t(m)
for (k in seq(2,15,1)) {
k=k
kc=kmeans(m1,k)
kc
sumofsquares[k]=kc$tot.withinss
print(c(k,sumofsquares[k]))
}
print(c(which.min(sumofsquares),sumofsquares[which.min(sumofsquares)]))
set.seed(1)
kc=kmeans(m1,(which.min(sumofsquares)+1))
result_cluster = data.frame(cbind(sentences,as.numeric(kc$cluster)))
result_cluster$V2=as.numeric(result_cluster$V2)
labels=c()
for (i in 1:(which.min(sumofsquares)+1)) {
group1=result_cluster$sentences[result_cluster$V2==i]
corpus_group1=Corpus(VectorSource(group1))
tdm = TermDocumentMatrix(corpus_group1,control = list(minWordLength=c(1,Inf)))
m=as.matrix(tdm)
freq = rowSums(m)
labels[i]=paste0(names(sort(freq,decreasing = T)[1]))
}

givelabels=data.frame(names(data.pca$loadings[,1]),kc$cluster)

labelling=c()
for (i in 1:length(givelabels$kc.cluster)) {
  if(givelabels$kc.cluster[i]==1){labelling[i]=labels[1]}
  else if(givelabels$kc.cluster[i]==2){labelling[i]=labels[2]}
  else if(givelabels$kc.cluster[i]==3){labelling[i]=labels[3]}
  else if(givelabels$kc.cluster[i]==4){labelling[i]=labels[4]}
  else if(givelabels$kc.cluster[i]==5){labelling[i]=labels[5]}
  else if(givelabels$kc.cluster[i]==6){labelling[i]=labels[6]}
  else if(givelabels$kc.cluster[i]==7){labelling[i]=labels[7]}
  else if(givelabels$kc.cluster[i]==8){labelling[i]=labels[8]}
  else if(givelabels$kc.cluster[i]==9){labelling[i]=labels[9]}
  else if(givelabels$kc.cluster[i]==10){labelling[i]=labels[10]}
  else if(givelabels$kc.cluster[i]==11){labelling[i]=labels[11]}
  else if(givelabels$kc.cluster[i]==12){labelling[i]=labels[12]}
  else if(givelabels$kc.cluster[i]==13){labelling[i]=labels[13]}
  else if(givelabels$kc.cluster[i]==14){labelling[i]=labels[14]}
  else if(givelabels$kc.cluster[i]==15){labelling[i]=labels[15]}
  else if(givelabels$kc.cluster[i]==16){labelling[i]=labels[16]}
  else if(givelabels$kc.cluster[i]==17){labelling[i]=labels[17]}
  else if(givelabels$kc.cluster[i]==18){labelling[i]=labels[18]}
  else if(givelabels$kc.cluster[i]==19){labelling[i]=labels[19]}
  else if(givelabels$kc.cluster[i]==20){labelling[i]=labels[20]}
  }


dataplot=data.frame(cbind(l1,l2,l3,l4,l5,l6,labelling))
colnames(dataplot)=c("l1","l2","l3","l4","l5","l6","labelling")



```

```{r Graph data using results of Principal Component Analysis, include=T}

#In this code section, the results of the Principle Component Analysis and the KNN are 
#graphically depicted combined and separately. Furthermore, the first six Principle Components 
#are used for exploring their relationship with presidential approval. However, not all results
#were included in the study.

#Plot the results of the Principal compoent analysis
fviz_pca_var(data.pca, col.var = "black")
fviz_cos2(data.pca, choice = "var", axes = 1:2)

#Percentage of explained variance:
noncumvar = fviz_eig(data.pca, addlabels = TRUE,main = "Percentage of Explained Variance",xlab = "Principal Components",ggtheme = theme_classic(),barcolor = "steelblue",barfill = "steelblue",ylab = "Percentage of explained variance" )
ggsave("plotpcanoncumvar.png", plot = noncumvar,width = 12,height =8,device = "png")

#Cumulated percentage of explaned variance:
data <- data.frame(Principal_Components = 1:2139, Importance = importance)
data_subset <- data[data$Principal_Components <= 10, ]
p <- ggplot(data_subset, aes(x = factor(Principal_Components), y = Importance)) +
  geom_bar(stat = "identity", width = 0.7, fill = "steelblue") +
  labs(x = "Principal Components", y = "Cumulated Percentage of Explained Variance", title = "Cumulated Percentage of Explained Variance") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
max_values <- data_subset %>% group_by(Principal_Components) %>% summarize(Max_Importance = max(Importance))
p <- p +
  geom_segment(data = max_values, aes(x = as.numeric(Principal_Components), xend = lead(as.numeric(Principal_Components)), y = Max_Importance, yend = lead(Max_Importance)), size = 1) +
  geom_text(data = max_values, aes(x = as.numeric(Principal_Components) + 0.1, y = Max_Importance + 0.03, label = round(Max_Importance, 2)), vjust = -0.5)
print(p)
ggsave("plotpcacumvar.png", plot = p,width = 12,height =8,device = "png")


#Graphing the variables related to 8 topics for each loading --> how important are the different topics for the different loadings:
library(ggmulti)
# parallel axes plot
for (i in 1:length(unique(dataplot$labelling))) {
  namefreq1=paste0("plot_",unique(dataplot$labelling)[i],"straight",".png")
  namefreq2=paste0("plot_",unique(dataplot$labelling)[i],"radial",".png")
  title_name=paste("Signifiance of variables related to topic:",toupper(unique(dataplot$labelling)[i]),"for the loadings of the Principal Components 1-6")
  plot1=ggplot(dataplot[dataplot$labelling==unique(dataplot$labelling)[i],],
       mapping = aes(
       Loading1=l1,
       Loading2=l2,
       Loading3=l3,
       Loading4=l4,
       Loading5=l5,
       Loading6=l6),
        color=factor(labelling)) +
  geom_path(alpha = 0.6)  +scale_shape_manual(values = 7)+theme_bw()+
     labs(title = title_name)+
  coord_serialaxes() -> p
  p
ggsave(namefreq1, plot = plot1,width = 14,height =8 )


p$coordinates$axes.layout <- "radial"
ggsave(namefreq2, plot = plot1,width = 14,height =8 )
print(p)
}



#Relationship between the different principal components and approval --> Linear regression and kernel regression
data <- data.frame(x1, x2, x3, x4, x5, x6, approval_q$Approval[13:nrow(approval_q)])
colnames(data)=c(PC1_names,PC2_names,PC3_names,PC4_names,PC5_names,PC6_names,"data[,7]")
p <- ggplot(data, aes(x = data[,1], y = data[,7], color = "PC1: Price, Security, Unemployment, Money")) +
  geom_point(data=data,aes(x=data[,1],y=data[,7]),size = 2)+
  geom_smooth(method = "loess", se = FALSE, color = "black", span = 0.5)
p <- p + 
  geom_point(data = data, aes(x = data[,2], y = data[,7], color = "PC2: Organizations, Households, Pensions, Credit"), size = 2) +
  geom_smooth(data = data, method = "loess", se = FALSE, color = "black", span = 0.5)

p <- p + 
  geom_point(data = data, aes(x = data[,3], y = data[,7], color = "PC3: Investment, Pensions, Estate, Equities"), size = 2) +
  geom_smooth(data = data, method = "loess", se = FALSE, color = "black", span = 0.5)

p <- p + 
  geom_point(data = data, aes(x = data[,4], y = data[,7], color = "PC4: Price, Households, Organizations, Deposits"), size = 2) +
  geom_smooth(data = data, method = "loess", se = FALSE, color = "black", span = 0.5)

p <- p + 
  geom_point(data = data, aes(x = data[,5], y = data[,7], color = "PC5: Deposits, Institutions, Unemployment, Investment"), size = 2) +
  geom_smooth(data = data, method = "loess", se = FALSE, color = "black", span = 0.5)

p <- p + 
  geom_point(data = data, aes(x = data[,6], y = data[,7], color = "PC6: Price, Government, Deposits, Equities"), size = 2) +
  geom_smooth(data = data, method = "loess", se = FALSE, color = "black", span = 0.5)

p <- p +
  geom_smooth(method = "lm", se = FALSE, color = "blue", formula = y ~ x) +
  geom_smooth(data = data, method = "lm", se = FALSE, color = "green", formula = y ~ x) +
  geom_smooth(data = data, method = "lm", se = FALSE, color = "red", formula = y ~ x) +
  geom_smooth(data = data, method = "lm", se = FALSE, color = "purple", formula = y ~ x) +
  geom_smooth(data = data, method = "lm", se = FALSE, color = "orange", formula = y ~ x) +
  geom_smooth(data = data, method = "lm", se = FALSE, color = "brown", formula = y ~ x)

p <- p +
  labs(x = "PCs", y = "Approval", color = NULL,title="Relationship between Presidential Approval and Economic Determinants based on Principle Component Analysis",subtitle = "1953-2023") +
  scale_color_manual(values = c("PC1: Price, Security, Unemployment, Money" = "blue", "PC2: Organizations, Households, Pensions, Credit" = "green", "PC3: Investment, Pensions, Estate, Equities" = "red", "PC4: Price, Households, Organizations, Deposits" = "purple", "PC5: Deposits, Institutions, Unemployment, Investment" = "orange", "PC6: Price, Government, Deposits, Equities" = "brown")) +
  theme_bw()
print(p)
ggsave("Approval_PCas.png", plot = p,width = 16,height =10 )


#Same relationship between approval and the different pcas for each PCA separately
library(reshape2)
data_melted <- melt(data, id.vars = "data[,7]", variable.name = "x_variable", value.name = "x_value")
colnames(data_melted)=c("Approval","x_variable","x_value")
p <- ggplot(data_melted, aes(x = x_value, y = Approval )) +
  geom_point(size = 2) +
  geom_smooth(method = "loess", se = FALSE, color = "red", span = 0.5) +
  labs(x = "PCs", y = "Approval",title="Relationship of Presidential Approval and Principal Components of Economic Variables") +
  facet_grid(. ~ x_variable, scales = "free_x") +
  theme_classic()
p <- p +
  geom_smooth(method = "lm", se = FALSE, color = "blue", formula = y ~ x)

# Show the plot
print(p)
ggsave("Approval_PCas_overall.png", plot = p,width = 46,height =12 )

regexplo = (lm(data$`data[,7]` ~ data$`PC1: Price, Security, Unemployment, Money`+data$`PC2: Organizations, Households, Pensions, Credit`+data$`PC3: Investment, Pensions, Estate, Equities`+data$`PC4: Price, Households, Organizations, Deposits`+data$`PC5: Deposits, Institutions, Unemployment, Investment`+data$`PC6: Price, Government, Deposits, Equities`))
names(regexplo$coefficients) = c("Intercept","PC1","PC2","PC3","PC4","PC5","PC6")

stargazer(centers,summary = F,rownames = F)
stargazer(regexplo)

```



```{r Stationarity analysis of independent variables=T}

#In this code section, the economic variables are tested for stationarity using both the
#KPSS and ADF test. 
#Due to the high-dimensionality, the results cannot be reported in tables, but are 
#reported in a heatmap.
#The procedure is repeated until all (differenced) variables are found to be stationary.
######################LEVELS#####################################

x=as.data.frame(data_principal_components)

num_variables <- ncol(x)
kpss_results_with_trend <- vector("numeric", length = num_variables)
kpss_results_without_trend <- vector("numeric", length = num_variables)
adf_results_with_trend <- vector("numeric", length = num_variables)
adf_results_without_trend <- vector("numeric", length = num_variables)
adf_cval_without_trend=vector("numeric", length = num_variables)
adf_cval_with_trend=vector("numeric", length = num_variables)
  
for (i in 1:num_variables) {
  variable <- x[, i]
  print(i)
  # KPSS Test with trend
  kpss_stat_with_trend <- kpss.test(variable, null = "Trend")$p.value
  kpss_results_with_trend[i] <- kpss_stat_with_trend
  
  # KPSS Test without trend
  kpss_stat_without_trend <- kpss.test(variable, null = "Level")$p.value
  kpss_results_without_trend[i] <- kpss_stat_without_trend
  # ADF Test with trend
  adf_stat_with_trend <- abs(attr(ur.df(variable,type = "trend",selectlags = "BIC"),"teststat"))[1]
  adf_results_with_trend[i] <- adf_stat_with_trend
  adf_cval_with_trend[i]=abs(attr(ur.df(variable,type = "trend",selectlags = "BIC"),"cval"))[1,2]
  
  # ADF Test without trend
  adf_stat_without_trend <- abs(attr(ur.df(variable,type = "none",selectlags = "BIC"),"teststat"))[1]
  adf_results_without_trend[i] <- adf_stat_without_trend
  adf_cval_without_trend[i]=abs(attr(ur.df(variable,type = "none",selectlags = "BIC"),"cval"))[2]
}

kpss_decision_with_trend <- vector("character", length = num_variables)
kpss_decision_without_trend <- vector("character", length = num_variables)

for (i in 1:num_variables) {
  print(i)
  # Decide based on KPSS Test with trend
  if (kpss_results_with_trend[i] < 0.05) {
    kpss_decision_with_trend[i] <- "I(1)"
  } else {
    kpss_decision_with_trend[i] <- "I(0)"
  }
  
  # Decide based on KPSS Test without trend
  if (kpss_results_without_trend[i] < 0.05) {
    kpss_decision_without_trend[i] <- "I(1)"
  } else {
    kpss_decision_without_trend[i] <- "I(0)"
  }
}

adf_decision_with_trend <- vector("character", length = num_variables)
adf_decision_without_trend <- vector("character", length = num_variables)

for (i in 1:num_variables) {
  print(i)
  # Decide based on ADF Test with trend
  if (adf_results_with_trend[i] > adf_cval_with_trend[i]) {
    adf_decision_with_trend[i] <- "I(0)"
  } else {
    adf_decision_with_trend[i] <- "I(1)"
  }
  
  # Decide based on ADF Test without trend
  if (adf_results_without_trend[i] > adf_cval_without_trend[i]) {
    adf_decision_without_trend[i] <- "I(0)"
  } else {
    adf_decision_without_trend[i] <- "I(1)"
  }
}

decisions_matrix <- matrix(NA, nrow = 4, ncol = num_variables)
decisions_matrix[1, ] <- as.numeric(kpss_decision_without_trend == "I(1)")
decisions_matrix[2, ] <- as.numeric(kpss_decision_with_trend == "I(1)")
decisions_matrix[3, ] <- as.numeric(adf_decision_without_trend == "I(1)")
decisions_matrix[4, ] <- as.numeric(adf_decision_with_trend == "I(1)")


decisions_matrix=data.frame(decisions_matrix)
decisions_matrix$Test = c("KPSS (No Trend)", "KPSS (With Trend)", "ADF (No Trend)", "ADF (With Trend)")

mdf <- gather(decisions_matrix, -Test, key="key", value="count")
plot_stat_levels <- ggplot(mdf, aes(x=key, y=Test, fill=count)) +
     geom_tile() +
     scale_fill_gradient(high="red2", low="green")+labs(title="Test for Stationarity of Economic Variables",subtitle="1953-2023, in Levels")
print(plot_stat_levels)

decisions_matrix <- matrix(NA, nrow = 4, ncol = num_variables)
decisions_matrix[1, ] <- as.numeric(kpss_decision_without_trend == "I(1)")
decisions_matrix[2, ] <- as.numeric(kpss_decision_with_trend == "I(1)")
decisions_matrix[3, ] <- as.numeric(adf_decision_without_trend == "I(1)")
decisions_matrix[4, ] <- as.numeric(adf_decision_with_trend == "I(1)")

df=data.frame(decisions_matrix)
rownames(df)== c("KPSS (No Trend)", "KPSS (With Trend)", "ADF (No Trend)", "ADF (With Trend)")

df$Nonstationary = c(sum(df[1,]==1),sum(df[2,]==1),sum(df[3,]==1),sum(df[4,]==1))
df$Stationary = c(sum(df[1,]==0),sum(df[2,]==0),sum(df[3,]==0),sum(df[4,]==0))

df %>%
    rownames_to_column('Test') %>% 
    select(Test, "Nonstationary", "Stationary") %>% 
    pivot_longer(cols = -Test) %>% 
    ggplot(aes(x = Test, y = value, fill = name)) + 
        geom_bar(stat = 'identity') + 
        theme_bw()+ scale_x_discrete(labels=c("KPSS (No Trend)", "KPSS (With Trend)", "ADF (No Trend)", "ADF (With Trend)"))+ 
  scale_fill_manual(values=c("red","green")) +labs(title="Test for Stationarity of Economic Variables",subtitle="1953-2023, in Levels")
ggsave("levels.png", plot = plot_stat_levels,width = 14,height =8 )




######################1st DIFF###################################

# Function to perform the operations based on the decision matrix while maintaining column order
manipulate_data <- function(df, decision_matrix) {
  result <- list()
  for (i in 1:ncol(decisions_matrix)) {
    if (decision_matrix[1,i] == 1) {
      result[[i]] <- c(NA, diff(df[, i]))
    } else {
      result[[i]] <- df[, i]
    }
  }
  manipulated_df <- do.call(cbind, result)
  return(manipulated_df)
}

# Applying the manipulation function to DataFrame 'x'
manipulated_x <- manipulate_data(x, decisions_matrix)

x = manipulated_x
x=na.omit(x)
#x=as.data.frame((data_principal_components))
#
#for (i in 1:ncol(x)) {
#  x[,i]=c(NA,diff(x[,i]))
##}
#x=na.omit(x)

num_variables <- ncol(x)
kpss_results_with_trend <- vector("numeric", length = num_variables)
kpss_results_without_trend <- vector("numeric", length = num_variables)
adf_results_with_trend <- vector("numeric", length = num_variables)
adf_results_without_trend <- vector("numeric", length = num_variables)
adf_cval_without_trend=vector("numeric", length = num_variables)
adf_cval_with_trend=vector("numeric", length = num_variables)
  
for (i in 1:num_variables) {
  variable <- x[, i]
  print(i)
  # KPSS Test with trend
  kpss_stat_with_trend <- kpss.test(variable, null = "Trend")$p.value
  kpss_results_with_trend[i] <- kpss_stat_with_trend
  
  # KPSS Test without trend
  kpss_stat_without_trend <- kpss.test(variable, null = "Level")$p.value
  kpss_results_without_trend[i] <- kpss_stat_without_trend
  # ADF Test with trend
  adf_stat_with_trend <- abs(attr(ur.df(variable,type = "trend",selectlags = "BIC"),"teststat"))[1]
  adf_results_with_trend[i] <- adf_stat_with_trend
  adf_cval_with_trend[i]=abs(attr(ur.df(variable,type = "trend",selectlags = "BIC"),"cval"))[1,2]
  
  # ADF Test without trend
  adf_stat_without_trend <- abs(attr(ur.df(variable,type = "none",selectlags = "BIC"),"teststat"))[1]
  adf_results_without_trend[i] <- adf_stat_without_trend
  adf_cval_without_trend[i]=abs(attr(ur.df(variable,type = "none",selectlags = "BIC"),"cval"))[2]
}

kpss_decision_with_trend <- vector("character", length = num_variables)
kpss_decision_without_trend <- vector("character", length = num_variables)

for (i in 1:num_variables) {
  print(i)
  # Decide based on KPSS Test with trend
  if (kpss_results_with_trend[i] < 0.05) {
    kpss_decision_with_trend[i] <- "I(1)"
  } else {
    kpss_decision_with_trend[i] <- "I(0)"
  }
  
  # Decide based on KPSS Test without trend
  if (kpss_results_without_trend[i] < 0.05) {
    kpss_decision_without_trend[i] <- "I(1)"
  } else {
    kpss_decision_without_trend[i] <- "I(0)"
  }
}

adf_decision_with_trend <- vector("character", length = num_variables)
adf_decision_without_trend <- vector("character", length = num_variables)

for (i in 1:num_variables) {
  print(i)
  # Decide based on ADF Test with trend
  if (adf_results_with_trend[i] > adf_cval_with_trend[i]) {
    adf_decision_with_trend[i] <- "I(0)"
  } else {
    adf_decision_with_trend[i] <- "I(1)"
  }
  
  # Decide based on ADF Test without trend
  if (adf_results_without_trend[i] > adf_cval_without_trend[i]) {
    adf_decision_without_trend[i] <- "I(0)"
  } else {
    adf_decision_without_trend[i] <- "I(1)"
  }
}

decisions_matrix <- matrix(NA, nrow = 4, ncol = num_variables)
decisions_matrix[1, ] <- as.numeric(kpss_decision_without_trend == "I(1)")
decisions_matrix[2, ] <- as.numeric(kpss_decision_with_trend == "I(1)")
decisions_matrix[3, ] <- as.numeric(adf_decision_without_trend == "I(1)")
decisions_matrix[4, ] <- as.numeric(adf_decision_with_trend == "I(1)")


decisions_matrix=data.frame(decisions_matrix)
decisions_matrix$Test = c("KPSS (No Trend)", "KPSS (With Trend)", "ADF (No Trend)", "ADF (With Trend)")

mdf <- gather(decisions_matrix, -Test, key="key", value="count")
plot_stat_levels <- ggplot(mdf, aes(x=key, y=Test, fill=count)) +
     geom_tile() +
     scale_fill_gradient(high="red2", low="green")+labs(title="Test for Stationarity of Economic Variables",subtitle="1953-2023, 1st Difference")
print(plot_stat_levels)
ggsave("1st_diff.png", plot = plot_stat_levels,width = 14,height =8 )


######################2 DIFF###################################

# Function to perform the operations based on the decision matrix while maintaining column order
manipulate_data <- function(df, decision_matrix) {
  result <- list()
  for (i in 1:ncol(decision_matrix)) {
    if (decision_matrix[1,i] == 1) {
      result[[i]] <- c(NA, diff(df[, i]))
    } else {
      result[[i]] <- df[, i]
    }
  }
  manipulated_df <- do.call(cbind, result)
  return(manipulated_df)
}

# Applying the manipulation function to DataFrame 'x'
decisions_matrix[,2140]=NULL
manipulated_x_2 <- manipulate_data(x, decisions_matrix)

x = manipulated_x_2

x=na.omit(x)

num_variables <- ncol(x)
kpss_results_with_trend <- vector("numeric", length = num_variables)
kpss_results_without_trend <- vector("numeric", length = num_variables)
adf_results_with_trend <- vector("numeric", length = num_variables)
adf_results_without_trend <- vector("numeric", length = num_variables)
adf_cval_without_trend=vector("numeric", length = num_variables)
adf_cval_with_trend=vector("numeric", length = num_variables)
  
for (i in 1:num_variables) {
  variable <- x[, i]
  print(i)
  # KPSS Test with trend
  kpss_stat_with_trend <- kpss.test(variable, null = "Trend")$p.value
  kpss_results_with_trend[i] <- kpss_stat_with_trend
  
  # KPSS Test without trend
  kpss_stat_without_trend <- kpss.test(variable, null = "Level")$p.value
  kpss_results_without_trend[i] <- kpss_stat_without_trend
  # ADF Test with trend
  adf_stat_with_trend <- abs(attr(ur.df(variable,type = "trend",selectlags = "BIC"),"teststat"))[1]
  adf_results_with_trend[i] <- adf_stat_with_trend
  adf_cval_with_trend[i]=abs(attr(ur.df(variable,type = "trend",selectlags = "BIC"),"cval"))[1,2]
  
  # ADF Test without trend
  adf_stat_without_trend <- abs(attr(ur.df(variable,type = "none",selectlags = "BIC"),"teststat"))[1]
  adf_results_without_trend[i] <- adf_stat_without_trend
  adf_cval_without_trend[i]=abs(attr(ur.df(variable,type = "none",selectlags = "BIC"),"cval"))[2]
}

kpss_decision_with_trend <- vector("character", length = num_variables)
kpss_decision_without_trend <- vector("character", length = num_variables)

for (i in 1:num_variables) {
  print(i)
  # Decide based on KPSS Test with trend
  if (kpss_results_with_trend[i] < 0.05) {
    kpss_decision_with_trend[i] <- "I(1)"
  } else {
    kpss_decision_with_trend[i] <- "I(0)"
  }
  
  # Decide based on KPSS Test without trend
  if (kpss_results_without_trend[i] < 0.05) {
    kpss_decision_without_trend[i] <- "I(1)"
  } else {
    kpss_decision_without_trend[i] <- "I(0)"
  }
}

adf_decision_with_trend <- vector("character", length = num_variables)
adf_decision_without_trend <- vector("character", length = num_variables)

for (i in 1:num_variables) {
  print(i)
  # Decide based on ADF Test with trend
  if (adf_results_with_trend[i] > adf_cval_with_trend[i]) {
    adf_decision_with_trend[i] <- "I(0)"
  } else {
    adf_decision_with_trend[i] <- "I(1)"
  }
  
  # Decide based on ADF Test without trend
  if (adf_results_without_trend[i] > adf_cval_without_trend[i]) {
    adf_decision_without_trend[i] <- "I(0)"
  } else {
    adf_decision_without_trend[i] <- "I(1)"
  }
}

decisions_matrix <- matrix(NA, nrow = 4, ncol = num_variables)
decisions_matrix[1, ] <- as.numeric(kpss_decision_without_trend == "I(1)")
decisions_matrix[2, ] <- as.numeric(kpss_decision_with_trend == "I(1)")
decisions_matrix[3, ] <- as.numeric(adf_decision_without_trend == "I(1)")
decisions_matrix[4, ] <- as.numeric(adf_decision_with_trend == "I(1)")


decisions_matrix=data.frame(decisions_matrix)
decisions_matrix$Test = c("KPSS (No Trend)", "KPSS (With Trend)", "ADF (No Trend)", "ADF (With Trend)")

mdf <- gather(decisions_matrix, -Test, key="key", value="count")
plot_stat_levels <- ggplot(mdf, aes(x=key, y=Test, fill=count)) +
     geom_tile() +
     scale_fill_gradient(high="red2", low="green")+labs(title="Test for Stationarity of Economic Variables",subtitle="1953-2023, 2nd Difference")
print(plot_stat_levels)
ggsave("2nd_diff.png", plot = plot_stat_levels,width = 14,height =8 )


```



```{r Introduction, include=T}

#In this code section, the plot and the correlation analysis that is depicted in the introduction is created.
#The graphs based on the monthly frequency of the data are not used for the study. 

data_Intro = data_overall_start1953[,-delete]
Unemployment = data_Intro$Unemployment.Rate
GDP = data_Intro$Gross.Domestic.Product
CPI = data_Intro$Consumer.Price.Index.for.All.Urban.Consumers..All.Items.in.U.S..City.Average
Income = data_Intro$Households.and.Nonprofit.Organizations..Disposable.Personal.Income..Transactions
Taxes = data_Intro$Households.and.Nonprofit.Organizations..Taxes.Including.Social.Security.Contributions.and.Other.Current.Transfers.Made..FSIs...Transactions
Rent=data_Intro$Consumer.Price.Index.for.All.Urban.Consumers..Rent.of.Primary.Residence.in.U.S..City.Average

Approval = (approval_q$Approval[13:nrow(approval_q)])
correlation_data <- data.frame(Unemployment, GDP, CPI, Income,Taxes,Rent ,Approval)
for (i in 1:ncol(correlation_data)) {
  correlation_data[,i]=c(NA,diff(correlation_data[,i]))
}
correlation_data=na.omit(correlation_data)
correlation_matrix <- cor(correlation_data)

melted_corr <- melt(correlation_matrix)
heatmap_intro <- ggplot(melted_corr, aes(Var1, Var2, fill = value, label = round(value, 3))) +
    geom_tile() +
    geom_text(color = "black") +  # Add text annotations
    scale_fill_gradient(low = "deepskyblue", high = "red3") +
    labs(title = "Correlation Heatmap") +
    theme_classic() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    coord_fixed()
ggsave("heatmap_Intro.png", plot = heatmap_intro,width = 6,height =6,device = "png")

stargazer(correlation_matrix,title = "main",header = F)

#################################BIDEN MONTHLY###########################################################

approvalm_Biden = approval_m[approval_m$Biden==1,]
plot_Biden=ggplot() +
  geom_line(aes(x = approvalm_Biden$Date, y =approvalm_Biden$Approval))+
    theme_classic() +
  labs(title="Gallup Presidential approval_q Rating",subtitle="1950-2023")+xlab("Time")+ylab("Percent")

plot_Biden <- ggplot() +
    geom_line(aes(x = approvalm_Biden$Date, y = approvalm_Biden$Approval)) +
    theme_classic() +
    labs(title = "Biden Approval Rating", subtitle = "Gallup; 2021-2023; Monthly; in %") +
    xlab("Time") +
    ylab("Percent")

event_dates <- c("2021-03-01", "2022-08-01", "2021-08-01", "2022-02-01", "2022-06-01", "2023-04-01",
                 "2021-11-01","2023-02-01", "2023-03-01", "2022-11-01")
event_names <- c("American Rescue Plan", "Build Back Better Plan, Chips and Science Act, Inflation Reduction Act", "Afghanistan withdrawal",
                 "Russian invasion of Ukraine", "Inflation peak at 9.1%",
                 "Unemployment minimum at 3.4%",
                 "Infrastructure, Investment, and Jobs Act", 
                 "Chinese spy baloon shoot down", "Gas price peak", "Midterms")

event_colors <- c("red", "blue", "green", "purple", "cyan", "magenta", "black", "pink", "gold", "gray")

events_data <- data.frame(event_dates = as.Date(event_dates), event_names, event_colors)

plot_Intro  = plot_Biden +
    geom_vline(data = events_data, aes(xintercept = as.numeric(event_dates), color = str_wrap(event_names,40)),
               linetype = "dashed", show.legend = T,size=1.25) +
    scale_color_manual(values = events_data$event_colors) +
    labs(color = "Events")  # Legend label for events


ggsave("plot_Intro.png", plot = plot_Intro,width = 10,height =6,device = "png")

#################################BIDEN QUARTERLY###########################################################

approvalq_Biden = approval_q[(approval_q$Biden == 1 | approval_q$Trump ==1),] 
plot__Biden=ggplot() +
  geom_line(aes(x = approvalq_Biden$`End Date`, y =approvalq_Biden$Approval))+
    theme_classic() +
  labs(title="Gallup Presidential approval_q Rating",subtitle="1950-2023")+xlab("Time")+ylab("Percent")+
    geom_rect(aes(xmin=Begin_q[11],xmax=End_q[11]
            ,fill="red"),ymin=0,ymax=100, size=0.5, alpha=0.1) +
    geom_rect(aes(xmin=Begin_q[12],xmax=End_q[12]
            ,fill="yellow"),ymin=0,ymax=100, size=0.5, alpha=0.1) +
  scale_fill_identity(labels = c(
      red = unique(approval$President)[11], yellow = unique(approval$President)[12]
    ),guide="legend",name=NULL,aesthetics = "fill")
knitr::opts_chunk$set(echo = TRUE)



```

```{r Reload data for analysis, include=FALSE}

#In this section, security copies of the datas are exported and re-imported.

write_xlsx(data.frame(manipulated_x_2), "manipulated_x_2.xlsx")
manipulated_x_2 <- as.data.frame(read_excel("manipulated_x_2.xlsx"))

write_xlsx(data.frame(data_principal_components), "data_principal_components.xlsx")
data_principal_components <- as.data.frame(read_excel("data_principal_components.xlsx"))

```


```{r Variable Selection, include=T}

#In this section, the automated variable selection-based on the Elastic-Net, SCAD,
#and SPECS penalized approach-is conducted. Due to the computational extensiveness,
#the models are estimated on the HPC. The results are stored in excel files, downloaded, 
#and re-imported into R. Subsequently, the resulting RMSEs are shown in heatmaps that also 
#indicate the minimum RMSE of each method. 
#ENET and SCAD are based on the dataset in which all variables are stationary, SPECS is based 
#on the original dataset containing stationary and nonstationary variables.
#All models are evaluated based on their RMSE for a 70/30 training/testing data split.
#As for the PCA, the data is scaled before being used in the variable selection process.

################STATIONARY ENET#################################

########################HPC#####################################
################################################################
################################################################
library(ncvreg)
set.seed(3)
#Y in Levels
y=approval_q$Approval[15:nrow(approval_q)]
#y in Lags
y=diff(approval_q$Approval[14:nrow(approval_q)])
#y=y[2:length(y)]
x = manipulated_x_2
x=na.omit(x)
x = data.frame(cbind(x,lag(x,n = 1L),lag(x,n = 2L),lag(x,n = 3L),lag(x,n = 4L),lag(y,n = 1L),lag(y,n = 2L),lag(y,n = 3L),lag(y,n = 4L)))
colnames(x)=c(rep(colnames(data_principal_components), times = 5),"Lag Y_1","Lag Y_2","Lag Y_3","Lag Y_4")
data = data.frame(cbind(x,y))
data=na.omit(data)



y=approval_q$Approval[15:nrow(approval_q)]
x = manipulated_x_2
x=na.omit(x)
x = data.frame(cbind(x,lag(x,n = 1L),lag(x,n = 2L),lag(x,n = 3L),lag(x,n = 4L),lag(y,n = 1L),lag(y,n = 2L),lag(y,n = 3L),lag(y,n = 4L)))
colnames(x)=c(rep(colnames(data_principal_components), times = 5),"Lag Y_1","Lag Y_2","Lag Y_3","Lag Y_4")
data = data.frame(cbind(x,y))
data=na.omit(data)
x_m = as.matrix(data[,-ncol(data)])
y_m=data[,ncol(data)]
colnames(x_m)=c(rep(colnames(data_principal_components), times = 5),"Lag Y_1","Lag Y_2","Lag Y_3","Lag Y_4")


alpha_seq <- seq(0, 0.99, by = 0.01)  # Alpha values from 0 to 0.99 (excluding 1)
lambda_seq <- seq(0, 1, by = 0.01)  # Lambda value of 0.3 (you can add more values)
param_grid <- expand.grid(alpha = alpha_seq, lambda = lambda_seq)
train_begin=floor(0.7*nrow(x_m))
test_size=nrow(x_m)-(floor(0.7*nrow(x_m))+1)

RMSE=c()
parameters_tot=matrix(NA,ncol=length(param_grid$alpha),nrow=ncol(x_m))
for (j in 1:nrow(param_grid)) {
  #Initialize alpha and lambda values
  alpha <- param_grid$alpha[j]
  lambda <- param_grid$lambda[j]
  print(c(alpha,lambda))
  
  #Create new vector and matrices that are only used in the i for loop.
  y_pred <- c()
  parameters_estimated <- matrix(NA,ncol=test_size+1,nrow=ncol(x_m))
  
    for (i in 1:(test_size+1)) {
  
      #One step ahead training length
      train_length=train_begin+i-1

      #Train model with combinations of alpha,lambdas, and store the one step ahead prediction.
      reg=glmnet(x_m[1:train_length,],y_m[1:train_length],alpha = alpha,lambda = lambda)
      y_pred[i] = predict(reg,x_m[train_length+1,])
      parameters_estimated[,i]=as.matrix(reg$beta)[,1]
      print(c(i,train_length,train_length+1))
    }
  #Calculate the RMSE and store the mean values of the estimated parameters 
  RMSE[j]=sqrt(sum((y_pred-y_m[(train_begin+1):length(y_m)]))^2)
  parameters_tot[,j]=rowMeans(parameters_estimated)
  print(RMSE[j])
}

#Is the maximum grid value shrinking all parameters to zero?
test = apply(parameters_tot, 2, function(c)sum(c!=0))



########################HPC END#################################
################################################################
################################################################

######################Import ENET HPC Results###################

RMSE <- c()
for (i in 1:1010) {
  filename <- paste0("rmse_", i, ".xlsx")
  path <- file.path("C:/Users/Yussuf Schwarz/OneDrive/Desktop/UniVWL/Projekt/ENET Results", filename)  
  rmse_df <- data.frame(read_xlsx(path))
  RMSE <- c(RMSE, rmse_df$RMSE)
}

#parameter_tot <- data.frame(matrix(nrow=10699,ncol=1))
#for (i in 1:1010) {
#  filename <- paste0("parameter_", i, ".xlsx")
#  path <- file.path("C:/Users/Yussuf Schwarz/OneDrive/Desktop/UniVWL/Projekt/ENET Results", filename)  
#  param_df <- data.frame(read_xlsx(path))
#  parameter_tot <- cbind(parameter_tot, param_df)
#}

######################ENET Results#############################

alpha_seq <- seq(0, 0.99, by = 0.01)  # Alpha values from 0 to 0.99 (excluding 1)
lambda_seq <- seq(0, 1, by = 0.01)  # Lambda value of 0.3 (you can add more values)
param_grid <- expand.grid(alpha = alpha_seq, lambda = lambda_seq)
library(ggplot2)
# Assuming you have the vectors RMSE and param_grid
results_df <- data.frame(RMSE = (RMSE), param_grid)

# Find the row with the minimal RMSE
min_RMSE_row <- which.min(RMSE)
min_RMSE_alpha <- param_grid$alpha[min_RMSE_row]
min_RMSE_lambda <- param_grid$lambda[min_RMSE_row]

set.seed(42)
#Select and estimate the optimal model:
param_opt = param_grid[which.min(RMSE),]
reg=glmnet(x_m,y_m,alpha = min_RMSE_alpha,lambda = min_RMSE_lambda)
beta_enet = as.matrix(reg$beta)
order_idx <- order(beta_enet[, "s0"])
beta_enet_ordered <- data.frame(beta_enet[order_idx, ])
beta_enet_ordered$names = rownames(beta_enet_ordered)
beta_enet_ordered = data.frame( beta_enet_ordered[1:12,])


# Create the scatterplot of RMSE depending on Alpha and Lambda
RMSE_alpha_beta_1 = ggplot(results_df, aes(x = alpha, y = lambda, color = RMSE)) +
  geom_point(size = 3) +
  scale_color_gradient(low = "blue", high = "red") +
  labs(x = "Alpha", y = "Lambda", color = "RMSE",title="RMSE of Stationary Elastic Net Regression") +
  theme_minimal() +
  geom_point(data = data.frame(alpha = min_RMSE_alpha, lambda = min_RMSE_lambda),
             aes(x = alpha, y = lambda), color = "green", size = 5) +
  geom_text(aes(x = min_RMSE_alpha, y = min_RMSE_lambda, 
                label = paste("Min RMSE:", round(min(RMSE), 2))), 
            hjust = +0.5, vjust = 1.5, color = "white", size = 4.5)+theme_classic() 
ggsave("RMSE_ENET.png", plot = RMSE_alpha_beta_1,width = 12,height =8 )

#Heatmap of all coefficients:

#parameter_tot = parameter_tot[,-1]
#write.csv(data.frame(parameter_tot),"parameter_tot.csv") #creating a table of the retrieved data
#parameter_tot = read.csv("parameter_tot.csv")
#Zero_coefficients = c()
#for (i in 1:ncol(parameter_tot)) {
#  Zero_coefficients[i] = sum(parameter_tot[,i]==0)
#}
#Nonzero_coefficients = nrow(parameter_tot) - Zero_coefficients
#library(ggplot2)
# Assuming you have the vectors RMSE and param_grid
#results_df <- data.frame(Coefficients = log(Nonzero_coefficients[-1]), param_grid)
# Find the row with the minimal RMSE
#min_RMSE_row <- which.min(RMSE)
#min_RMSE_alpha <- param_grid$alpha[min_RMSE_row]
#min_RMSE_lambda <- param_grid$lambda[min_RMSE_row]
# Create the scatterplot of RMSE depending on Alpha and Lambda
#RMSE_alpha_beta_1 = ggplot(results_df, aes(x = alpha, y = lambda, color = Coefficients)) +
#  geom_point(size = 3) +
#  scale_color_gradient(low = "blue", high = "red") +
#  labs(x = "Alpha", y = "Lambda", color = "RMSE") +
#  theme_minimal() +
#  geom_point(data = data.frame(alpha = min_RMSE_alpha, lambda = min_RMSE_lambda),
#             aes(x = alpha, y = lambda), color = "green", size = 5) +
#  geom_text(aes(x = min_RMSE_alpha, y = min_RMSE_lambda, 
#                label = paste("Min RMSE:", round(min(RMSE), 2))), 
#            hjust = +0.5, vjust = 1.5, color = "white", size = 4.5)+theme_classic() 
#ggsave("Coefficients_ENET.png", plot = RMSE_alpha_beta_1,width = 12,height =8 )



####################################STATIONARY SCAD#######################################################

########################HPC#####################################
################################################################
################################################################

set.seed(42)
# Define alpha, lambda, and gamma sequences
alpha_seq <- seq(0.01, 0.99, by = 0.01)
lambda_seq <- seq(0, 4, by = 0.01)
gamma_seq <- seq(2.1, 4, by = 0.1)

# Create parameter grid
param_grid <- expand.grid(alpha = alpha_seq, lambda = lambda_seq, gamma = gamma_seq)

# Set up data splitting
train_begin <- floor(0.7 * nrow(x_m))
test_size <- nrow(x_m) - (floor(0.7 * nrow(x_m)) + 1)

RMSE <- vector()
parameters_tot <- matrix(NA, ncol = length(param_grid$alpha), nrow = ncol(x_m))

for (j in 1:nrow(param_grid)) {
  # Extract alpha, lambda, and gamma values
  alpha <- param_grid$alpha[j]
  lambda <- param_grid$lambda[j]
  gamma <- param_grid$gamma[j]
  
  # Initialize storage vectors/matrices
  y_pred <- vector()
  parameters_estimated <- matrix(NA, ncol = test_size + 1, nrow = ncol(x_m))
  
  for (i in 1:(test_size + 1)) {
    # One step ahead training length
    train_length <- train_begin + i - 1
    
    # Train model with combinations of alpha, lambdas, gammas using ncvfit
    reg <- ncvfit(x_m[1:train_length, ], y_m[1:train_length],
                  penalty = "SCAD", gamma = gamma, alpha = alpha, lambda = lambda)
    
    # Make predictions using matrix multiplication
    y_pred[i] <- as.vector(reg$beta) %*% as.vector(x_m[train_length + 1, ])
    parameters_estimated[, i] <- as.vector(reg$beta)
  }
  
  # Calculate RMSE and store mean values of estimated parameters
  RMSE[j] <- sqrt(sum((y_pred - y_m[(train_begin + 1):length(y_m)]))^2)
  parameters_tot[, j] <- rowMeans(parameters_estimated)
}

########################HPC END#################################
################################################################
################################################################

######################Import SCAD HPC Results###################

directory_path <- "C:/Users/Yussuf Schwarz/OneDrive/Desktop/UniVWL/Projekt/SCAD Results"
expected_files <- sprintf("rmse_SCAD_%d.xlsx", 1:1980)
existing_files = character(0)
missing_files <- character(0)
for (file_name in expected_files) {
  file_path <- file.path(directory_path, file_name)
  if (!file.exists(file_path)) {
    missing_files <- c(missing_files, file_name)}
    if (file.exists(file_path)) {
    existing_files <- c(existing_files, file_name)
  }
}
library(stringr)
numbers <- str_extract(missing_files, "\\d+")
numbers <- as.numeric(numbers)
print(numbers)

RMSE_length = c()
RMSE_SCAD <- c()
for (i in 1:1980) {
  filename <- paste0("rmse_SCAD_", i, ".xlsx")
  path <- file.path("C:/Users/Yussuf Schwarz/OneDrive/Desktop/UniVWL/Projekt/SCAD Results", filename)  
  rmse_df <- data.frame(read_xlsx(path))
  colnames(rmse_df) = "RMSE"
  RMSE_length[i] = length(rmse_df$RMSE)
  RMSE_SCAD <- c(RMSE_SCAD, rmse_df$RMSE)
}


######################SCAD Results#############################
# Define alpha, lambda, and gamma sequences
alpha_seq <- seq(0.01, 0.99, by = 0.01)
lambda_seq <- seq(0, 4, by = 0.01)
gamma_seq <- seq(2.1, 4, by = 0.1)
param_grid <- expand.grid(alpha = alpha_seq, lambda = lambda_seq, gamma = gamma_seq)
# Assuming you have the vectors RMSE, alpha, lambda, and gamma in results_df
results_df <- data.frame(RMSE = RMSE_SCAD, alpha = param_grid$alpha, lambda = param_grid$lambda, gamma = param_grid$gamma)
# Find the row with the minimal RMSE
min_RMSE_row <- which.min(RMSE_SCAD)
min_RMSE_alpha <- param_grid$alpha[min_RMSE_row]
min_RMSE_lambda <- param_grid$lambda[min_RMSE_row]
min_RMSE_gamma <- param_grid$gamma[min_RMSE_row]

rmse_scad_names = c()
limits_graph_l = c(2.1,2.5,3.0,3.5)
limits_graph_u = c(2.5,3.0,3.5,4.0)
limits_graph = data.frame(cbind(limits_graph_l,limits_graph_u))
for (i in 1:nrow(limits_graph)) {
  print(i)
  results_df_neu = results_df[(results_df$gamma>limits_graph[i,1] & results_df$gamma<limits_graph[i,2]) ,]
  print(c(limits_graph[i,1],limits_graph[i,2]))
  # Create the scatterplot of RMSE depending on Alpha and Lambda with Gamma represented by shape and color
  name_plot = paste0("rmse_plot_gamma",i)
RMSE_alpha_beta_gamma <- ggplot(results_df_neu, aes(x = alpha, y = lambda, color = scale(RMSE))) +
  geom_point(size = 3) +
  scale_color_gradient(low = "blue", high = "red") + labs(title = paste0("Gamma between"," ", limits_graph[i,1]," ","and"," ",limits_graph[i,2]))+
  labs(x = "Alpha", y = "Lambda", color = "RMSE") +
  geom_point(data = data.frame(alpha = min_RMSE_alpha, lambda = min_RMSE_lambda),
             aes(x = alpha, y = lambda), color = "green", size = 5, shape = 8) +
  geom_text(aes(x = min_RMSE_alpha, y = min_RMSE_lambda, 
                label = paste("Min RMSE:", round(min(RMSE), 5))), 
            hjust = +1, vjust = 1.5, color = "white", size = 5.5) +
    theme_classic()  # Adjust legend 
assign(name_plot,RMSE_alpha_beta_gamma, env = .GlobalEnv)
  rmse_scad_names = c(rmse_scad_names,name_plot)
}
library(gridExtra)
library(grid)
tg <- textGrob("RMSE of Stationary SCAD Penalized Regression for different Gamma ranges", gp = gpar(fontsize = 20, fontface = 'plain',fontfamily="Arial"))

RMSE_with_gamma_visual = grid.arrange(rmse_plot_gamma1,rmse_plot_gamma2,rmse_plot_gamma3,rmse_plot_gamma4,nrow=2,ncol=2,top=tg)
ggsave("RMSE_SCAD.png", plot = RMSE_with_gamma_visual, width = 16, height = 8)

#plot3d(results_df$alpha, results_df$lambda, results_df$RMSE, col=factor(results_df$gamma))
#legend3d("topright", legend = factor(results_df$gamma),col=factor(results_df$gamma), pch = 16, cex=1, inset=c(0.02))
#parameters_df <- as.data.frame(parameters_tot)
#parameters_df$row <- 1:nrow(parameters_df)
#parameters_df_long <- gather(parameters_df, key = "column", value = "value", -row)
#parameters_df_long$value_log <- log1p(parameters_df_long$value)
#heatmap_scad <-  ggplot(parameters_df_long, aes(x = column, y = row, fill = value_log)) +
#  geom_tile() +
#  scale_fill_gradient(low = "blue", high = "red",na.value = "black") +
#  labs(title = "Heatmap of Parameter Estimation Values (Log)", x = "Columns", y = "Rows") 





####################################PENALIZED SINGLE EQUATION ERROR CORRECTION#######################################################


########################HPC#####################################
################################################################
################################################################


library(dplyr)
library(tidyr)
library(sparsevar)
#install.packages("C:/Users/Yussuf Schwarz/Downloads/specs_0.1.1.tar.gz", repos = NULL, type = "source")
library(specs)
library(specs)
library(readxl)
library(Matrix)
library(glmnet)
library(dplyr,warn.conflicts = FALSE)
library(tidyr,warn.conflicts = FALSE)
library(ggplot2)
library(writexl)

set.seed(42)  # For reproducibility
run = 2 #run 1-15

manipulated_x_2 <- as.data.frame(read_excel("manipulated_x_2.xlsx"))
data_principal_components <- as.data.frame(read_excel("data_principal_components.xlsx"))
approval_q <- as.data.frame(read_excel("approval_q.xlsx"))


y <- ts(approval_q$Approval[13:nrow(approval_q)])
x <- ts(as.data.frame(data_principal_components))
# Combine y and x into a single data frame or matrix
data <- cbind(y, x)

# Determine the number of rows for the training set (70%)
train_rows <- round(0.7 * nrow(data))

# Split into training and testing sets
train_data <- data[1:train_rows, ]
test_data <- data[(train_rows + 1):nrow(data), ]

# Separate y and x in training and testing sets
train_y <- train_data[, 1]  # Assuming y is the first column
train_x <- train_data[, -1] # Assuming x is from the second column onwards

test_y <- test_data[, 1]
test_x <- test_data[, -1]
test_data_os_diff=diff(test_y)

x=ts(data[,-1])
y=ts(data[,1])

index <- Sys.getenv(c("SLURM_ARRAY_TASK_ID")) 


# Create a grid around lambda_g and lambda_i
lambda_g_grid <-  seq(0, 4, by = 0.1)
lambda_i_grid <-  seq(0, 4, by = 0.1)  # Change 0.1 to suit your precision
lag_grid = c(0,1,2,3,4)
det_grid = c("none","constant","both")

param_grid <- expand.grid(alpha = lambda_g_grid , lambda = lambda_i_grid, det = det_grid,lag=lag_grid)



lower = seq(1681,25215,by=1681)-1680
upper = seq(1681,25215,by=1681)
boundaries = data.frame(cbind(lower,upper))

lower_run = boundaries[run,1]
upper_run= boundaries[run,2]

param_grid = param_grid[c(lower_run:upper_run),]

lambda_g_grid <-  param_grid[index,1]
lambda_i_grid <- param_grid[index,2]
lag_grid = param_grid[index,4]
det_grid = param_grid[index,3]

param_grid = param_grid[index,]

name_parameter = paste0("parameter_SPECS_1",as.numeric(rownames(param_grid)[1]),".xlsx")
name_rmse = paste0("rmse_SPECS_1",rownames(param_grid)[1],".xlsx")

lagpad <- function(x, k) {
  if (k>0) {
    return (c(rep(NA, k), x)[1 : length(x)] );
  }
  else {
    return (c(x[(-k+1) : length(x)], rep(NA, -k)));
  }

pad  <- function(x,k) {
  x=lagpad(x,k=k)
  c(diff(x),NA)
} 
d_x  = function(k){
  apply(data[,-1],MARGIN = 2,FUN = pad,k=k)
}
d_y  = function(k){
  pad(data[,1],k=k)
}

iteration_count <- 0  # Initialize the iteration count
RMSE_total=c()
length_m = length(lambda_g_grid)*length(lambda_i_grid)*length(lag_grid)*length(det_grid)
combination=matrix(NA,ncol=4,nrow=length_m)
estimates=matrix(NA,ncol=length_m,nrow=(ncol(test_x)*(length(lag_grid)+1)+length(lag_grid)))
# Loop over all combinations
for (j in 1:nrow(param_grid)) {
  lambda_g <-  param_grid$alpha[j]
  lambda_i <- param_grid$lambda[j]
  lag = param_grid$lag[j]
  det = as.character(param_grid$det[j])
        iteration_count <- iteration_count + 1  # Increment the iteration count
        # Do something with the combination
        combination[iteration_count,1]=lambda_g
        combination[iteration_count,2]=lambda_i
        combination[iteration_count,3]=lag
        combination[iteration_count,4]=det
        RMSE=c()
        gamma_estimates=matrix(NA,nrow=(ncol(test_x)*(lag+2)+(lag+1)),ncol=length(test_y))
        for (i in 1:(length(test_y)-1)) {
          train_data_os=data[1:((length(train_y)+i)-1),]
          test_data_os=data[(length(train_y)+i),]
          test_data_os_lead=diff(lead(data))[(length(train_y)+i),]
          train_data_y <- train_data_os[, 1] 
          train_data_x <- train_data_os[, -1]
          test_data_y <- test_data_os[1]
          test_data_x <- test_data_os[-1]
          fit = specs(train_data_y,train_data_x,p=lag,deterministics = det,ADL = FALSE,weights = "none",
                      lambda_g = lambda_g,lambda_i = lambda_i)
          if (lag==0) {dat_new=(c((test_data_y),test_data_x,(test_data_os_lead)[-1]))}
          else if(lag==1){dat_new=(c((test_data_y),(test_data_x),(test_data_os_lead)[-1],(test_data_os_lead)[1],d_x(0)[(length(train_y)+i),]))}
          else if(lag==2){dat_new=(c((test_data_y),(test_data_x),(test_data_os_lead)[-1],(test_data_os_lead)[1],d_x(0)[(length(train_y)+i),],d_y(0)[(length(train_y)+i)][1],d_x(1)[(length(train_y)+i),]))}
          else if(lag==3){dat_new=(c((test_data_y),(test_data_x),(test_data_os_lead)[-1],(test_data_os_lead)[1],d_x(0)[(length(train_y)+i),],d_y(0)[(length(train_y)+i)][1],d_x(1)[(length(train_y)+i),],d_y(1)[(length(train_y)+i)][1],d_x(2)[(length(train_y)+i),]))}
          else if(lag==4){dat_new=(c((test_data_y),(test_data_x),(test_data_os_lead)[-1],(test_data_os_lead)[1],d_x(0)[(length(train_y)+i),],d_y(0)[(length(train_y)+i)][1],d_x(1)[(length(train_y)+i),],d_y(1)[(length(train_y)+i)][1],d_x(2)[(length(train_y)+i),],d_y(2)[(length(train_y)+i)][1],d_x(3)[(length(train_y)+i),]))}
          else if(lag==5){dat_new=(c((test_data_y),(test_data_x),(test_data_os_lead)[-1],(test_data_os_lead)[1],d_x(0)[(length(train_y)+i),],d_y(0)[(length(train_y)+i)][1],d_x(1)[(length(train_y)+i),],d_y(1)[(length(train_y)+i)][1],d_x(2)[(length(train_y)+i),],d_y(2)[(length(train_y)+i)][1],d_x(3)[(length(train_y)+i),],d_y(3)[(length(train_y)+i)][1],d_x(4)[(length(train_y)+i),]))}
          else if(lag==6){dat_new=(c((test_data_y),(test_data_x),(test_data_os_lead)[-1],(test_data_os_lead)[1],d_x(0)[(length(train_y)+i),],d_y(0)[(length(train_y)+i)][1],d_x(1)[(length(train_y)+i),],d_y(1)[(length(train_y)+i)][1],d_x(2)[(length(train_y)+i),],d_y(2)[(length(train_y)+i)][1],d_x(3)[(length(train_y)+i),],d_y(3)[(length(train_y)+i)][1],d_x(4)[(length(train_y)+i),],d_y(4)[(length(train_y)+i)][1],d_x(5)[(length(train_y)+i),]))}
          else if(lag==7){dat_new=(c((test_data_y),(test_data_x),(test_data_os_lead)[-1],(test_data_os_lead)[1],d_x(0)[(length(train_y)+i),],d_y(0)[(length(train_y)+i)][1],d_x(1)[(length(train_y)+i),],d_y(1)[(length(train_y)+i)][1],d_x(2)[(length(train_y)+i),],d_y(2)[(length(train_y)+i)][1],d_x(3)[(length(train_y)+i),],d_y(3)[(length(train_y)+i)][1],d_x(4)[(length(train_y)+i),],d_y(4)[(length(train_y)+i)][1],d_x(5)[(length(train_y)+i),],d_y(5)[(length(train_y)+i)][1],d_x(6)[(length(train_y)+i),]))}
          else if(lag==8){dat_new=(c((test_data_y),(test_data_x),(test_data_os_lead)[-1],(test_data_os_lead)[1],d_x(0)[(length(train_y)+i),],d_y(0)[(length(train_y)+i)][1],d_x(1)[(length(train_y)+i),],d_y(1)[(length(train_y)+i)][1],d_x(2)[(length(train_y)+i),],d_y(2)[(length(train_y)+i)][1],d_x(3)[(length(train_y)+i),],d_y(3)[(length(train_y)+i)][1],d_x(4)[(length(train_y)+i),],d_y(4)[(length(train_y)+i)][1],d_x(5)[(length(train_y)+i),],d_y(5)[(length(train_y)+i)][1],d_x(6)[(length(train_y)+i),],d_y(6)[(length(train_y)+i)][1],d_x(7)[(length(train_y)+i),]))}
          fit$gammas[is.nan(fit$gammas)]=0
          y_hat = dat_new%*%fit$gammas
          result=as.matrix(cbind(test_data_os_diff[1],y_hat))
          result=na.omit(result)
          RMSE[i]=sqrt((1/length(result[,1]))*sum((result[,1]-result[,2])^2))
          print(paste("For the given set, one-step iteration:",i, "of",(length(test_y)),"iterations with RMSE of",RMSE[i]))
          gamma_estimates[,i] = fit$gammas
        }
        
        gamma_estimates=t(na.omit(t(data)))
        gamma_estimates=t(gamma_estimates)
        gamma_est = rowMeans(gamma_estimates)
        length(gamma_est)=nrow(estimates)
        estimates[,iteration_count]=gamma_est
        RMSE=na.omit(RMSE)
        RMSE_total[iteration_count]=mean(RMSE)
        print(paste("Iteration:", iteration_count, "| lambda_g:", lambda_g, "lambda_i:", lambda_i, "lag:", lag,"Det:",det,"RMSE:",RMSE_total
                    
                    
                    [iteration_count]))
      }


write_xlsx(data.frame(RMSE_total), name_rmse)
write_xlsx(data.frame(gamma_est), name_parameter)


########################HPC END#################################
################################################################
################################################################

######################Import SPECS HPC Results###################

directory_path <- "C:/Users/Yussuf Schwarz/OneDrive/Desktop/UniVWL/Projekt/SPECS Results"
expected_files <- sprintf("rmse_SPECS_1%d.xlsx", 1:25215)
existing_files = character(0)
missing_files <- character(0)
for (file_name in expected_files) {
  file_path <- file.path(directory_path, file_name)
  if (!file.exists(file_path)) {
    missing_files <- c(missing_files, file_name)}
    if (file.exists(file_path)) {
    existing_files <- c(existing_files, file_name)
  }
}
library(stringr)
numbers <- str_extract(missing_files, "\\d+")
numbers <- as.numeric(numbers)
print(numbers)

RMSE_length = c()
RMSE_SPECS <- c()
for (i in 1:25215) {
  filename <- paste0("rmse_SPECS_1", i, ".xlsx")
  path <- file.path("C:/Users/Yussuf Schwarz/OneDrive/Desktop/UniVWL/Projekt/SPECS Results", filename)  
  rmse_df <- data.frame(read_xlsx(path))
  RMSE_length[i] = length(rmse_df$RMSE)
  RMSE_SPECS <- c(RMSE_SPECS, rmse_df$RMSE)
}

######################SPECS Results#############################

#Optimal model:
min_RMSE_row <- which.min(RMSE_SPECS)
min_RMSE_alpha <- param_grid$alpha [min_RMSE_row]
min_RMSE_lambda <- param_grid$lambda [min_RMSE_row]
min_RMSE_gamma <- param_grid$det[min_RMSE_row]
fit = specs(train_data_y,train_data_x,p=lag,deterministics = det,ADL = FALSE,weights = "none",
                      lambda_g = 0,lambda_i = 4)
sort(fit$gammas,decreasing = F)

#For Lags:
lambda_g_grid <-  seq(0, 4, by = 0.1)
lambda_i_grid <-  seq(0, 4, by = 0.1)  # Change 0.1 to suit your precision
lag_grid = c(0,1,2,3,4)
det_grid = c("none","constant","both")
param_grid <- expand.grid(alpha = lambda_g_grid , lambda = lambda_i_grid, det = det_grid,lag=lag_grid)

results_df_SPECS <- data.frame(RMSE = RMSE_SPECS, lambda_g = param_grid$alpha , lambda_i = param_grid$lambda, lag = param_grid$lag , det = param_grid$det)

results_df_SPECS_sorted <- arrange(results_df_SPECS, RMSE)

# Find the row with the minimal RMSE
min_RMSE_row <- which.min(RMSE_SPECS)
min_RMSE_alpha <- param_grid$alpha [min_RMSE_row]
min_RMSE_lambda <- param_grid$lambda [min_RMSE_row]
min_RMSE_gamma <- param_grid$det[min_RMSE_row]
#For Lags:
rmse_specs_names = c()
for (i in unique(param_grid$lag)) {
lambda_g_grid <-  seq(0, 4, by = 0.1)
lambda_i_grid <-  seq(0, 4, by = 0.1)  # Change 0.1 to suit your precision
lag_grid = c(0,1,2,3,4)
det_grid = c("none","constant","both")
  name_plot = paste0("rmse_plot_",i)
param_grid <- expand.grid(alpha = lambda_g_grid , lambda = lambda_i_grid, det = det_grid,lag=lag_grid)
results_df_SPECS <- data.frame(RMSE = RMSE_SPECS, lambda_g = param_grid$alpha , lambda_i = param_grid$lambda, lag = param_grid$lag , det = param_grid$det)
# Find the row with the minimal RMSE
min_RMSE_row <- which.min(RMSE_SPECS)
min_RMSE_alpha <- param_grid$alpha [min_RMSE_row]
min_RMSE_lambda <- param_grid$lambda [min_RMSE_row]
min_RMSE_gamma <- param_grid$det[min_RMSE_row]

library(hrbrthemes)
results_df_SPECS = results_df_SPECS[(results_df_SPECS$RMSE<quantile(results_df_SPECS$RMSE,0.95)),]
results_df_SPECS = results_df_SPECS[results_df_SPECS$lag == i,]

RMSE_alpha_beta_gamma = ggplot(results_df_SPECS, aes(x=lambda_g, y=lambda_i)) + 
  geom_tile(aes(fill= scale(RMSE))) +
    scale_fill_gradient(low="blue", high="red") + labs(title = paste0("Lag=",i)) +
  theme_classic()+ 
  geom_point(data = data.frame(x = min_RMSE_alpha, y = min_RMSE_lambda),
             aes(x = x, y = y), color = "green", size = 8) +
  geom_text(aes(x = min_RMSE_alpha, y = min_RMSE_lambda, 
                label = paste("Min RMSE:", round(min(RMSE), 2))), 
            hjust = -0.15, vjust = 1.5, color = "white", size = 8)
  assign(name_plot,RMSE_alpha_beta_gamma, env = .GlobalEnv)
  rmse_specs_names = c(rmse_specs_names,name_plot)
}

library(gridExtra)
library(grid)
tg <- textGrob("RMSE of SPECS Regression for different lags", gp = gpar(fontsize = 28, fontface = 'plain',fontfamily="Arial"))

RMSE_with_gamma_visual = grid.arrange(rmse_plot_0,rmse_plot_1,rmse_plot_2,rmse_plot_3,rmse_plot_4,nrow=2,ncol=3,top=tg)
ggsave("RMSE_SPECS_LAGS.png", plot = RMSE_with_gamma_visual, width = 20, height = 12)


#For Deterministic values:
lambda_g_grid <-  seq(0, 4, by = 0.1)
lambda_i_grid <-  seq(0, 4, by = 0.1)  # Change 0.1 to suit your precision
lag_grid = c(0,1,2,3,4)
det_grid = c("none","constant","both")
param_grid <- expand.grid(alpha = lambda_g_grid , lambda = lambda_i_grid, det = det_grid,lag=lag_grid)

results_df_SPECS <- data.frame(RMSE = RMSE_SPECS, lambda_g = param_grid$alpha , lambda_i = param_grid$lambda, lag = param_grid$lag , det = param_grid$det)
# Find the row with the minimal RMSE
min_RMSE_row <- which.min(RMSE_SPECS)
min_RMSE_alpha <- param_grid$alpha [min_RMSE_row]
min_RMSE_lambda <- param_grid$lambda [min_RMSE_row]
min_RMSE_gamma <- param_grid$det[min_RMSE_row]
#For Lags:
rmse_specs_names = c()
for (i in unique(param_grid$det)) {
lambda_g_grid <-  seq(0, 4, by = 0.1)
lambda_i_grid <-  seq(0, 4, by = 0.1)  # Change 0.1 to suit your precision
lag_grid = c(0,1,2,3,4)
det_grid = c("none","constant","both")
  name_plot = paste0("rmse_plot_",i)
param_grid <- expand.grid(alpha = lambda_g_grid , lambda = lambda_i_grid, det = det_grid,lag=lag_grid)
results_df_SPECS <- data.frame(RMSE = RMSE_SPECS, lambda_g = param_grid$alpha , lambda_i = param_grid$lambda, lag = param_grid$lag , det = param_grid$det)
# Find the row with the minimal RMSE
min_RMSE_row <- which.min(RMSE_SPECS)
min_RMSE_alpha <- param_grid$alpha [min_RMSE_row]
min_RMSE_lambda <- param_grid$lambda [min_RMSE_row]
min_RMSE_gamma <- param_grid$det[min_RMSE_row]

library(hrbrthemes)
results_df_SPECS = results_df_SPECS[(results_df_SPECS$RMSE<quantile(results_df_SPECS$RMSE,0.95)),]
results_df_SPECS = results_df_SPECS[results_df_SPECS$det == i,]

RMSE_alpha_beta_gamma = ggplot(results_df_SPECS, aes(x=lambda_g, y=lambda_i)) + 
  geom_tile(aes(fill= scale(RMSE))) +
    scale_fill_gradient(low="blue", high="red") +
  theme_classic()+ labs(title=paste0("Deterministic value=",i))+
  geom_point(data = data.frame(x = min_RMSE_alpha, y = min_RMSE_lambda),
             aes(x = x, y = y), color = "green", size = 8) +
  geom_text(aes(x = min_RMSE_alpha, y = min_RMSE_lambda, 
                label = paste("Min RMSE:", round(min(RMSE), 2))), 
            hjust = -0.15, vjust = 1.5, color = "white", size = 8)
  assign(name_plot,RMSE_alpha_beta_gamma, env = .GlobalEnv)
  rmse_specs_names = c(rmse_specs_names,name_plot)
}

library(gridExtra)
library(grid)
tg <- textGrob("RMSE of SPECS Regression for different deterministic values", gp = gpar(fontsize = 28, fontface = 'plain',fontfamily="Arial"))

RMSE_with_gamma_visual = grid.arrange(rmse_plot_none,rmse_plot_constant,rmse_plot_both,nrow=1,ncol=3,top=tg)
ggsave("RMSE_SPECS_DETS.png", plot = RMSE_with_gamma_visual, width = 24, height = 10)
 
 


#parameters_df <- as.data.frame(estimates)
#parameters_df$row <- 1:nrow(parameters_df)
#parameters_df_long <- gather(parameters_df, key = "column", value = "value", -row)
#parameters_df_long$value_log <- log1p(parameters_df_long$value)
#data$RMSE <- RMSE_total
#data_none = data[data$det=="none",]
#data_constant =  data[data$det=="constant",]
#data_both =  data[data$det=="both",]
#  # Find the row with minimum RMSE
#min_RMSE_row <- data_none[which.min(data_none$RMSE), ]
## Create the bar plot with the minimum RMSE in red
#min_RMSE_index <- min(data$RMSE)
#bar_colors <- ifelse(data$RMSE == min_RMSE_index, "red", "skyblue")
#ggplot(data, aes(x=index(data),y = RMSE,fill=factor(det))) +
#  geom_bar(stat = "identity") +
#  labs(x = "", y = "RMSE_total", title = "RMSE_total Bar Plot") +
#  theme_minimal() +
#  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())+
#  facet_grid(.~lag) 
#heatmap_specs <-  ggplot(parameters_df_long, aes(x = column, y = row, fill = value_log)) +
#  geom_tile() +
#  scale_fill_gradient(low = "blue", high = "red",na.value = "black") +
#  labs(title = "Heatmap of Parameter Estimation Values (Log)", x = "Columns", y = "Rows") 






```



```{r Variable Selection results, include=T}

#In this section, the results of the variable selection are summarized. The best model's 
#variables based on the RMSE-the SCAD-are used for analyzing the relevant economic determinants of 
#presidential approval. The selected Lag-structure is analyzed, before the economic variables itself
#are analyzed. For both analysis, the absolute value of the estimated SCAD coefficient are used.
#Subsequently, the selected variables are categorized manually into six categories. The weights for
#the categorization are based on the standardized absolute SCAD coefficients.

######################TABLE SUMMARY VARIABLE SELECTION###################
RMSE_result = c(round(min(RMSE),6), round(min(RMSE_SCAD),6),  round(min(RMSE_SPECS),6))
RMSE_names = c("Elastic Net Regression","SCAD Regression","Single Equation Penalized Error Correction")
Selected_variables = c("Intercept+7","Intercept+42","Intercept+0")
RMSE_result_table = data.frame(cbind(RMSE_names,Selected_variables,RMSE_result))

stargazer(RMSE_result_table,title = "RMSE Automated Variable Selection",summary = F,rownames = F,out = ".tex",digits = 6)


###############################ESTIMATE BEST MODEL#################################################################


######################SCAD Results#############################
library(ncvreg)
set.seed(42)
y=approval_q$Approval[15:nrow(approval_q)]
x = manipulated_x_2
x=na.omit(x)
x = data.frame(cbind(x,lag(x,n = 1L),lag(x,n = 2L),lag(x,n = 3L),lag(x,n = 4L),lag(y,n = 1L),lag(y,n = 2L),lag(y,n = 3L),lag(y,n = 4L)))
colnames(x)=c(rep(colnames(data_principal_components), times = 5),"Lag Y_1","Lag Y_2","Lag Y_3","Lag Y_4")
data = data.frame(cbind(x,y))
data=na.omit(data)
x_m = as.matrix(data[,-ncol(data)])
y_m=data[,ncol(data)]
colnames(x_m)=c(rep(colnames(data_principal_components), times = 5),"Lag Y_1","Lag Y_2","Lag Y_3","Lag Y_4")
alpha_seq <- seq(0.01, 0.99, by = 0.01)
lambda_seq <- seq(0, 4, by = 0.01)
gamma_seq <- seq(2.1, 4, by = 0.1)
param_grid <- expand.grid(alpha = alpha_seq, lambda = lambda_seq, gamma = gamma_seq)
results_df <- data.frame(RMSE = RMSE_SCAD, alpha = param_grid$alpha, lambda = param_grid$lambda, gamma = param_grid$gamma)

# Find the row with the minimal RMSE
min_RMSE_row <- which.min(RMSE_SCAD)
min_RMSE_alpha <- param_grid$alpha[min_RMSE_row]
min_RMSE_lambda <- param_grid$lambda[min_RMSE_row]
min_RMSE_gamma <- param_grid$gamma[min_RMSE_row]

alpha = min_RMSE_alpha
gamma = min_RMSE_gamma
lambda = min_RMSE_lambda
reg_min <- ncvfit(x_m, y_m,penalty = "SCAD", gamma = gamma, alpha = alpha, lambda = lambda)
parameters_opt <- as.vector(reg_min$beta)
parameters_opt_names = colnames(x_m)
parameters_optimal = data.frame(cbind(parameters_opt_names,parameters_opt))
parameters_optimal_df = parameters_optimal[order(parameters_optimal$parameters_opt,decreasing = T),]  
parameters_optimal_df = parameters_optimal_df[!parameters_optimal_df$parameters_opt == 0 ,]
colnames(parameters_optimal_df) = c("Names","Values")

lag_x = c()
for (i in 1:length(as.numeric(rownames(parameters_optimal_df)))) {
  print(i)
  if (as.numeric(rownames(parameters_optimal_df))[i] < 2140) {lag_x[i] = "Lag 0"}
  else if ((as.numeric(rownames(parameters_optimal_df))[i] > 2139 &  as.numeric(rownames(parameters_optimal_df))[i] < 4279 )) {lag_x[i] = "Lag 1"}
  else if ((as.numeric(rownames(parameters_optimal_df))[i] > 4278 &  as.numeric(rownames(parameters_optimal_df))[i] < 6418 )) {lag_x[i] = "Lag 2"}
  else if ((as.numeric(rownames(parameters_optimal_df))[i] > 6417 &  as.numeric(rownames(parameters_optimal_df))[i] < 8557 )) {lag_x[i] = "Lag 3"}
  else if ((as.numeric(rownames(parameters_optimal_df))[i] > 8556 &  as.numeric(rownames(parameters_optimal_df))[i] < 10696 )) {lag_x[i] = "Lag 4"}
  else if (as.numeric(rownames(parameters_optimal_df))[i] == 10696) {lag_x[i] = "Lag 1"}
  else if (as.numeric(rownames(parameters_optimal_df))[i] == 10697) {lag_x[i] = "Lag 2"}
  else if (as.numeric(rownames(parameters_optimal_df))[i] == 10698) {lag_x[i] = "Lag 3"}
  else if (as.numeric(rownames(parameters_optimal_df))[i] == 10699) {lag_x[i] = "Lag 4"}
  else {print("Error")}
}

parameters_optimal_df$Lags = lag_x
parameters_optimal_df$Names = paste(parameters_optimal_df$Names,lag_x)
parameters_optimal_df$Names = gsub(x=parameters_optimal_df$Names,pattern = "[[:punct:]]",replacement = " ")
parameters_optimal_df$Names = gsub(x=parameters_optimal_df$Names,pattern = "NA",replacement = "")

###write_xlsx(parameters_optimal_df,path = "Optimal_Parameters.xlsx")

#########################ANALYSIS OF LAG-STRUCTURE#############################################


parameters_optimal_df_x = parameters_optimal_df[-c(5,9,17,19),]

library(ggplot2)


lag_counts <- table(parameters_optimal_df_x$Lags)
lag_counts_df <- as.data.frame(lag_counts)
names(lag_counts_df) <- c("Lag", "Count")
custom_colors <- c("#E41A1C", "blue", "#4DAF4A", "grey", "black")
total_lag = ggplot(lag_counts_df, aes(x = "", y = Count, fill = Lag)) +
  geom_col(color = "black") +
  geom_text(aes(label = Count),
            position = position_stack(vjust = 0.5),color = "red") +
    labs(title = "Number of Lags of the Selected Economic Variables", fill = "Lag") +
  coord_polar(theta = "y") +
  scale_fill_brewer() +
  theme_void()
ggsave("Lag_total.png", plot = total_lag, width = 12, height = 6)


weights_xlags = c()
weights_xlags[1] = sum(abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$Lags == "Lag 0"])))/length(parameters_optimal_df_x$Lags == "Lag 0")
weights_xlags[2] = sum(abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$Lags == "Lag 1"])))/length(parameters_optimal_df_x$Lags == "Lag 1")
weights_xlags[3] = sum(abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$Lags == "Lag 2"])))/length(parameters_optimal_df_x$Lags == "Lag 2")
weights_xlags[4] = sum(abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$Lags == "Lag 3"])))/length(parameters_optimal_df_x$Lags == "Lag 3")
weights_xlags[5] = sum(abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$Lags == "Lag 4"])))/length(parameters_optimal_df_x$Lags == "Lag 4")

parameters_optimal_df_x$weights = NA
for (i in 1:nrow(parameters_optimal_df_x)) {
  if (parameters_optimal_df_x$Lags[i] == "Lag 0") { parameters_optimal_df_x$weights[i] = weights_xlags[1] }
  else if (parameters_optimal_df_x$Lags[i] == "Lag 1") { parameters_optimal_df_x$weights[i] = weights_xlags[2] }
  else if (parameters_optimal_df_x$Lags[i] == "Lag 2") { parameters_optimal_df_x$weights[i] = weights_xlags[3] }
  else if (parameters_optimal_df_x$Lags[i] == "Lag 3") { parameters_optimal_df_x$weights[i] = weights_xlags[4] }
  else if (parameters_optimal_df_x$Lags[i] == "Lag 4") { parameters_optimal_df_x$weights[i] = weights_xlags[5] }
  else {print("error")}
}

Lag_struc_x = ggplot(parameters_optimal_df_x, aes(x = Lags,y = weights,fill = rep(1,times = nrow(parameters_optimal_df_x)), color = rep(1,times = nrow(parameters_optimal_df_x)))) +
  geom_bar(stat = 'identity') +
  labs(title = "Bar Plot of Lag Factors",
       x = "Lags",
       y = "Share of selected lags weighted by SCAD coefficients") +theme_classic()+ labs(title=("Lag structure of economic determinants of presidential approval"),subtitle = "Selected by SCAD regression") + theme(legend.position="none")

parameters_optimal_df_y  = parameters_optimal_df[c(5,9,17,19),]
weights_ylags = c()
weights_ylags[1] = sum(abs(as.numeric(parameters_optimal_df_y$Values[parameters_optimal_df_y$Lags == "Lag 1"])))
weights_ylags[2] = sum(abs(as.numeric(parameters_optimal_df_y$Values[parameters_optimal_df_y$Lags == "Lag 2"])))
weights_ylags[3] = sum(abs(as.numeric(parameters_optimal_df_y$Values[parameters_optimal_df_y$Lags == "Lag 3"])))
weights_ylags[4] = sum(abs(as.numeric(parameters_optimal_df_y$Values[parameters_optimal_df_y$Lags == "Lag 4"])))

parameters_optimal_df_y$weights = NA
for (i in 1:nrow(parameters_optimal_df_y)) {
  if (parameters_optimal_df_y$Lags[i] == "Lag 1") { parameters_optimal_df_y$weights[i] = weights_ylags[1] }
  else if (parameters_optimal_df_y$Lags[i] == "Lag 2") { parameters_optimal_df_y$weights[i] = weights_ylags[2] }
  else if (parameters_optimal_df_y$Lags[i] == "Lag 3") { parameters_optimal_df_y$weights[i] = weights_ylags[3] }
  else if (parameters_optimal_df_y$Lags[i] == "Lag 4") { parameters_optimal_df_y$weights[i] = weights_ylags[4] }
  else {print("error")}
}
Lag_struc_y = ggplot(parameters_optimal_df_y, aes(x = Lags, y = weights, fill = rep(1,times = nrow(parameters_optimal_df_y)), color = rep(1,times = nrow(parameters_optimal_df_y)))) + 
  geom_bar(stat = 'identity') +
  labs(
       x = "Lags",
       y = "Share of selected lags weighted by SCAD coefficients") +theme_classic()+ labs(title=("Lag structure of autoregressive term of presidential approval"),subtitle = "Selected by SCAD regression")+ theme(legend.position="none")

library(gridExtra)
library(grid)

Lag_struc= grid.arrange(Lag_struc_x, Lag_struc_y,nrow=1,ncol=2)
ggsave("Lag_structure.png", plot = Lag_struc, width = 12, height = 6)

###################################ENET SELECTED VARIABLES###############################################

beta_enet_ordered$names
enet_names <- gsub("\\.", " ", rownames(beta_enet_ordered))

#########################ANALYSIS OF ECONOMICS VARIABLES#############################################


Categories <- c("Monetary Policy","Public Sector","Public Sector","Public Sector","Monetary Policy","Labor Market","Labor Market","Recession","Recession","Monetary Policy","Housing and Mortgages","Labor Market","Public Sector","Private Sector","Housing and Mortgages","Labor Market","Labor Market","Labor Market","Private Sector","Public Sector","Private Sector","Labor Market","Public Sector","Private Sector","Recession","Housing and Mortgages","Public Sector","Housing and Mortgages","Private Sector","Monetary Policy","Public Sector","Labor Market","Recession","Labor Market","Housing and Mortgages","Private Sector","Private Sector","Housing and Mortgages","Private Sector","Private Sector",  "Private Sector","Private Sector")

parameters_optimal_df_x$categories = Categories


cat1 = parameters_optimal_df_x$Names[parameters_optimal_df_x$categories == unique(Categories)[1]]
cat2 =parameters_optimal_df_x$Names[parameters_optimal_df_x$categories == unique(Categories)[2]]
cat3 =parameters_optimal_df_x$Names[parameters_optimal_df_x$categories == unique(Categories)[3]]
cat4 =parameters_optimal_df_x$Names[parameters_optimal_df_x$categories == unique(Categories)[4]]
cat5 =parameters_optimal_df_x$Names[parameters_optimal_df_x$categories == unique(Categories)[5]]
cat6 =parameters_optimal_df_x$Names[parameters_optimal_df_x$categories == unique(Categories)[6]]
#cat7 =parameters_optimal_df_x$Names[parameters_optimal_df_x$categories == unique(Categories)[7]]
#cat8 =parameters_optimal_df_x$Names[parameters_optimal_df_x$categories == unique(Categories)[8]]

stargazer(c(cat1,cat2,cat3,cat4,cat5,cat6),summary = FALSE,title = "Selected economic variables by SCAD regression",out = ".tex")





weights_xvariables = c()
weights_xvariables[1] = sum(abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Monetary Policy"])))/length(parameters_optimal_df_x$categories=="Monetary Policy")
weights_xvariables[2] = sum(abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Public Sector"])))/length(parameters_optimal_df_x$categories=="Public Sector")
weights_xvariables[3] = sum(abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Labor Market"])))/length(parameters_optimal_df_x$categories=="Labor Market")
weights_xvariables[4] = sum(abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Recession"])))/length(parameters_optimal_df_x$categories=="Recession")
weights_xvariables[5] = sum(abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Housing and Mortgages"])))/length(parameters_optimal_df_x$categories=="Housing and Mortgages")
weights_xvariables[6] = sum(abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Private Sector"])))/length(parameters_optimal_df_x$categories=="Private Sector")


parameters_optimal_df_x$weights = NA
for (i in 1:nrow(parameters_optimal_df_x)) {
  if (parameters_optimal_df_x$categories[i] == unique(Categories)[1]) { parameters_optimal_df_x$weights[i] = weights_xvariables[1] }
  else if (parameters_optimal_df_x$categories[i] == unique(Categories)[2]) { parameters_optimal_df_x$weights[i] = weights_xvariables[2] }
  else if (parameters_optimal_df_x$categories[i] == unique(Categories)[3]) { parameters_optimal_df_x$weights[i] = weights_xvariables[3] }
  else if (parameters_optimal_df_x$categories[i] == unique(Categories)[4]) { parameters_optimal_df_x$weights[i] = weights_xvariables[4] }
  else if (parameters_optimal_df_x$categories[i] == unique(Categories)[5]) { parameters_optimal_df_x$weights[i] = weights_xvariables[5] }
  else if (parameters_optimal_df_x$categories[i] == unique(Categories)[6]) { parameters_optimal_df_x$weights[i] = weights_xvariables[6] }
  else {print("error")}
}


x_var= ggplot(parameters_optimal_df_x, aes(x = categories, y = weights, fill = rep(1,times = nrow(parameters_optimal_df_x)), color = rep(1,times = nrow(parameters_optimal_df_x)))) + 
  geom_bar(stat = 'identity') +
  labs(title = "Econonmic determinants of presidential approval", subtitle = "Selected by SCAD regression and manually categorized",
       x = "Categories",
       y = "Share of variables in each category weighted by SCAD coefficients") +theme_classic()+ theme(legend.position="none")+ theme(axis.text.x = element_text(angle = 0))

ggsave("x_variables.png", plot = x_var, width = 16, height = 7)



mon_pol_cat = as.numeric(rownames(parameters_optimal_df_x[parameters_optimal_df_x$categories == "Monetary Policy",]))
monetary_policy = abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Monetary Policy"]))[1]*x_m[,mon_pol_cat[1]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Monetary Policy"]))[2]*x_m[,mon_pol_cat[2]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Monetary Policy"]))[3]*x_m[,mon_pol_cat[3]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Monetary Policy"]))[4]*x_m[,mon_pol_cat[4]]

pub_sec_cat = as.numeric(rownames(parameters_optimal_df_x[parameters_optimal_df_x$categories == "Public Sector",]))
public_sector = abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Public Sector"]))[1]*x_m[,pub_sec_cat[1]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Public Sector"]))[2]*x_m[,pub_sec_cat[2]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Public Sector"]))[3]*x_m[,pub_sec_cat[3]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Public Sector"]))[4]*x_m[,pub_sec_cat[4]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Public Sector"]))[5]*x_m[,pub_sec_cat[5]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Public Sector"]))[6]*x_m[,pub_sec_cat[6]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Public Sector"]))[7]*x_m[,pub_sec_cat[7]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Public Sector"]))[8]*x_m[,pub_sec_cat[8]]

priv_sec_cat = as.numeric(rownames(parameters_optimal_df_x[parameters_optimal_df_x$categories == "Private Sector",]))
private_sector = abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Private Sector"]))[1]*x_m[,priv_sec_cat[1]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Private Sector"]))[2]*x_m[,priv_sec_cat[2]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Private Sector"]))[3]*x_m[,priv_sec_cat[3]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Private Sector"]))[4]*x_m[,priv_sec_cat[4]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Private Sector"]))[5]*x_m[,priv_sec_cat[5]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Private Sector"]))[6]*x_m[,priv_sec_cat[6]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Private Sector"]))[7]*x_m[,priv_sec_cat[7]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Private Sector"]))[8]*x_m[,priv_sec_cat[8]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Private Sector"]))[9]*x_m[,priv_sec_cat[9]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Private Sector"]))[10]*x_m[,priv_sec_cat[10]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Private Sector"]))[11]*x_m[,priv_sec_cat[11]]

lab_mar_cat = as.numeric(rownames(parameters_optimal_df_x[parameters_optimal_df_x$categories == "Labor Market",]))
labor_market = abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Labor Market"]))[1]*x_m[,lab_mar_cat[1]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Labor Market"]))[2]*x_m[,lab_mar_cat[2]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Labor Market"]))[3]*x_m[,lab_mar_cat[3]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Labor Market"]))[4]*x_m[,lab_mar_cat[4]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Labor Market"]))[5]*x_m[,lab_mar_cat[5]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Labor Market"]))[6]*x_m[,lab_mar_cat[6]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Labor Market"]))[7]*x_m[,lab_mar_cat[7]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Labor Market"]))[8]*x_m[,lab_mar_cat[8]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Labor Market"]))[9]*x_m[,lab_mar_cat[9]]

rec_cat = as.numeric(rownames(parameters_optimal_df_x[parameters_optimal_df_x$categories == "Recession",]))
recession = abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Recession"]))[1]*x_m[,rec_cat[1]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Recession"]))[2]*x_m[,rec_cat[2]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Recession"]))[3]*x_m[,rec_cat[3]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Recession"]))[4]*x_m[,rec_cat[4]]


hous_mort_cat = as.numeric(rownames(parameters_optimal_df_x[parameters_optimal_df_x$categories == "Housing and Mortgages",]))
housing_and_mortgages = abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Housing and Mortgages"]))[1]*x_m[,hous_mort_cat[1]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Housing and Mortgages"]))[2]*x_m[,hous_mort_cat[2]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Housing and Mortgages"]))[3]*x_m[,hous_mort_cat[3]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Housing and Mortgages"]))[4]*x_m[,hous_mort_cat[4]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Housing and Mortgages"]))[5]*x_m[,hous_mort_cat[5]]+abs(as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Housing and Mortgages"]))[6]*x_m[,hous_mort_cat[6]]


```



```{r VAR model, include=T}


#In this section, the manually categorized variables from the last section are used to estimate a VAR model.
#The VAR model is analysed using IRF and Granger causality. The results of the Granger causality test are 
#not used in the study. The cumulated and non-cumulated IRFs are orthogonalized to allow for 
#contemporaneous effects.

plot_irf = function(var_model, cause_var, effect_var) {
  irf = irf(var_model, response = effect_var, impulse = cause_var, boot = T,n.ahead=30,ortho=T)
  data=data.frame(cbind(data.frame(irf$irf)[effect_var],data.frame(irf$Lower)[effect_var],data.frame(irf$Upper)[effect_var]))
  irf_plot=ggplot(data = data, aes(x = 0:30, y = data[,1])) +
    geom_line() +
    geom_line(aes(y = data[,2], colour = 'red')) +
    geom_line(aes(y = data[,3]), colour = 'red')+
    geom_hline(yintercept = 0, linetype = "dashed") +
    labs(title = paste("Reaction of",
                        effect_var, "to a shock in the variable",cause_var), x = "Time", y = "Impulse Response") +
  theme_classic()+
    guides(color = "none")
  return(irf_plot)
}

plot_irf_cum = function(var_model, cause_var, effect_var) {
  irf = irf(var_model, response = effect_var, impulse = cause_var, boot = T,n.ahead=30,ortho=T,cumulative = T)
  data=data.frame(cbind(data.frame(irf$irf)[effect_var],data.frame(irf$Lower)[effect_var],data.frame(irf$Upper)[effect_var]))
  irf_plot=ggplot(data = data, aes(x = 0:30, y = data[,1])) +
    geom_line() +
    geom_line(aes(y = data[,2], colour = 'red')) +
    geom_line(aes(y = data[,3]), colour = 'red')+
    geom_hline(yintercept = 0, linetype = "dashed") +
    labs(title = paste("Reaction of",
                        effect_var, "to a shock in the variable",cause_var), x = "Time", y = "Impulse Response") +
  theme_classic()+
    guides(color = "none")
  return(irf_plot)
}



x = data_overall_start1953[,-delete]
y=approval_q$Approval[13:nrow(approval_q)]
x = data.frame(cbind(x,lag(x,n = 1L),lag(x,n = 2L),lag(x,n = 3L),lag(x,n = 4L),lag(y,n = 1L),lag(y,n = 2L),lag(y,n = 3L),lag(y,n = 4L)))
x_m = na.omit(x)
y_m=approval_q$Approval[17:nrow(approval_q)]


mon_pol_cat = as.numeric(rownames(parameters_optimal_df_x[parameters_optimal_df_x$categories == "Monetary Policy",]))
monetary_policy = ((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Monetary Policy"]))[1]/length(mon_pol_cat))*x_m[,mon_pol_cat[1]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Monetary Policy"]))[2]/length(mon_pol_cat))*x_m[,mon_pol_cat[2]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Monetary Policy"]))[3]/length(mon_pol_cat))*x_m[,mon_pol_cat[3]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Monetary Policy"]))[4]/length(mon_pol_cat))*x_m[,mon_pol_cat[4]]

pub_sec_cat = as.numeric(rownames(parameters_optimal_df_x[parameters_optimal_df_x$categories == "Public Sector",]))
public_sector = ((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Public Sector"]))[1]/length(pub_sec_cat))*x_m[,pub_sec_cat[1]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Public Sector"]))[2]/length(pub_sec_cat))*x_m[,pub_sec_cat[2]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Public Sector"]))[3]/length(pub_sec_cat))*x_m[,pub_sec_cat[3]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Public Sector"]))[4]/length(pub_sec_cat))*x_m[,pub_sec_cat[4]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Public Sector"]))[5]/length(pub_sec_cat))*x_m[,pub_sec_cat[5]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Public Sector"]))[6]/length(pub_sec_cat))*x_m[,pub_sec_cat[6]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Public Sector"]))[7]/length(pub_sec_cat))*x_m[,pub_sec_cat[7]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Public Sector"]))[8]/length(pub_sec_cat))*x_m[,pub_sec_cat[8]]

priv_sec_cat = as.numeric(rownames(parameters_optimal_df_x[parameters_optimal_df_x$categories == "Private Sector",]))
private_sector = ((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Private Sector"]))[1]/length(priv_sec_cat))*x_m[,priv_sec_cat[1]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Private Sector"]))[2]/length(priv_sec_cat))*x_m[,priv_sec_cat[2]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Private Sector"]))[3]/length(priv_sec_cat))*x_m[,priv_sec_cat[3]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Private Sector"]))[4]/length(priv_sec_cat))*x_m[,priv_sec_cat[4]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Private Sector"]))[5]/length(priv_sec_cat))*x_m[,priv_sec_cat[5]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Private Sector"]))[6]/length(priv_sec_cat))*x_m[,priv_sec_cat[6]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Private Sector"]))[7]/length(priv_sec_cat))*x_m[,priv_sec_cat[7]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Private Sector"]))[8]/length(priv_sec_cat))*x_m[,priv_sec_cat[8]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Private Sector"]))[9]/length(priv_sec_cat))*x_m[,priv_sec_cat[9]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Private Sector"]))[10]/length(priv_sec_cat))*x_m[,priv_sec_cat[10]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Private Sector"]))[11]/length(priv_sec_cat))*x_m[,priv_sec_cat[11]]

lab_mar_cat = as.numeric(rownames(parameters_optimal_df_x[parameters_optimal_df_x$categories == "Labor Market",]))
labor_market = ((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Labor Market"]))[1]/length(lab_mar_cat))*x_m[,lab_mar_cat[1]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Labor Market"]))[2]/length(lab_mar_cat))*x_m[,lab_mar_cat[2]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Labor Market"]))[3]/length(lab_mar_cat))*x_m[,lab_mar_cat[3]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Labor Market"]))[4]/length(lab_mar_cat))*x_m[,lab_mar_cat[4]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Labor Market"]))[5]/length(lab_mar_cat))*x_m[,lab_mar_cat[5]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Labor Market"]))[6]/length(lab_mar_cat))*x_m[,lab_mar_cat[6]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Labor Market"]))[7]/length(lab_mar_cat))*x_m[,lab_mar_cat[7]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Labor Market"]))[8]/length(lab_mar_cat))*x_m[,lab_mar_cat[8]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Labor Market"]))[9]/length(lab_mar_cat))*x_m[,lab_mar_cat[9]]

rec_cat = as.numeric(rownames(parameters_optimal_df_x[parameters_optimal_df_x$categories == "Recession",]))
recession = ((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Recession"]))[1]/length(rec_cat))*x_m[,rec_cat[1]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Recession"]))[2]/length(rec_cat))*x_m[,rec_cat[2]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Recession"]))[3]/length(rec_cat))*x_m[,rec_cat[3]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Recession"]))[4]/length(rec_cat))*x_m[,rec_cat[4]]

hous_mort_cat = as.numeric(rownames(parameters_optimal_df_x[parameters_optimal_df_x$categories == "Housing and Mortgages",]))
housing_and_mortgages = ((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Housing and Mortgages"]))[1]/length(hous_mort_cat))*x_m[,hous_mort_cat[1]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Housing and Mortgages"]))[2]/length(hous_mort_cat))*x_m[,hous_mort_cat[2]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Housing and Mortgages"]))[3]/length(hous_mort_cat))*x_m[,hous_mort_cat[3]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Housing and Mortgages"]))[4]/length(hous_mort_cat))*x_m[,hous_mort_cat[4]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Housing and Mortgages"]))[5]/length(hous_mort_cat))*x_m[,hous_mort_cat[5]]+((as.numeric(parameters_optimal_df_x$Values[parameters_optimal_df_x$categories == "Housing and Mortgages"]))[6]/length(hous_mort_cat))*x_m[,hous_mort_cat[6]]


kpss.test(monetary_policy)
kpss.test(public_sector)
kpss.test(private_sector)
kpss.test(labor_market)
kpss.test(housing_and_mortgages)
kpss.test(recession)
kpss.test(y_m)

vardata1 = data.frame((cbind(monetary_policy,c(NA,diff(public_sector)),c(NA,diff(private_sector)),c(NA,diff(labor_market)),housing_and_mortgages,recession,c(NA,diff(y_m)))))

colnames(vardata1) = c("monetary_policy","public_sector","private_sector","labor_market","housing_and_mortgages","recession","presidential_approval")
vardata1 = na.omit(vardata1)

VARselect(vardata1,lag.max =8,type = "none")
Var1 = VAR(vardata1,ic="AIC",type = "const",lag.max=8)


JB1=normality.test(Var1)
Serial1=serial.test(Var1)
Serial1
Homosked1= arch.test(Var1,lags.multi = 10)
Homosked1


set.seed(100)

plot_irf(Var1,"monetary_policy","presidential_approval")
plot_irf_cum(Var1,"monetary_policy","presidential_approval")

plot_irf(Var1,"public_sector","presidential_approval")
plot_irf_cum(Var1,"public_sector","presidential_approval")

plot_irf(Var1,"labor_market","presidential_approval")
plot_irf_cum(Var1,"labor_market","presidential_approval")

plot_irf(Var1,"private_sector","presidential_approval")
plot_irf_cum(Var1,"private_sector","presidential_approval")

plot_irf(Var1,"housing_and_mortgages","presidential_approval")
plot_irf_cum(Var1,"housing_and_mortgages","presidential_approval")

plot_irf(Var1,"recession","presidential_approval")
plot_irf_cum(Var1,"recession","presidential_approval")

granger_causality(Var1,var.x = c("recession","monetary_policy","public_sector","private_sector","housing_and_mortgages","labor_market"),var.y = c("presidential_approval"))


```


```{r Out of Sample Forecasting, include=T}

#In this section, pseudo out-of-sample forecasting is conducted to gain further insights. The models are all based on a 70/30 training split.
#As baseline models, a white noise, AR(1) and ARIMA based on different information criteria are estimated. Additionally, the VAR estimated in 
#the last section is used for forecasting. Furthermore, Random Forest, and LSTM models are estimated. The estimation of the LSTM was comptuationally 
#too extensive. Therefore, for the estimation the HPC was again utilized. 
#Models were based on the non-stationary uncategorized, the stationary uncategorized, and the statoinary categorized variables.
#Subsequently, the results are summarized in tables.

################################DATA############################################

cat_data = data.frame((cbind(monetary_policy,c(NA,diff(public_sector)),c(NA,diff(private_sector)),c(NA,diff(labor_market)),housing_and_mortgages,recession,c(NA,diff(y_m)))))
colnames(cat_data) = c("monetary_policy","public_sector","private_sector","labor_market","housing_and_mortgages","recession","presidential_approval")
cat_data = na.omit(cat_data)

x = data_overall_start1953[,-delete]
y=approval_q$Approval[13:nrow(approval_q)]
x = data.frame(cbind(x,lag(x,n = 1L),lag(x,n = 2L),lag(x,n = 3L),lag(x,n = 4L),lag(y,n = 1L),lag(y,n = 2L),lag(y,n = 3L),lag(y,n = 4L)))
x_m = na.omit(x)
y_m=approval_q$Approval[17:nrow(approval_q)]

uncat_data = data.frame(cbind(x_m[,as.numeric(rownames(parameters_optimal_df_x))],y_m))
colnames(uncat_data) = c(colnames(x_m)[as.numeric(rownames(parameters_optimal_df_x))],"presidential_approval")



################################DATA SPLIT######################################

cat_data_train = cat_data[(1:floor(nrow(cat_data)*0.7)),]
cat_data_test = cat_data[(((floor(nrow(cat_data)*0.7))+1):nrow(cat_data)),]

uncat_data_train = uncat_data[(1:floor(nrow(uncat_data)*0.7)),]
uncat_data_test = uncat_data[(((floor(nrow(uncat_data)*0.7))+1):nrow(uncat_data)),]

write_xlsx(cat_data,"cat_data.xlsx")
write_xlsx(cat_data_train,"cat_data_train.xlsx")
write_xlsx(cat_data_test,"cat_data_test.xlsx")

write_xlsx(uncat_data,"uncat_data.xlsx")
write_xlsx(uncat_data_train,"uncat_data_train.xlsx")
write_xlsx(uncat_data_test,"uncat_data_test.xlsx")

cat_data <- as.data.frame(read_excel("cat_data.xlsx"))
cat_data_train <- as.data.frame(read_excel("cat_data_train.xlsx"))
cat_data_test <- as.data.frame(read_excel("cat_data_test.xlsx"))

uncat_data <- as.data.frame(read_excel("uncat_data.xlsx"))
uncat_data_train <- as.data.frame(read_excel("uncat_data_train.xlsx"))
uncat_data_test <- as.data.frame(read_excel("uncat_data_test.xlsx"))



##########################NAIVE FORECASTS#######################################


forecast_naive = function(parameter1,parameter2,model,h){
model = model
h = h
forecasts = c()
begin = nrow(uncat_data_train)
n_steps = nrow(cat_data_test)
  for (i in 1:n_steps) {
    End = begin+h*(i-1)
    print(End)
    train_set = uncat_data$presidential_approval[1:End]
    test_set = uncat_data_test$presidential_approval[i]
    arima_model = arima(train_set,c(parameter1,parameter2,0))
    forecast = predict(arima_model, n.ahead=h)$pred
    forecasts = c(forecasts,forecast)
  }
  nameforecast = paste0("forecast_",model,"_",h)
  assign(nameforecast,forecasts[1:n_steps],envir = .GlobalEnv)
  forecast_errors_wn = uncat_data_test$presidential_approval - forecasts[1:n_steps]
  namermse = paste0("RMSE_",model,"_",h)
  RMSE = sqrt(mean(forecast_errors_wn^2))
  assign(namermse,RMSE,envir = .GlobalEnv)
}

for (H in 1:4) {
  forecast_naive(0,0,"wn",H)
}
for (H in 1:4) {
  forecast_naive(1,1,"ar1",H)
}


  #Auto ARIMA based on BIC

forecast_autoarima = function(IC,model,h){
model = model
h = h
forecasts = c()
begin = nrow(uncat_data_train)
n_steps = nrow(cat_data_test)
  for (i in 1:n_steps) {
    End = begin+h*(i-1)
    print(End)
    train_set = uncat_data$presidential_approval[1:End]
    test_set = uncat_data_test$presidential_approval[i]
    arima_model = auto.arima(train_set,ic = IC)
    forecast = predict(arima_model, n.ahead=h)$pred
    forecasts = c(forecasts,forecast)
  }
  nameforecast = paste0("forecast_",model,"_",h)
  assign(nameforecast,forecasts[1:n_steps],envir = .GlobalEnv)
  forecast_errors_wn = uncat_data_test$presidential_approval - forecasts[1:n_steps]
  namermse = paste0("RMSE_",model,"_",h)
  RMSE = sqrt(mean(forecast_errors_wn^2))
  assign(namermse,RMSE,envir = .GlobalEnv)
}

for (H in 1:4) {
  forecast_autoarima("bic","BIC",H)
}
for (H in 1:4) {
    forecast_autoarima("aic","AIC",H)
}



  ##########################VAR FORECASTS#######################################


    #VAR none based on AIC
forecast_var = function(det,IC,model,h){
model = model
h = h
forecasts = c()
n_steps = nrow(cat_data_test)
begin = nrow(cat_data_train)

  for (i in 1:n_steps) {
    End = begin+h*(i-1)
    print(End)
    train_set = cat_data[1:End,]
    train_set = na.omit(train_set)
    test_set = cat_data_test$presidential_approval[i]
    var_model = VAR(train_set,type = det,ic = IC,lag.max=8)
    forecast = predict(var_model, n.ahead=H)$fcst$presidential_approval[,1]
    forecasts = c(forecasts,forecast)
  }
  nameforecast = paste0("forecast_var_",det,"_",model,"_",h)
  assign(nameforecast,forecasts[1:n_steps],envir = .GlobalEnv)
  forecast_errors_wn = cat_data_test$presidential_approval - forecasts[1:n_steps]
  namermse = paste0("RMSE_var_",det,"_",model,"_",h)
  RMSE = sqrt(mean(forecast_errors_wn^2))
  assign(namermse,RMSE,envir = .GlobalEnv)
}

determinants = c("none","const","trend","both")
model_types = c("AIC","SC")
h = c(1,2,3,4)
grid_var_forecasts = as.data.frame(expand.grid(determinants,model_types,h))

for (b in 1:nrow(grid_var_forecasts)) {
  det=as.character(grid_var_forecasts$Var1[b])
  type=as.character(grid_var_forecasts$Var2[b])
  H = grid_var_forecasts$Var3[b]
  print(c(det,type,H))
  forecast_var(det,type,type,H)
}



  ##########################Random Forest#######################################

  

##########################Uncategorized#######################################

  
library(randomForest)
library(xts)
library(ggplot2)

##not checked for stationarity:

x = data_overall_start1953[,-delete]
y=approval_q$Approval[13:nrow(approval_q)]
x = data.frame(cbind(x,lag(x,n = 1L),lag(x,n = 2L),lag(x,n = 3L),lag(x,n = 4L),lag(y,n = 1L),lag(y,n = 2L),lag(y,n = 3L),lag(y,n = 4L)))
x_m = na.omit(x)
y_m=approval_q$Approval[17:nrow(approval_q)]

uncat_data = data.frame(cbind(x_m[,as.numeric(rownames(parameters_optimal_df_x))],y_m,x_m$lag.y..n...1L.,x_m$lag.y..n...2L.,x_m$lag.y..n...3L.,x_m$lag.y..n...4L.))
colnames(uncat_data) = c(colnames(x_m)[as.numeric(rownames(parameters_optimal_df_x))],"presidential_approval","presidential_approval_lag_1","presidential_approval_lag_2","presidential_approval_lag_3","presidential_approval_lag_4")

uncat_data_train = uncat_data[(1:floor(nrow(uncat_data)*0.7)),]
uncat_data_test = uncat_data[(((floor(nrow(uncat_data)*0.7))+1):nrow(uncat_data)),]


forecast_randomforest = function(h,ntree){
forecasts = c()
n_steps = nrow(uncat_data_test)
begin = nrow(uncat_data_train)

  for (i in 1:n_steps) {
    End = begin+h*(i-1)
    print(End)
    train_set = uncat_data[1:End,]
    train_set = na.omit(train_set)
    test_set = uncat_data_test$presidential_approval[i]
    rf_model <- randomForest(presidential_approval ~ ., data = uncat_data_train, ntree = ntree)
    Begin_test = i*h-(h-1)
    End_test = i*h
    print(c(Begin_test,End_test,length(Begin_test:End_test)))
    forecast = predict(rf_model, newdata = uncat_data_test[(Begin_test:End_test),])
    forecasts = c(forecasts,forecast)
  }
  nameforecast = paste0("forecast_rf_",ntree,"_",h)
  assign(nameforecast,forecasts[1:n_steps],envir = .GlobalEnv)
  forecast_errors_wn = uncat_data_test$presidential_approval - forecasts[1:n_steps]
  namermse = paste0("RMSE_rf_",ntree,"_",h)
  RMSE = sqrt(mean(forecast_errors_wn^2))
  assign(namermse,RMSE,envir = .GlobalEnv)
}

h_forest = c(1,2,3,4)
tree_size = c(10,100,500,1000,5000,10000)
forest_parameters = expand.grid(h_forest,tree_size)


for (B in 1:nrow(forest_parameters)) {
  h = forest_parameters[B,1]
  t_s = forest_parameters[B,2]
  forecast_randomforest(h,t_s)
}


 
#checked for stationarity:

uncat_data
uncat_data_stationary = as.data.frame(matrix(NA,nrow=nrow(uncat_data),ncol = ncol(uncat_data)))
for (i in 1:ncol(uncat_data)) {
 kpss_test_rf =  kpss.test(uncat_data[,i])
  kpss_test = kpss_test_rf$p.value
  if (kpss_test < 0.05) {
   uncat_data_stationary[,i] = c(NA,diff(uncat_data[,i])) 
  }
  else{uncat_data_stationary[,i] = uncat_data[,i]}
}
colnames(uncat_data_stationary)=colnames(uncat_data)
uncat_data_stationary = na.omit(uncat_data_stationary) 
uncat_data_stationary_train = uncat_data_stationary[(1:floor(nrow(uncat_data_stationary)*0.7)),]
uncat_data_stationary_test = uncat_data_stationary[(((floor(nrow(uncat_data_stationary)*0.7))+1):nrow(uncat_data_stationary)),]


forecast_randomforest_stat = function(h,ntree){
forecasts = c()
n_steps = nrow(uncat_data_stationary_test)
begin = nrow(uncat_data_stationary_train)

  for (i in 1:n_steps) {
    End = begin+h*(i-1)
    print(End)
    train_set = uncat_data_stationary[1:End,]
    train_set = na.omit(train_set)
    test_set = uncat_data_stationary_test$presidential_approval[i]
    rf_model <- randomForest(presidential_approval ~ ., data = uncat_data_stationary_train, ntree = ntree)
    Begin_test = i*h-(h-1)
    End_test = i*h
    print(c(Begin_test,End_test,length(Begin_test:End_test)))
    forecast = predict(rf_model, newdata = uncat_data_stationary_test[(Begin_test:End_test),])
    forecasts = c(forecasts,forecast)
  }
  nameforecast = paste0("forecast_rf_st_",ntree,"_",h)
  assign(nameforecast,forecasts[1:n_steps],envir = .GlobalEnv)
  forecast_errors_wn = uncat_data_stationary_test$presidential_approval - forecasts[1:n_steps]
  namermse = paste0("RMSE_rf_st_",ntree,"_",h)
  RMSE = sqrt(mean(forecast_errors_wn^2))
  assign(namermse,RMSE,envir = .GlobalEnv)
}

h_forest = c(1,2,3,4)
tree_size = c(10,100,500,1000,5000,10000)
forest_parameters = expand.grid(h_forest,tree_size)


for (B in 1:nrow(forest_parameters)) {
  h = forest_parameters[B,1]
  t_s = forest_parameters[B,2]
  forecast_randomforest_stat(h,t_s)
}

##########################Categorized#######################################

cat_data = data.frame((cbind(monetary_policy,c(NA,diff(public_sector)),c(NA,diff(private_sector)),c(NA,diff(labor_market)),housing_and_mortgages,recession,c(NA,diff(y_m)))))
colnames(cat_data) = c("monetary_policy","public_sector","private_sector","labor_market","housing_and_mortgages","recession","presidential_approval")
cat_data = na.omit(cat_data)
cat_data = data.frame(cbind(cat_data,lag(cat_data$presidential_approval,n=1L),lag(cat_data$presidential_approval,n=2L),lag(cat_data$presidential_approval,n=3L),lag(cat_data$presidential_approval,n=4L)))
cat_data = na.omit(cat_data)
colnames(cat_data) = c("monetary_policy","public_sector","private_sector","labor_market","housing_and_mortgages","recession","presidential_approval","presidential_approval_lag_1","presidential_approval_lag_2","presidential_approval_lag_3","presidential_approval_lag_4")

cat_data_stationary=cat_data
cat_data_stationary_train = cat_data[(1:floor(nrow(cat_data)*0.7)),]
cat_data_stationary_test = cat_data[(((floor(nrow(cat_data)*0.7))+1):nrow(cat_data)),]




forecast_randomforest_stat_cat = function(h,ntree){
forecasts = c()
n_steps = nrow(cat_data_stationary_test)
begin = nrow(cat_data_stationary_train)

  for (i in 1:n_steps) {
    End = begin+h*(i-1)
    print(End)
    train_set = cat_data_stationary[1:End,]
    train_set = na.omit(train_set)
    test_set = cat_data_stationary_test$presidential_approval[i]
    rf_model <- randomForest(presidential_approval ~ ., data = cat_data_stationary_train, ntree = ntree)
    Begin_test = i*h-(h-1)
    End_test = i*h
    print(c(Begin_test,End_test,length(Begin_test:End_test)))
    forecast = predict(rf_model, newdata = cat_data_stationary_test[(Begin_test:End_test),])
    forecasts = c(forecasts,forecast)
  }
  nameforecast = paste0("forecast_rf_st_cat_",ntree,"_",h)
  assign(nameforecast,forecasts[1:n_steps],envir = .GlobalEnv)
  forecast_errors_wn = cat_data_stationary_test$presidential_approval - forecasts[1:n_steps]
  namermse = paste0("RMSE_rf_st_cat_",ntree,"_",h)
  RMSE = sqrt(mean(forecast_errors_wn^2))
  assign(namermse,RMSE,envir = .GlobalEnv)
}





h_forest = c(1,2,3,4)
tree_size = c(10,100,500,1000,5000,10000)
forest_parameters = expand.grid(h_forest,tree_size)


for (B in 1:nrow(forest_parameters)) {
  h = forest_parameters[B,1]
  t_s = forest_parameters[B,2]
  forecast_randomforest_stat_cat(h,t_s)
}

 #######################LSTM################################### 

library(devtools)
library(keras)
library(tensorflow)

##########################Uncategorized#######################################

########################HPC#####################################
################################################################
################################################################


find_divisors <- function(number) {
  divisors <- numeric(0)
  
  for (i in 1:20) {
    if (number %% i == 0) {
      divisors <- c(divisors, i)
    }
  }
  
  return(divisors)
}


x = data_overall_start1953[,-delete]
y=approval_q$Approval[13:nrow(approval_q)]
x = data.frame(cbind(x,lag(x,n = 1L),lag(x,n = 2L),lag(x,n = 3L),lag(x,n = 4L),lag(y,n = 1L),lag(y,n = 2L),lag(y,n = 3L),lag(y,n = 4L)))
x_m = na.omit(x)
y_m=approval_q$Approval[17:nrow(approval_q)]

uncat_data = data.frame(cbind(x_m[,as.numeric(rownames(parameters_optimal_df_x))],y_m,x_m$lag.y..n...1L.,x_m$lag.y..n...2L.,x_m$lag.y..n...3L.,x_m$lag.y..n...4L.))
colnames(uncat_data) = c(colnames(x_m)[as.numeric(rownames(parameters_optimal_df_x))],"presidential_approval","presidential_approval_lag_1","presidential_approval_lag_2","presidential_approval_lag_3","presidential_approval_lag_4")

uncat_data_train = uncat_data[(1:floor(nrow(uncat_data)*0.7)),]
uncat_data_test = uncat_data[(((floor(nrow(uncat_data)*0.7))+1):nrow(uncat_data)),]

train_x = uncat_data_train[,-43] 
train_y =uncat_data_train[,43] 

train_x = data.matrix(train_x)
train_y = data.matrix(train_y)

test_x = uncat_data_test[,-43] 
test_y =uncat_data_test[,43] 

test_x = data.matrix(test_x)
test_y = data.matrix(test_y)


train_x_arr <- array(
    data = train_x,
    dim = c(
        193,1,46))

train_y_arr <- array(
    data = train_y,
    dim = c(
        193,1))

test_x_arr <- array(
    data = test_x,
    dim = c(
        83,1,46))

test_y_arr <- array(
    data = test_y,
    dim = c(
        83,1))


forecast_lstm_uncat = function(h){
forecasts = c()
n_steps = floor(nrow(uncat_data_test)/h)
begin = nrow(uncat_data_train)
 
    
  for (i in 1:n_steps) {
    library(keras)
    
    End = begin+h*(i-1)
    print(End)
 
    train_x = uncat_data[c(1:End),-43] 
    train_y =uncat_data[c(1:End),43] 

    train_x = data.matrix(train_x)
    train_y = data.matrix(train_y)
    train_x_arr <- array(
    data = train_x,
    dim = c(
        nrow(train_x),1,46))
    train_y_arr <- array(
    data = train_y,
    dim = c(
        nrow(train_x),1))
    
    
    model <- keras_model_sequential()
    model %>%
    layer_lstm(units            = 124, 
               input_shape      = c(1, 46), 
               batch_size       = 1,
               return_sequences = TRUE, 
               stateful         = TRUE) %>% 
    layer_lstm(units            = 124, 
               return_sequences = TRUE, 
               stateful         = TRUE) %>% 
      layer_lstm(units            = 124, 
               return_sequences = TRUE, 
               stateful         = TRUE) %>% 
      layer_lstm(units            = 124, 
               return_sequences = FALSE, 
               stateful         = TRUE) %>% 
    layer_dense(units = 24)
    model %>% 
    compile(loss = 'mean_squared_error', optimizer = 'adam')
    model %>% fit(x = train_x_arr, 
                  y = train_y_arr, 
                  batch_size = 1,
                  epochs     = 1, 
                  verbose    = 1, 
                  shuffle    = FALSE)
    
    Begin_test = i*h-(h-1)
    End_test = i*h
    print(c(Begin_test,End_test,length(Begin_test:End_test)))
    
    test_range=cbind(c(1:83)*h-(h-1),c(1:83)*h)
    end_range = floor(83/h)
    test_range = test_range[c(1:end_range),]
    
    Begin_test_range = test_range[index,1] 
    End_test_range = test_range[index,2]
    
    test_x = uncat_data_test[(Begin_test:End_test),-43] 
    test_x = data.matrix(test_x)
    test_x_arr <- array(
    data = test_x,
    dim = c(
        1,1,46))
    pred_out <- model %>% 
    predict(test_x_arr, batch_size = 1) %>%
    .[,1] 
    forecast = pred_out
    print(pred_out)
    forecasts = c(forecasts,forecast)
      model = c()
  detach("package:keras",unload=T)
  }
  nameforecast = paste0("forecast_lsmt_uncat_","_",h)
  assign(nameforecast,forecasts[1:n_steps],envir = .GlobalEnv)
  forecast_errors_wn = uncat_data_test$presidential_approval - forecasts[1:n_steps]
  namermse = paste0("RMSE_lmst_uncat_",h)
  RMSE = sqrt(mean(forecast_errors_wn^2))
  assign(namermse,RMSE,envir = .GlobalEnv)
}



set.seed(42)

h_lmst = c(1,2,3,4)

for (B in 1:length(h_lmst)) {
  print(h_lmst[B])
  h = h_lmst[B]
  forecast_lstm_uncat(h)
}

forecast_lstm_uncat(1)


########################HPC END#################################
################################################################
################################################################


######################Import LMST Uncat not stationary checked HPC Results###################

load_lmst_results = function(Path,Filename,savename,h,spec){
    path <- file.path(paste0("C:/Users/Yussuf Schwarz/OneDrive/Desktop/UniVWL/Projekt/LMST Forecasts/",Path))  

  forecast_lmst_uncat1_1 = c()
  max_range = floor(83/h)
  for (i in 1:max_range) {
    filename <- paste0(Filename, i, ".xlsx")
    forecast_df <- data.frame(read_xlsx(paste0(path,"/",filename)))
    colnames(forecast_df) = "forecast"
    forecast_lmst_uncat1_1 <- c(forecast_lmst_uncat1_1, forecast_df$forecast)
  }
  uncat_data_test = uncat_data[(((floor(nrow(uncat_data)*0.7))+1):nrow(uncat_data)),]
  print(c(forecast_lmst_uncat1_1,(floor(83/h)*h)))
  forecast_error_lmst = uncat_data_test$presidential_approval[1:(floor(83/h)*h)]-forecast_lmst_uncat1_1
  RMSE_lmst = sqrt(mean(forecast_error_lmst^2))
  print(c(RMSE_lmst,sqrt(mean(forecast_error_lmst^2))))
  savename=paste0(savename,"_",spec)
  assign(savename,RMSE_lmst,envir = .GlobalEnv)
}


uncat_names_lstm = paste0("Uncat",1:6)
count = 0
for (i in uncat_names_lstm) {
  count = count +1
  load_lmst_results(i,"Forecast_lmst_uncat_1","RMSE_lmst_uncat_1",1,count)
  load_lmst_results(i,"Forecast_lmst_uncat_2","RMSE_lmst_uncat_2",2,count)
  load_lmst_results(i,"Forecast_lmst_uncat_3","RMSE_lmst_uncat_3",3,count)
  load_lmst_results(i,"Forecast_lmst_uncat_4","RMSE_lmst_uncat_4",4,count)
}



########################HPC#####################################
################################################################
################################################################


##########################Uncategorized#######################################
##########################Checked for stationarity############################



x = data_overall_start1953[,-delete]
y=approval_q$Approval[13:nrow(approval_q)]
x = data.frame(cbind(x,lag(x,n = 1L),lag(x,n = 2L),lag(x,n = 3L),lag(x,n = 4L),lag(y,n = 1L),lag(y,n = 2L),lag(y,n = 3L),lag(y,n = 4L)))
x_m = na.omit(x)
y_m=approval_q$Approval[17:nrow(approval_q)]

uncat_data = data.frame(cbind(x_m[,as.numeric(rownames(parameters_optimal_df_x))],y_m,x_m$lag.y..n...1L.,x_m$lag.y..n...2L.,x_m$lag.y..n...3L.,x_m$lag.y..n...4L.))
colnames(uncat_data) = c(colnames(x_m)[as.numeric(rownames(parameters_optimal_df_x))],"presidential_approval","presidential_approval_lag_1","presidential_approval_lag_2","presidential_approval_lag_3","presidential_approval_lag_4")


uncat_data
uncat_data_stationary = as.data.frame(matrix(NA,nrow=nrow(uncat_data),ncol = ncol(uncat_data)))
for (i in 1:ncol(uncat_data)) {
 kpss_test_rf =  kpss.test(uncat_data[,i])
  kpss_test = kpss_test_rf$p.value
  if (kpss_test < 0.05) {
   uncat_data_stationary[,i] = c(NA,diff(uncat_data[,i])) 
  }
  else{uncat_data_stationary[,i] = uncat_data[,i]}
}
uncat_data_stationary = na.omit(uncat_data_stationary) 
colnames(uncat_data_stationary) = c(colnames(x_m)[as.numeric(rownames(parameters_optimal_df_x))],"presidential_approval","presidential_approval_lag_1","presidential_approval_lag_2","presidential_approval_lag_3","presidential_approval_lag_4")

write_xlsx(data.frame(uncat_data_stationary), "uncat_data_stationary.xlsx")


manipulated_x_2 <- as.data.frame(read_excel("manipulated_x_2.xlsx"))
data_principal_components <- as.data.frame(read_excel("data_principal_components.xlsx"))
approval_q <- as.data.frame(read_excel("approval_q.xlsx"))
data_overall_start1953 <- as.data.frame(read_excel("data_overall_start1953.xlsx"))
parameters_optimal_df <- as.data.frame(read_excel("Optimal_Parameters.xlsx"))
uncat_data_stationary <- as.data.frame(read_excel("uncat_data_stationary.xlsx"))
parameters_optimal_df_x <- as.data.frame(read_excel("parameters_optimal_df_x.xlsx"))
parameters_optimal_df_y <- as.data.frame(read_excel("parameters_optimal_df_y.xlsx"))

#uncat_data_stationary = scale(uncat_data_stationary)
uncat_data_train = uncat_data_stationary[(1:floor(nrow(uncat_data_stationary)*0.7)),]
uncat_data_test = uncat_data_stationary[(((floor(nrow(uncat_data_stationary)*0.7))+1):nrow(uncat_data_stationary)),]

train_x = uncat_data_train[,-43] 
train_y =uncat_data_train[,43] 

train_x = data.matrix(train_x)
train_y = data.matrix(train_y)

test_x = uncat_data_test[,-43] 
test_y =uncat_data_test[,43] 

test_x = data.matrix(test_x)
test_y = data.matrix(test_y)


train_x_arr <- array(
    data = train_x,
    dim = c(
        192,1,46))

train_y_arr <- array(
    data = train_y,
    dim = c(
        192,1))

test_x_arr <- array(
    data = test_x,
    dim = c(
        83,1,46))

test_y_arr <- array(
    data = test_y,
    dim = c(
        83,1))

forecast_lstm_uncat = function(h){
  n_steps = nrow(uncat_data_test)
  begin = nrow(uncat_data_train)
  
  
  i = index
    library(keras)
    End = begin+h*(i-1)
    print(End)
    
    train_x = uncat_data[c(1:End),-43] 
    train_y =uncat_data[c(1:End),43] 
    
    train_x = data.matrix(train_x)
    train_y = data.matrix(train_y)
    train_x_arr <- array(
      data = train_x,
      dim = c(
        nrow(train_x),1,46))
    train_y_arr <- array(
      data = train_y,
      dim = c(
        nrow(train_x),1))
    
    model <- keras_model_sequential()
    model %>%
      layer_lstm(units            = 84, 
                 input_shape      = c(1, 46), 
                 batch_size       = 1,
                 return_sequences = TRUE, 
                 stateful         = TRUE) %>% 
      layer_lstm(units            = 84, 
                 return_sequences = TRUE, 
                 stateful         = TRUE) %>% 
      layer_lstm(units            = 84, 
                 return_sequences = TRUE, 
                 stateful         = TRUE) %>% 
      layer_lstm(units            = 84, 
                 return_sequences = FALSE, 
                 stateful         = TRUE) %>% 
      layer_dense(units = 64)
    model %>% 
      compile(loss = 'mean_squared_error', optimizer = 'adam')
    model %>% fit(x = train_x_arr, 
                  y = train_y_arr, 
                  batch_size = 1,
                  epochs     = 300, 
                  verbose    = 1, 
                  shuffle    = FALSE)
    
    Begin_test = i*h-(h-1)
    End_test = i*h
    print(c(Begin_test,End_test,length(Begin_test:End_test)))
    
    test_x = uncat_data_test[(Begin_test:End_test),-43] 
    test_x = data.matrix(test_x)
    test_x_arr <- array(
      data = test_x,
      dim = c(
        h,1,46))
    pred_out <- model %>% 
      predict(test_x_arr, batch_size = 1) %>%
      .[,1] 
    forecast = pred_out
    print(pred_out)
    nameforecast_save = paste0("Forecast_lmst_uncat_st_",h,index,".xlsx")
    write_xlsx(data.frame(forecast), nameforecast_save)
    model = c()
    detach("package:keras",unload=T)
}


set.seed(42)

h_lmst = c(1,2,3,4)

for (i in h_lmst) {
  forecast_lstm_uncat(as.numeric(i))
}

########################HPC END#################################
################################################################
################################################################



######################Import LMST Uncat stationary checked HPC Results###################

load_lmst_results_st = function(Path,Filename,savename,h,spec){
    path <- file.path(paste0("C:/Users/Yussuf Schwarz/OneDrive/Desktop/UniVWL/Projekt/LMST Forecasts/",Path))  
  uncat_data_test = uncat_data_stationary[(((floor(nrow(uncat_data_stationary)*0.7))+1):nrow(uncat_data_stationary)),]
uncat_data_test = as.data.frame(uncat_data_test)
  forecast_lmst_uncat1_1 = c()
  max_range = floor(83/h)
  for (i in 1:max_range) {
    filename <- paste0(Filename, i, ".xlsx")
    forecast_df <- data.frame(read_xlsx(paste0(path,"/",filename)))
    colnames(forecast_df) = "forecast"
    forecast_lmst_uncat1_1 <- c(forecast_lmst_uncat1_1, forecast_df$forecast)
  }
  uncat_data_test = uncat_data_stationary[(((floor(nrow(uncat_data_stationary)*0.7))+1):nrow(uncat_data_stationary)),]
  print(c(forecast_lmst_uncat1_1,(floor(83/h)*h)))
  forecast_error_lmst = uncat_data_test$presidential_approval[1:(floor(83/h)*h)]-forecast_lmst_uncat1_1
  RMSE_lmst = sqrt(mean(forecast_error_lmst^2))
  print(c(RMSE_lmst,sqrt(mean(forecast_error_lmst^2))))
  savename=paste0(savename,"_",spec)
  assign(savename,RMSE_lmst,envir = .GlobalEnv)
}

uncat_st_names_lstm = paste0("Uncat",1:15,"st")
count=0
for (i in uncat_st_names_lstm) {
  count=count+1
  load_lmst_results_st(i,"Forecast_lmst_uncat_st_1","RMSE_lmst_uncat_st_1",1,count)
  load_lmst_results_st(i,"Forecast_lmst_uncat_st_2","RMSE_lmst_uncat_st_2",2,count)
  load_lmst_results_st(i,"Forecast_lmst_uncat_st_3","RMSE_lmst_uncat_st_3",3,count)
  load_lmst_results_st(i,"Forecast_lmst_uncat_st_4","RMSE_lmst_uncat_st_4",4,count)
}



########################HPC#####################################
################################################################
################################################################


##########################Categorized#######################################
##########################Checked for stationarity############################

library(readxl)
library(Matrix)
library(glmnet)
library(dplyr,warn.conflicts = FALSE)
library(tidyr,warn.conflicts = FALSE)
library(ggplot2)
library(writexl)








index <- Sys.getenv(c("SLURM_ARRAY_TASK_ID")) 
index=as.numeric(index)



library(keras)
library(tensorflow)


manipulated_x_2 <- as.data.frame(read_excel("manipulated_x_2.xlsx"))
data_principal_components <- as.data.frame(read_excel("data_principal_components.xlsx"))
approval_q <- as.data.frame(read_excel("approval_q.xlsx"))
data_overall_start1953 <- as.data.frame(read_excel("data_overall_start1953.xlsx"))
parameters_optimal_df <- as.data.frame(read_excel("Optimal_Parameters.xlsx"))
uncat_data_stationary <- as.data.frame(read_excel("cat_data.xlsx"))
parameters_optimal_df_x <- as.data.frame(read_excel("parameters_optimal_df_x.xlsx"))
parameters_optimal_df_y <- as.data.frame(read_excel("parameters_optimal_df_y.xlsx"))

uncat_data_train = uncat_data_stationary[(1:floor(nrow(uncat_data_stationary)*0.7)),]
uncat_data_test = uncat_data_stationary[(((floor(nrow(uncat_data_stationary)*0.7))+1):nrow(uncat_data_stationary)),]



uncat_data_train[is.na(uncat_data_train)] <- 0
uncat_data_test[is.na(uncat_data_test)] <- 0

train_x = uncat_data_train[,-7] 
train_y =uncat_data_train[,7] 

train_x = data.matrix(train_x)
train_y = data.matrix(train_y)

test_x = uncat_data_test[,-7] 
test_y =uncat_data_test[,7] 

test_x = data.matrix(test_x)
test_y = data.matrix(test_y)


train_x_arr <- array(
  data = train_x,
  dim = c(
    192,1,6))

train_y_arr <- array(
  data = train_y,
  dim = c(
    192,1))

test_x_arr <- array(
  data = test_x,
  dim = c(
    83,1,6))

test_y_arr <- array(
  data = test_y,
  dim = c(
    83,1))

forecast_lstm_uncat = function(h){
  n_steps = floor(nrow(uncat_data_test)/h)
  begin = nrow(uncat_data_train)
  
  
  i = index
  library(keras)
  train_range=cbind(c(c(1:83)*h))
  end_train_range = floor(83/h)
  train_range = c(0,train_range[c(1:end_train_range)])
  
  End = begin+train_range[index]
  print(End)
  
  train_x = uncat_data_stationary[c(1:End),-7] 
  train_y =uncat_data_stationary[c(1:End),7] 
  
  train_x = data.matrix(train_x)
  train_y = data.matrix(train_y)
  train_x_arr <- array(
    data = train_x,
    dim = c(
      nrow(train_x),1,6))
  train_y_arr <- array(
    data = train_y,
    dim = c(
      nrow(train_x),1))
  
  model <- keras_model_sequential()
  model %>%
    layer_lstm(units            = 64, 
               input_shape      = c(1, 6), 
               batch_size       = 1,
               return_sequences = TRUE, 
               stateful         = TRUE) %>%  
    layer_lstm(units            = 4, 
               return_sequences = FALSE, 
               stateful         = TRUE) %>%  
    layer_dense(units = 2)
  model %>% 
    compile(loss = 'mean_squared_error', optimizer = 'adam')
  model %>% fit(x = train_x_arr, 
                y = train_y_arr, 
                batch_size = 1,
                epochs     = 300, 
                verbose    = 1, 
                shuffle    = FALSE)
  
  test_range=cbind(c(1:83)*h-(h-1),c(1:83)*h)
  end_range = floor(83/h)
  test_range = test_range[c(1:end_range),]
  
  Begin_test_range = test_range[index,1] 
  End_test_range = test_range[index,2]
  
  test_x = uncat_data_test[(Begin_test_range:End_test_range),-7] 
  test_x = data.matrix(test_x)
  test_x_arr <- array(
    data = test_x,
    dim = c(
      h,1,6))
  pred_out <- model %>% 
    predict(test_x_arr, batch_size = 1) %>%
    .[,1] 
  forecast = pred_out
  print(pred_out)
  nameforecast_save = paste0("Forecast_lmst_cat_st_",h,index,".xlsx")
  write_xlsx(data.frame(forecast), nameforecast_save)
  model = c()
  detach("package:keras",unload=T)
}


set.seed(42)

h_lmst = c(1,2,3,4)



for (b in h_lmst) {
  if (index <= floor(83/as.numeric(b))) {  forecast_lstm_uncat(as.numeric(b))}
  else{next}
}


########################HPC END#################################
################################################################
################################################################



######################Import LSTM cat stationary checked HPC Results###################

load_lmst_results_cat = function(Path,Filename,savename,h,spec){
    path <- file.path(paste0("C:/Users/Yussuf Schwarz/OneDrive/Desktop/UniVWL/Projekt/LMST Forecasts/",Path))  
  uncat_data_test = cat_data_stationary[(((floor(nrow(cat_data_stationary)*0.7))+1):nrow(cat_data_stationary)),]
uncat_data_test = as.data.frame(cat_data_test)
  forecast_lmst_cat1_1 = c()
  max_range = floor(83/h)
  for (i in 1:max_range) {
    filename <- paste0(Filename, i, ".xlsx")
    forecast_df <- data.frame(read_xlsx(paste0(path,"/",filename)))
    colnames(forecast_df) = "forecast"
    forecast_lmst_cat1_1 <- c(forecast_lmst_cat1_1, forecast_df$forecast)
  }
  uncat_data_test = cat_data[(((floor(nrow(cat_data_stationary)*0.7))+1):nrow(cat_data_stationary)),]
  print(c(forecast_lmst_cat1_1,(floor(83/h)*h)))
  forecast_error_lmst = cat_data_test$presidential_approval[1:(floor(83/h)*h)]-forecast_lmst_cat1_1
  print(sum(na.omit(forecast_error_lmst)^2))
  forecast_error_lmst = na.omit(forecast_error_lmst)
  RMSE_lmst = sqrt(mean(forecast_error_lmst^2))
  print(c(RMSE_lmst,sqrt(mean(forecast_error_lmst^2))))
  savename=paste0(savename,"_",spec)
  assign(savename,RMSE_lmst,envir = .GlobalEnv)
}

cat_names_lstm = paste0("Cat",1:14)
count=0
for (i in cat_names_lstm) {
  count=count+1
  load_lmst_results_cat(i,"Forecast_lmst_cat_st_1","RMSE_lmst_cat_st_1",1,count)
  load_lmst_results_cat(i,"Forecast_lmst_cat_st_2","RMSE_lmst_cat_st_2",2,count)
  load_lmst_results_cat(i,"Forecast_lmst_cat_st_3","RMSE_lmst_cat_st_3",3,count)
  load_lmst_results_cat(i,"Forecast_lmst_cat_st_4","RMSE_lmst_cat_st_4",4,count)
}



 #######################Forecast results######################################## 


#####Naive#####

RMSE_wn = c(RMSE_wn_1,RMSE_wn_2,RMSE_wn_3,RMSE_wn_4)
RMSE_ar = c(RMSE_ar1_1,RMSE_ar1_2,RMSE_ar1_3,RMSE_ar1_4)
RMSE_ar_BIC = c(RMSE_BIC_1,RMSE_BIC_2,RMSE_BIC_3,RMSE_BIC_4)
RMSE_ar_AIC = c(RMSE_AIC_1,RMSE_AIC_2,RMSE_AIC_3,RMSE_AIC_4)

Model_name_ar = c(rep("White Noise",times=4),rep("AR(1)",times=4),rep("ARIMA(P,d,Q) BIC",times=4),rep("ARIMA(P,d,Q) AIC",times=4))

Forecast_horizon_ar = rep(c(1,2,3,4),times=4)

ar_grid = data.frame(cbind(Model_name_ar,Forecast_horizon_ar,c(RMSE_wn,RMSE_ar,RMSE_ar_BIC,RMSE_ar_AIC)))
colnames(ar_grid) = c("Model","Forecast Horizon","RMSE")
ar_grid <- ar_grid[order(ar_grid$`Forecast Horizon`), ]

#####Random Forest Non-stationary Non-categorized#####

Name = "RMSE_rf_"
tree_size = c(10,100,500,1000,5000,10000)
Forecast_horizon = c(1,2,3,4)
rf_grid = expand.grid(Name,tree_size,Forecast_horizon)
rfs_RMSE = c()
rf_grid$RMSE = NA
for (i in 1:nrow(rf_grid)) {
  Name_to_get = paste0(rf_grid$Var1[i],rf_grid$Var2[i],"_",rf_grid$Var3[i])
  print(get(Name_to_get,envir=.GlobalEnv))
  rf_grid$RMSE[i]=get(Name_to_get,envir=.GlobalEnv)
}
colnames(rf_grid) = c("Model","Tree Size","Forecast Horizon","RMSE")
rf_grid$Model = rep("Random Forest Non-Stationary Non-Categorized",times=nrow(rf_grid))

#####Random Forest Stationary Non-categorized#####

Name = "RMSE_rf_st_"
tree_size = c(10,100,500,1000,5000,10000)
Forecast_horizon = c(1,2,3,4)
rf_grid_st = expand.grid(Name,tree_size,Forecast_horizon)
rfs_RMSE_st = c()
rf_grid_st$RMSE = NA
for (i in 1:nrow(rf_grid_st)) {
  Name_to_get = paste0(rf_grid_st$Var1[i],rf_grid_st$Var2[i],"_",rf_grid_st$Var3[i])
  print(get(Name_to_get,envir=.GlobalEnv))
  rf_grid_st$RMSE[i]=get(Name_to_get,envir=.GlobalEnv)
}
colnames(rf_grid_st) = c("Model","Tree Size","Forecast Horizon","RMSE")
rf_grid_st$Model = rep("Random Forest Stationary Non-Categorized",times=nrow(rf_grid_st))


#####Random Forest Stationary Categorized#####

Name = "RMSE_rf_st_cat_"
tree_size = c(10,100,500,1000,5000,10000)
Forecast_horizon = c(1,2,3,4)
rf_grid_st_cat = expand.grid(Name,tree_size,Forecast_horizon)
rfs_RMSE_st_cat = c()
rf_grid_st$RMSE = NA
for (i in 1:nrow(rf_grid_st_cat)) {
  Name_to_get = paste0(rf_grid_st_cat$Var1[i],rf_grid_st_cat$Var2[i],"_",rf_grid_st_cat$Var3[i])
  print(get(Name_to_get,envir=.GlobalEnv))
  rf_grid_st_cat$RMSE[i]=get(Name_to_get,envir=.GlobalEnv)
}
colnames(rf_grid_st_cat) = c("Model","Tree Size","Forecast Horizon","RMSE")
rf_grid_st_cat$Model = rep("Random Forest Stationary Categorized",times=nrow(rf_grid_st_cat))



#####VAR Modelle#####

Name = "RMSE_var_"
determinants = c("none","const","trend","both")
model_types = c("AIC","SC")
h = c(1,2,3,4)
grid_var = as.data.frame(expand.grid(Name,determinants,model_types,h))
grid_var$RMSE = NA
for (i in 1:nrow(grid_var)) {
  Name_to_get = paste0(grid_var$Var1[i],grid_var$Var2[i],"_",grid_var$Var3[i],"_",grid_var$Var4[i])
  print(get(Name_to_get,envir=.GlobalEnv))
  grid_var$RMSE[i]=get(Name_to_get,envir=.GlobalEnv)
}
colnames(grid_var) = c("Model","Determinant","Information Criteria","Forecast Horizon","RMSE")
grid_var$Model = rep("VAR",times=nrow(grid_var))

stargazer(grid_var,rf_grid,rf_grid_st,rf_grid_st_cat,summary = F)


#####LSTM Modelle Non-Stationary Uncategorized#####

#Spec: #LSTM Layers, Dim. Hidden State, #Neurons dense layer, #Parameter,Dropout rate
Name = "RMSE_lmst_uncat_"
lstm_spec = 1:6
Forecast_horizon = c(1,2,3,4)
lstm_grid_uncat = expand.grid(Name,lstm_spec,Forecast_horizon)
lstm_RMSE_uncat = c()
lstm_grid_uncat$RMSE = NA
for (i in 1:nrow(lstm_grid_uncat)) {
  Name_to_get = paste0(lstm_grid_uncat$Var1[i],lstm_grid_uncat$Var3[i],"_",lstm_grid_uncat$Var2[i])
  print(get(Name_to_get,envir=.GlobalEnv))
  lstm_grid_uncat$RMSE[i]=get(Name_to_get,envir=.GlobalEnv)
}
colnames(lstm_grid_uncat) = c("Model","Specification","Forecast Horizon","RMSE")
lstm_grid_uncat$Model = rep("Long Short-Term Memory Networks (LSTM) Non-Stationary Uncategorized",times=nrow(lstm_grid_uncat))

#Spec: #LSTM Layers, Dim. Hidden State, #Neurons dense layer, #Parameter,Dropout rate, #Epochs

lstm_grid_uncat=lstm_grid_uncat[order(lstm_grid_uncat$Specification),]
spec1 = rep(c("5,124,64,710336,0,300","2,24,4,11620,0,300","5,24,4,25732,0.2,300","2,2;4,2,514,0,300","9,84,64,503728,0,300","2,4,2,970,0,300"),times=c(4,4,4,4,4,4))

lstm_grid_uncat$Specification = spec1
lstm_grid_uncat=lstm_grid_uncat[order(lstm_grid_uncat$`Forecast Horizon`),]


#####LSTM Modelle Stationary Uncategorized#####


Name = "RMSE_lmst_uncat_st_"
lstm_spec = 1:15
Forecast_horizon = c(1,2,3,4)
lstm_grid_uncat_st = expand.grid(Name,lstm_spec,Forecast_horizon)
lstm_RMSE_uncat_st = c()
lstm_grid_uncat_st$RMSE = NA
for (i in 1:nrow(lstm_grid_uncat_st)) {
  Name_to_get = paste0(lstm_grid_uncat_st$Var1[i],lstm_grid_uncat_st$Var3[i],"_",lstm_grid_uncat_st$Var2[i])
  print(get(Name_to_get,envir=.GlobalEnv))
  lstm_grid_uncat_st$RMSE[i]=get(Name_to_get,envir=.GlobalEnv)
}
colnames(lstm_grid_uncat_st) = c("Model","Specification","Forecast Horizon","RMSE")
lstm_grid_uncat_st$Model = rep("Long Short-Term Memory Networks (LSTM) Stationary Uncategorized",times=nrow(lstm_grid_uncat_st))



#Spec: #LSTM Layers, Dim. Hidden State, #Neurons dense layer, #Parameter,Dropout rate

lstm_grid_uncat_st=lstm_grid_uncat_st[order(lstm_grid_uncat_st$Specification),]
spec2 = rep(c("2,4;64,2,29530,0,300","2,4;64,2,29530,0,300","2,4,2,970,0,300","2,4,2,970,0.4,300","2,2,1,435,0.2,300","2,1,1,206,0.4,300","1,1,1,194,0.6,300","1,1,0,192,0,300","2,1,1,206,0.8,300","4,1,1,230,0.8,300","4,1,1,230,0.9,600","4,1,1,230,0.8,1000","4,1,1,230,0.7,500","3,1,1,218,0.7,500","2,2,1,410,0.7,500"),times=c(4,4,4,4,4,4,4,4,4,4,4,4,4,4,4))

lstm_grid_uncat_st$Specification = spec2
lstm_grid_uncat_st=lstm_grid_uncat_st[order(lstm_grid_uncat_st$`Forecast Horizon`),]


#####LSTM Modelle Stationary Categorized#####


Name = "RMSE_lmst_cat_st_"
lstm_spec = 1:14
Forecast_horizon = c(1,2,3,4)
lstm_grid_cat_st = expand.grid(Name,lstm_spec,Forecast_horizon)
lstm_RMSE_cat_st = c()
lstm_grid_cat_st$RMSE = NA
for (i in 1:nrow(lstm_grid_cat_st)) {
  Name_to_get = paste0(lstm_grid_cat_st$Var1[i],lstm_grid_cat_st$Var3[i],"_",lstm_grid_cat_st$Var2[i])
  print(get(Name_to_get,envir=.GlobalEnv))
  lstm_grid_cat_st$RMSE[i]=get(Name_to_get,envir=.GlobalEnv)
}
colnames(lstm_grid_cat_st) = c("Model","Specification","Forecast Horizon","RMSE")
lstm_grid_cat_st$Model = rep("Long Short-Term Memory Networks (LSTM) Stationary Categorized",times=nrow(lstm_grid_cat_st))


#Spec: #LSTM Layers, Dim. Hidden State, #Neurons dense layer, #Parameter,Dropout rate

lstm_grid_cat_st=lstm_grid_cat_st[order(lstm_grid_cat_st$Specification),]
spec3 = rep(c("2,64;4,2,19290,0,300","2,4,2,330,0,300","2,2,1,115,0.4,300","2,1,1,46,0.4,300","1,1,1,34,0.4,300","1,1,0,32,0,300","2,1,1,46,0.8,300","4,1,1,70,0.8,300","2,1,1,46,0.7,1000","2,1,1,46,0.8,1500","2,1,1,46,0.7,1500","2,1,1,46,0.6,1000","2,1,1,46,0.6,1000","1,2,1,75,0.8,1000"),times=c(4,4,4,4,4,4,4,4,4,4,4,4,4,4))

lstm_grid_cat_st$Specification = spec3
lstm_grid_cat_st=lstm_grid_cat_st[order(lstm_grid_cat_st$`Forecast Horizon`),]

stargazer(ar_grid,summary = F,rownames = F)
stargazer(grid_var,summary = F,rownames = F)
stargazer(rbind(rf_grid,rf_grid_st,rf_grid_st_cat),summary = F,rownames = F)
stargazer(lstm_grid_uncat,summary = F,rownames = F)
stargazer(lstm_grid_uncat_st,summary = F,rownames = F)
stargazer(lstm_grid_cat_st,summary = F,rownames = F)


forecast_table_1 = rbind(
ar_grid[ar_grid$`Forecast Horizon` ==1,][which.min(ar_grid[ar_grid$`Forecast Horizon` ==1,]$RMSE),],
ar_grid[ar_grid$`Forecast Horizon` ==2,][which.min(ar_grid[ar_grid$`Forecast Horizon` ==2,]$RMSE),],
ar_grid[ar_grid$`Forecast Horizon` ==3,][which.min(ar_grid[ar_grid$`Forecast Horizon` ==3,]$RMSE),],
ar_grid[ar_grid$`Forecast Horizon` ==4,][which.min(ar_grid[ar_grid$`Forecast Horizon` ==4,]$RMSE),])


forecast_table_2 = rbind(grid_var[grid_var$`Forecast Horizon` ==1,][which.min(grid_var[grid_var$`Forecast Horizon` ==1,]$RMSE),],
grid_var[grid_var$`Forecast Horizon` ==2,][which.min(grid_var[grid_var$`Forecast Horizon` ==2,]$RMSE),],
grid_var[grid_var$`Forecast Horizon` ==3,][which.min(grid_var[grid_var$`Forecast Horizon` ==3,]$RMSE),],
grid_var[grid_var$`Forecast Horizon` ==4,][which.min(grid_var[grid_var$`Forecast Horizon` ==4,]$RMSE),])

forecast_table_3 = rbind(rf_grid[rf_grid$`Forecast Horizon` ==1,][which.min(rf_grid[rf_grid$`Forecast Horizon` ==1,]$RMSE),],
rf_grid[rf_grid$`Forecast Horizon` ==2,][which.min(rf_grid[rf_grid$`Forecast Horizon` ==2,]$RMSE),],
rf_grid[rf_grid$`Forecast Horizon` ==3,][which.min(rf_grid[rf_grid$`Forecast Horizon` ==3,]$RMSE),],
rf_grid[rf_grid$`Forecast Horizon` ==4,][which.min(rf_grid[rf_grid$`Forecast Horizon` ==4,]$RMSE),])

forecast_table_4 = rbind(rf_grid_st[rf_grid_st$`Forecast Horizon` ==1,][which.min(rf_grid_st[rf_grid_st$`Forecast Horizon` ==1,]$RMSE),],
rf_grid_st[rf_grid_st$`Forecast Horizon` ==2,][which.min(rf_grid_st[rf_grid_st$`Forecast Horizon` ==2,]$RMSE),],
rf_grid_st[rf_grid_st$`Forecast Horizon` ==3,][which.min(rf_grid_st[rf_grid_st$`Forecast Horizon` ==3,]$RMSE),],
rf_grid_st[rf_grid_st$`Forecast Horizon` ==4,][which.min(rf_grid_st[rf_grid_st$`Forecast Horizon` ==4,]$RMSE),])

forecast_table_5 = rbind(rf_grid_st_cat[rf_grid_st_cat$`Forecast Horizon` ==1,][which.min(rf_grid_st_cat[rf_grid_st_cat$`Forecast Horizon` ==1,]$RMSE),],
rf_grid_st_cat[rf_grid_st_cat$`Forecast Horizon` ==2,][which.min(rf_grid_st_cat[rf_grid_st_cat$`Forecast Horizon` ==2,]$RMSE),],
rf_grid_st_cat[rf_grid_st_cat$`Forecast Horizon` ==3,][which.min(rf_grid_st_cat[rf_grid_st_cat$`Forecast Horizon` ==3,]$RMSE),],
rf_grid_st_cat[rf_grid_st_cat$`Forecast Horizon` ==4,][which.min(rf_grid_st_cat[rf_grid_st_cat$`Forecast Horizon` ==4,]$RMSE),])

overall_rf_grid = rbind(rf_grid,rf_grid_st,rf_grid_st_cat)
forecast_table_overallrf = rbind(overall_rf_grid[overall_rf_grid$`Forecast Horizon`==1,][which.min(overall_rf_grid[overall_rf_grid$`Forecast Horizon`==1,]$RMSE),],overall_rf_grid[overall_rf_grid$`Forecast Horizon`==2,][which.min(overall_rf_grid[overall_rf_grid$`Forecast Horizon`==2,]$RMSE),],overall_rf_grid[overall_rf_grid$`Forecast Horizon`==3,][which.min(overall_rf_grid[overall_rf_grid$`Forecast Horizon`==3,]$RMSE),],overall_rf_grid[overall_rf_grid$`Forecast Horizon`==4,][which.min(overall_rf_grid[overall_rf_grid$`Forecast Horizon`==4,]$RMSE),]
                                 )
stargazer(forecast_table_overallrf,summary = F,rownames=F)


forecast_table_6 = rbind(lstm_grid_uncat[lstm_grid_uncat$`Forecast Horizon` ==1,][which.min(lstm_grid_uncat[lstm_grid_uncat$`Forecast Horizon` ==1,]$RMSE),],
lstm_grid_uncat[lstm_grid_uncat$`Forecast Horizon` ==2,][which.min(lstm_grid_uncat[lstm_grid_uncat$`Forecast Horizon` ==2,]$RMSE),],
lstm_grid_uncat[lstm_grid_uncat$`Forecast Horizon` ==3,][which.min(lstm_grid_uncat[lstm_grid_uncat$`Forecast Horizon` ==3,]$RMSE),],
lstm_grid_uncat[lstm_grid_uncat$`Forecast Horizon` ==4,][which.min(lstm_grid_uncat[lstm_grid_uncat$`Forecast Horizon` ==4,]$RMSE),])

forecast_table_7 = rbind(lstm_grid_uncat_st[lstm_grid_uncat_st$`Forecast Horizon` ==1,][which.min(lstm_grid_uncat_st[lstm_grid_uncat_st$`Forecast Horizon` ==1,]$RMSE),],
lstm_grid_uncat_st[lstm_grid_uncat_st$`Forecast Horizon` ==2,][which.min(lstm_grid_uncat_st[lstm_grid_uncat_st$`Forecast Horizon` ==2,]$RMSE),],
lstm_grid_uncat_st[lstm_grid_uncat_st$`Forecast Horizon` ==3,][which.min(lstm_grid_uncat_st[lstm_grid_uncat_st$`Forecast Horizon` ==3,]$RMSE),],
lstm_grid_uncat_st[lstm_grid_uncat_st$`Forecast Horizon` ==4,][which.min(lstm_grid_uncat_st[lstm_grid_uncat_st$`Forecast Horizon` ==4,]$RMSE),])

forecast_table_8 = rbind(lstm_grid_cat_st[lstm_grid_cat_st$`Forecast Horizon` ==1,][which.min(lstm_grid_cat_st[lstm_grid_cat_st$`Forecast Horizon` ==1,]$RMSE),],
lstm_grid_cat_st[lstm_grid_cat_st$`Forecast Horizon` ==2,][which.min(lstm_grid_cat_st[lstm_grid_cat_st$`Forecast Horizon` ==2,]$RMSE),],
lstm_grid_cat_st[lstm_grid_cat_st$`Forecast Horizon` ==3,][which.min(lstm_grid_cat_st[lstm_grid_cat_st$`Forecast Horizon` ==3,]$RMSE),],
lstm_grid_cat_st[lstm_grid_cat_st$`Forecast Horizon` ==4,][which.min(lstm_grid_cat_st[lstm_grid_cat_st$`Forecast Horizon` ==4,]$RMSE),])


lstm_overall_grid = rbind(lstm_grid_uncat,lstm_grid_uncat_st,lstm_grid_cat_st)
lstm_overall_table = rbind(lstm_overall_grid[lstm_overall_grid$`Forecast Horizon` ==1,][which.min(lstm_overall_grid[lstm_overall_grid$`Forecast Horizon` ==1,]$RMSE),],lstm_overall_grid[lstm_overall_grid$`Forecast Horizon` ==2,][which.min(lstm_overall_grid[lstm_overall_grid$`Forecast Horizon` ==2,]$RMSE),],lstm_overall_grid[lstm_overall_grid$`Forecast Horizon` ==3,][which.min(lstm_overall_grid[lstm_overall_grid$`Forecast Horizon` ==3,]$RMSE),],lstm_overall_grid[lstm_overall_grid$`Forecast Horizon` ==4,][which.min(lstm_overall_grid[lstm_overall_grid$`Forecast Horizon` ==4,]$RMSE),])

stargazer(lstm_overall_table,summary = F,rownames=F)

stargazer(forecast_table_1,summary = F,rownames=F)
stargazer(forecast_table_2,summary = F,rownames=F)
stargazer(forecast_table_3,summary = F,rownames=F)
stargazer(forecast_table_4,summary = F,rownames=F)
stargazer(forecast_table_5,summary = F,rownames=F)
stargazer(forecast_table_6,summary = F,rownames=F)
stargazer(forecast_table_7,summary = F,rownames=F)
stargazer(forecast_table_8,summary = F,rownames=F)

```



```{r Presidential election 2024 forecasts, include=T}

#In this section, a forecast of approval based on the best model specification for the election in 2024 is conducted.
#However, this is not included in the thesis as the inclusion would lead to surpassing the allowed scope of the text.

#last retreived data: 2023 Q2 --> election in 2024 Q4 --> 6 Quarters need to be forecasted.
#CIs are bootstrapped 


###based on the best model for 1 step ahead predictions:

#Begin 270 until 275: 271 --> 277; 272 --> 278; 273 --> 279; 274 --> 280; 275 --> 281; 276 --> 282 


nrow(uncat_data)
count = 0

####Bootstrapping Data#####

bootstrap_sample = 0
for (bootstrap in 1:5000) {
bootstrap_sample = bootstrap_sample +1

h=6
data_bootstrapped = data.frame(matrix(NA,nrow=276,ncol =ncol(uncat_data_train)))
bootstrap_range=cbind(c(1:276)*h-(h-1),c(1:276)*h)
end_range = floor(276/h)
bootstrap_range = bootstrap_range[c(1:end_range),]
index_bootstrapped = sort(sample(x = c(1:46),replace = T,size = 46))

bootstrap_range_data = bootstrap_range[index_bootstrapped,]
  for (b in 1:nrow(bootstrap_range)) {
   bootstrap_index_begin = bootstrap_range_data[b,1]
   bootstrap_index_end = bootstrap_range_data[b,2]
   
   data_bootstrapped[c(bootstrap_range[b,1]:bootstrap_range[b,2]),] = uncat_data_train[c(bootstrap_index_begin:bootstrap_index_end),]}
}


  






```